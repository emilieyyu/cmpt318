delay, and can immediately of updates, also a common disadvantage updated. To design or reply to the request of CScAsr, when a sender generates bursts pattern, the advantage over ABCAST is even greater. On the part of the table being it is not hard to using CBCAST is that the sender needs mutual exclusion our experience suggests that if mutual exclusion A single locking. 

However, applications have strong benefits, may suffice to have this property. Operation for a whole series structuring the data of multicasts, and in some case locking can be entirely avoided just by the appropriate application itself. This translates to a huge benefit for many asynchronous presented in [BSS91]. The distinction settings between causal and total event orderings system as seen in the performance. CBCAST and ABCAST have parallels a causal delivery ordering in other as part of. 

Although Isis was the first distributed to enforce Isis applications, locks are used primarily for mutual exclusion on possibly conflicting operations, such as updates on related data items. In the case of replicated data, this results in an algorithm similar to primary copy update in which the primary copy changes dynamically. The execution model is non-transactional thesis no need for read-locks or for two-phase locking rule. This is discussed further in Sec. 7.19 a communication time. 

Moreover, subsystem [Bir85]. the approach draws on Lampon's respects anticipated prior work on logical notions of replication the approach [BHG87]. was in some Similarly, software by work on primary copy both to Lamport's in database approach systems close synchrony is related state machine discussed to developing distributed [SclO0] architecture and to the database serializability has yielded a memory model further below. 

Work on a parallel processor update model called weak in the cache [-DSB86, TH90], which uses a causal dependency and, a causal [ABHN91] principle to increase the parallelism of a parallel processor memory discussion multiprocessors property has been used in work on lazy database systems [JB89, LLS90].

A more detailed in [Sch88, BJ89] and distribution of the conditions under which can be used in place of AI_&Vr. Summary of benefits due to virtual synchrony. Brevity precludes algorithms a more detailed discussion of virtual synchrony, or how it is used in developing the benefits of the model distributed within Isis. However, it may be useful to summarize. Allows code to be developed, supports a meaningful assuming a simplified, closely synchronous execution model. Replicated notion of group state and state transfer, both when groups manage is dynamically partitioned among group members. 

Data, and when a computation, asynchronous, treatment pipelined communication process group membership changes and failures through a single, of communication, execution model. Event-oriented failures handling through a consistent subsystem. This is in contrast presented system membership list integrated with the communication to the usual approach of sensing failures through timeouts and channels breaking, which would not guarantee consistency. The approach also has limitations: reduced availability during LAN partition failures. Only allows progress in a single partition, and requires that a majority of sites be available in that partition, risks incorrectly classifying an operational site or process as faulty. 

The virtual synchrony theoretical model is unusual in offering these benefits within a single framework.Moreover, evade arguments exist that no system that provides consistency. Our experience, distributed behavior can completely these limitations, has been that the issues addressed. Applications, and that the approach by virtual synchrony is general, complete, are encountered and theoretically in even the simplest distributed system.

The Isis toolkit provides a collection and implementing group-based of higher-level for forming and managing process groups of the approach by discussing of a distributed database software. This section illustrates the specifics the styles of group supported by the system and giving application. 

A simple example: Isis is not the first system to use process groups as programmers developed. Cheriton's V system had received wide visibility [CZ83]. More recently, group mechanisms the Chorus operating system become common, exemplified by the Ameoba system [KTHB89], system developed [RAA+88], IBM's AAS the Psync system [PBS$9], system [CDg0], a high availability by Ladin and Liskov [LLSg0], and Transis [ADKM91]. 

Nonetheless, solutions Isis was first to propose the virtual to a wide variety of problems synchrony model. The high performance approach is now gaining through its toolkit wide acceptance.

5.1 Styles of groups

Styles of groups of a distributed system is limited by the information. This was a consideration simplicity in developing available to the protocols employed for efficient communication. The Isis process group interface, of accurate where trade-off information about had to be made between group membership, introduces of the interface and the availability. As a consequence, interact for use in multicast address expansion.

The Isis application interface in four styles of process groups that differ in how processes writing. Our group is working with the software with the group, illustrated of a new version that the time of technology foundation which on integration into Mach (the OSF 1/AD version) and with Unix International, plans a reliable group mechanism for UI Atlas.

Fig. 8 (anonymous groups are not distinguished from explicit groups this level of the system). Isis is optimized to detect and handle each of these cases efficiently. Peer groups: These arise where a set of p_ cooperate closely, for example, to replicate data. The membership is often used as an input to the algorithm used in handling requests, as for the concurrent earlier. With any group given the group's name and of a group will multicast to it repeatedly, better database search described Client-server groups.

In Isis, any process can communicate. However, if a non-member appropriate permissions performance to optimize. Diffusion group is obtained by lust registering the group addressing protocol the sender as a client of the group. A diffusion group is a client-server group in which the clients register themselves but in which the members of the group send messages. Hierarchical groups: a hierarchical application to the full client set and the clients are passive sinks. Component groups, the group is a structure built from multiple that use the hierarchical group initially reasons of scale. are subsequently contact its root group, but be redirected to one of the constituent "subgroups". Group data would normally be partitioned among the subgroups of the hierarchy, the most common. 

Although tools are provided for multicasting communication pattern involves interaction with the full membership between a client and the members of some subgroup. There is no requirement executed that the members of a group be identical, or even coded in the same language and an individual or on the same architecture. Moreover, multiple groups can be overlapped process can belong to as many as several hundred different groups, although scaling is discussed further below, this is uncommon.

5.2 The toolkit interface

As noted earlier, pipelining the performance of a distributed system of asynchronous less efficient is often limited by the degree of communication problems can be tricky, and for this reason, the toolkit paradigms these tool by achieved.

The development solutions to distributed than risk errors. Many Isis users would rather employ includes asynchronous implementations solutions of the more important distributed programming include a synchronization for managing tool that supports a form of locking (based on distributed tokens), a replication primary-backup server design that load-balances replicated data, a tool for fault-tolerant making different group members act as the primary for different programming requests, and so forth. A partial list appears in ISIS manual, even non-experts distributed software in Table I.

Using these tools, and following have been successful in developing examples fault-tolerant, highly asynchronous process. 

Groups: create, delete, join (transferring CBCAST, ABCAST, collecting state). 0, 1 QUORUM or ALL replies. 0 replies gives an group multicast, asynchronous multicast.
Synchronization: Locking, with symbolic strings to represent locks. 

Deadlock detection or avoidance must be addressed at the application level. Token passing. Replicated processes data: Implemented by broadcasting updates to group having copies. Dynamic spooling system reconfiguration Transfer values to using replicated that join using state transfer facility data in an update facility: logging, configuration. 

Monitoring for state recovery after failure. Monitor watch a process, a site, trigger actions, site failures and etc after failures and recoveries. Changes to process group membership, Distributed multiple automated execution facilities. Redundant computation (primary/backup) all take the same action subdivided among servers. Coordinator-cohort recovery: when site recovers, program automatically restarted.

If first to recover, state loaded from logs or initialized by. State 
join active process group and transfer WAN communication: Reliable long-haul message passing and file transfer facility.

The example the procedures messages is in standard C. The server initializes main loop in Isis and declares incoming that will handle update and inquiry requests, dispatches to these procedures generation as needed (other styles of main loop are also supported). Scanning is specific to the C interface. The formatted-I/O is not style of message available where type information at runtime. the current contents of the database to a server that an existing server. The "state transfer" routines are concerned with sending has just been started and is joining the group. In this situation, do a state transfer, invoking will cause to an invocation the message its state sending of its state.

Isis arbitrarily selects each call that this procedure makes side; in our example, on the receiving, the latter simply passes and update to the update procedure (the same message format is used by send_state it is possible. Of course, there are many variants on this basic scheme; for example, only certain servers to join, and so forth. The client program does a pg_lookup to find the server. should be allowed to handle to indicate to the system that processes state transfer requests, to refuse to allow certain calls to its query and update. Subsequently, procedures are mapped into messages to timeserver. 

Here are some programming examples on how isis handles incoming SQL queries.

For the group - ABCAST in this case, the database server of Figure 9 uses a redundant style of execution request and will receive multiple, identical replies from all copies, in which the client broadcasts. In practice, each client will wait for the reaction to failure, but reply and ignore all others. Such an approach provides the fastest possible solution. The disadvantage of consuming n times the sources of a fault-intolerant process group. An alternative would have been to subdivide the search so that each server performs the work. Here, the client would combine fails instead of replying, a condition Isis interfaces have been developed exist for UNIX-workstations responses from all the servers, in Isis. Repeating the request if a server readily detected. 

For C, C++, Fortran, Common Lisp, Ada and Smalltalk, from all major vendors and ports of Isis and mainframes as well as for Mach, Chorus, ISC and is represented in the SCO UNIX, the DEC VMS system, binary format used by the sending (if necessary), automatically and Honeywell's machine, Lynx OS.  Data within messages and converted to the format of the destination upon reception transparently.

6. Who uses Isis, and how?

This section briefly reviews several Isis applications, looking at the roles that Isis plays.

6.1 Brokerage: a number of introduction

Underlying Isis users are concerned Figure 11 illustrates employed with financial computing systems such as the one cited in the perspective in which groups one. Such a system, now seen from an internal by the broker become streams of data evident the services.

The architecture is a client-server in which the servers filter and analyze and reorganize themselves so that service is not interrupted. Fault-tolerance here refers to two very different specific aspects of the application. 

First, financial systems must rapidly restart after failed components or hardware failures. Second, software system functions rebooting that require fault-tolerance at the level of files or database, such as a guarantee that after Isis was designed, a file or database manager will be able to recover local data files at low cost.

To address the first sort of problem, but includes several tools for solving the latter one. Generally, information periods the approach taken is to represent key services using process groups, replicating service state so that even if one server process fails the other can respond to requests on its behalf.

When n service programs are operational, one can often exploit the redundancy to improve response must pay for fault-tolerance, begins to outweigh failures. The benefit is time. Thus, rather than asking how much such an application pilate questions concurrency, something concern the level of replication acceptable at which the overhead assuming and the minimum performance approach.

k component Fault-tolerance of a side-effect of the replication computing. A significant communication theme in financial primitives is the use of a subscription/publication style.

The basic Isis numbing over the do not spool messages for future replay, hence an application ti_s functionality. a dynamically predicting varying collection system, the NEWS facility, has been developed. A final aspect of brokerage systems to support that service.

A firm may work with dozens or hundreds of financial models, instruments needed to be waded under varying market. Market behavior for the financial will be only a small subset consists of a processor of these services at any time. Thus, systems conditions of this sort generally pool on which services execution and load a remote can be started as necessary, balancing mechanism. And this creates a need to support an automatic of typical network complicates. 

The heterogeneity by introducing or require special pattern matching processors (i.e. certain programs may be subject to licensing for some specific hardware configuration) described or may simply have been compiled. This problem is solved using the Isis network resource manager, an application later in this section.

6.2 Database replication and database triggers

Although the Isls computation model differs from a transactional model (see also Sec. 7), Isis is useful in constructing distributed database applications. In fact, many, as half of the applications as with which we are familiar, are concerned with this problem. focus on replicating a database for fault-tolerance the database system or to support need not be.

Typical uses of Isis in database applications concurrent searches for improved performance. 

In such an architecture that aware Isis is present. Database clients access the database through a layer of software that multicasts updates. Servers are supervised to the set of servers, by a process while issuing queries directly to the least loaded server. Clients of load changes in the server updates. Isis supervises the restart of a failed server from a checkpoint and log of subsequent addresses.

It is interesting to realize that even such an unsophisticated need among database users would require extending standards. Beyond database replication, approach to database replication support addresses a widely perceived such as this long run. Of course, comprehensive execution for applications Isis to support a transactional model and to implement the XA/XOpen Isis users have developed.  By monitoring WAN databases by placing a local database system traffic on a LAN, updates of importance to each LAN in a WAN system. Remote users can be intercepted monitors for incoming control the update and distributed through the IsIs WAN architecture. To avoid costly updates and applies, developers send them to the database server as necessary.

Concurrency problem of applications, such as these normally partition the database so that the data associated with each LAN is directly updated only from within that LAN. On remote LAN's, for many applications. A trigger is a query that if such data can only be queried and could be stale, but this is still sufficient. 

A final use of Isis in database is incrementally specified evaluated becomes settings is to implement database triggers against the database as updates true. For example, position exceeds condition occur, causing some action immediately to be sounded. A broker might request that an alarm some threshold. As data enters the financial database by the brokerage, such a query would be evaluated repeatedly. The role of Isis is in providing programs tools for reliably notifying capable applications when such a trigger becomes enabled, and for developing of taking the desired actions despite failures.

6.3 Major Isis based utilities

In the above subsection, we alluded to some of the fault-tolerant utilities that have been built over Isis.

There are currently five such systems.

NeWS: This application supports a collection of communication topics to which users can subscribe with file-system address style a replay of recent postings or post messages. Topics are identified using and it is possible to post to topics on a remote network a "mail notation. Thus, a Swiss brokerage application of messages joins firm might post some quotes to "/GENEVA/QUOT_/IBM@NEW-YORK". It creates a process group for each topic, monitoring posted to it for replay to new subscribers, each such group to maintain a history using a state transfer when a new member NMGR this program manages.

This involves monitoring batch-style jobs and performs load sharing in a distributed into a processor pool setting. And the candidate machines, which are collected scheduling on the pool, job machines are suitable one. This criteria can readily be opposed failed to run batch-style Parallel.

A pattern matching mechanism is used for job placement for a given job (a criteria based on load and available memory be changed). When employed to manage each service critical is used to select services (as system jobs), the program monitors make is an example application and automatically program restarts that uses components of a distributed application NMGR for job placement. This system compiles by fanning out compilation subtasks to compatible [SBM89] provides fault-tolerant NFS-compatible file storage. Replicates files are replied for both to increase performance tolerance; the level of replication meantime is varied depending on the style of access detected any files managed are automatically by the system after a failed node recovers, file replication brought up to date. The file-system reactive control interface applicator or approach conceals · MErA/LOMrrA: lions [MCWB91, environment, monitored from the user, who sees a compatible system for building fault-tolerant META is an extensive Woo91]. It consists of a layer for instrumenting a sensor, a distributed application by defining sensors by the system sensors and actuators. An actuator is any entity capable of taking an action the status of software. User-defined is any typed value that can be polled on request.

Built-in sensors include the load on a machine, and the set of users on each machine. The "raw" sensors layer and hardware components of the system, and actuators extend this initial set. sensors by an intermediate facility. This layer sensors, such actuators of the lowest layer are mapped to abstract a simple database-style interface which also supports an entity-relation fluency and a triggering supports as polling data model and fault-tolerance. and conceals Sensors. many of the details of the physical can be aggregated, for example, by taking the average load on the servers that manage a replicated language, which will initiate a pre-specified database. The interface supports a simple trigger is detected. Terms, called action when a specified for specifying control condition.  Running over Mm'A is a distributed language LoMrrA. LoMrrA code is embedded actions in a high-level interpreter, triggered into the UNIX CSH command. At runtime, LoMrrA by events that can control statements is expanded into distributed finite state machines local to a sensor or system components. A process group is used to implement when a monitored condition aggregates, perform these state transition, and to notify applications and for saving messages. This subsystem arises [MB90] and is responsible for wide-area communication. It conceals to groups that are only active periodically. Communication interface link failures present an exactly-once. 

6.4 Other Isis applications

Although this section covered a variety of Isis applications, over the system. In addition, a systematic review of the full range of soRwate that has been developed to the problems cited above. 

Isis has been applied to systems, reliable to telecommunications replacement switching for the AEGIS and "intelligent aircraft tracking networking" and combat applications, engagement military systems, system, medical control, weather such as a proposed graphics and virtual reality applications, management and resource scheduling seismology, for shared factory automation facilities, and production and a wide-area popular computing prediction computing and storm tracking system at laboratories [1oh93, Thog0. ASC92].

Isis has also proved for scientific as a beam such as CERN and Los Alamos, accelerator, and has been applied to such problems that combine a highly parallel focusing system for a particle with a vectorized management. It should architecture atmospheric a weather-simulation ocean model and resource model and displays output on advanced graphics workstations, facilities. On LAN issues, software for shared supercomputing that although also be noted the paper has focused compose. 

Isls also supports a WAN cited and has been used in WANs as LAN solutions of up to ten LANs. Many of the applications by a reliable, but less responsive, WAN layer. above are structured interconnected.

7 Isis and other distributed computing technologies

Our discussion has overlooked next generation issues that arise in the Advanced Automation System [CASD85], which also uses a process-group model compares the sorts of real-time air-traffic similarly, one might wonder how the Isis execution models. Unfortunately, these are complex technology issues, like the one used in AAS differs from Isis in providing strong real-time control based computing system being developed with transactional model by IBM for the FAA [CD90]. It would be difficult to do justice database execution to them without a lengthy proof.

Briefly, a process that experiences a timing fault in the AAS model could receive messages because the criteria for accepting violations or rejecting if faulty of such will reject, or reject messages guarantees provided that timing assumptions. This can lead to consistency resynchronized it from initiating others accept, uses the value of the local clock.  (e.g. could the clock is subsequently "spread:" nothing that other processes a message is transient a process accept deadline prevents with other clocks) Moreover, the inconsistency which other processes will be maintained. Isis, on the other hand, guarantees that consistency will be achieved.

The relationship and transactional offered between Isis and transactional are order-based focus on isolation of concurrent transactions, but not that real-time delivery. However, where the "tools" persistent between data and rollback (abort) mechanisms, members of groups, failure handling by a database system from one another, in the fact that both virtual synchrony [BHG87]. Systems execution originates models those offered in Isis are concerned with direct cooperation reconfigure and ensuring that a system can dynamically itself to make serializability.

The WAN architecture of isis ii similar to the WAN structure, bet because WAN partition are more common, encourages a more synchronous programming style. WAN communication and link state is logged to disk files (unlike WAN communication), WAN issues are which enables Isis to retransmit messages lost when · WAN partition to suppress duplicate messages. to discuss in more detail in [MB90].

Persistency of data is a big issue in database systems, but a commit of a multicast forward progress when partial failures occur is a form of reliable multicast, while delivery much less so in Isis. For example, serializability and permanence of the transaction being committed in isis provides much weaker guarantees. 

8 Conclusions

We have argued that the next generation of distributed computing systems semantics exceed will benefit from support for process groups and group programming. Arriving the abilities or the reliability would be a difficult problem, application performance development and implementing those semantics for a process group mechanism of many distributed systems. Either the operating system must implement applications of group-structured is unlikely to be acceptable with process groups. 

The Isis system provides tools for programming leads us to the following conclusions. Process synchronized groups should embody strong semantics. A simple and powerful communication, distributed for group membership, and synchronous execution, is a synchronized model can be based on closely but high performance heavily pipelined.

The virtual synchrony approach in which communication using a closely style of execution combines these benefits, synchronous execution safely be relaxed. Efficient protocols, Non-experts model, but deriving a substantial performance benefit when message ordering can have been developed for supporting virtual synchrony. 

This paper is being written as the first phase of the Isis effort approaches system has demonstrated achieves levels the feasibility of a new style of distributed to those afforded a resulting system relatively easy to use.

We feel that the initial in [BSS91], Isis computing suitable for integration incorporate on the same platforms.  As reported of performance comparable by standard technologies (RPC and streams) 

Looking to the future, we are now developing operating [RBG92] systems, an Isis "microkernel' into next-generation a security architecture such as Mach and Chorus. This new system will be a real-time communication that operate on distributed to implement high-reliability suite. The programming model will be unchanged. Group programming could ignite a wave of advances platforms in reliable distributed computing, and of applications developers.

Using current technologies, it is impractical to employ software, self-managing or to develop software for typical replicated after distributed systems, that reconfigures data or simple coarse-grained parallelism, automatically failure or recovery.

Consequently, although current resources deficient distributed, the programmers are severely constrained software who develop software infrastructure. By removing these unnecessary environments obstacles, embody tremendously powerful networks computing, a vast groundswell of reliable application development can be unleashed.

Model-Based Evaluation of Expert Cell Phone Menu Interaction
ROBERT ST. AMANT and THOMAS E. HORTON North Carolina State University and FRANK E. RITTER The Pennsylvania State University

We describe concepts to support the analysis of cell phone menu hierarchies, based on cognitive models of users and easy-to-use optimization techniques. We present an empirical study of user performance on five simple tasks of menu traversal on an example cell phone. Two of the models applied to these tasks, based on GOMS and ACT-R, give good predictions of behavior. We use the empirically supported models to create an effective evaluation and improvement process for menu hierarchies. Our work makes three main contributions: a novel and timely study of a new, very common HCI task; new versions of existing models for accurately predicting performance; and a search procedure to generate menu hierarchies that reduce traversal time, in simulation studies, by about a third. 

1. INTRODUCTION 

There are 2 billion cellular telephones in use today, and this number is expected to reach 3 billion in 2008 [DiPrima 2006]. Cell phones are used for more than making calls; they now include tools for managing contact information, voice mail, and hardware settings, and often software for playing games, browsing the Web, and connecting to specialized information services. The market penetration of cell phones is much higher than that of conventional computers, which raises significant opportunities and challenges for HCI. This article presents techniques for evaluating and improving cell phone usability, in particular the usability of the hierarchical menus that provide access to most functionality aside from dialing and data entry. While cell phone menu interfaces may appear simple at first glance, they pose a nontrivial design problem. Consider the menu hierarchy for the Kyocera 2325 cell phone, the first 25 items of which are shown in Table I. If we count as terminals those selections that open an application (e.g., a game), a list of data (e.g., recent calls), or a set of choices in the cell phone equivalent of a dialog box (e.g., for setting the ringer volume), then this hierarchy contains 98 terminals, reachable through 22 intermediate selections. The longest menu contains 12 items­all associated with the selection of different sounds. The shortest menu contains a single item, for entering a voice memo. Terminals in the hierarchy are up to four levels deep, and the mean number of actions to reach an item (scrolling plus selection), over all 98 terminals, is 13.3, taking on the order of 7 seconds for an experienced user. This menu hierarchy is as large as that of a moderately sized desktop application (e.g., Eudora 5.2 with 103 items). This is not unusual for cell phones; the menu hierarchy for the Samsung MM-A800, which includes a digital camera, contains a remarkable 583 items [Pogue 2005]. Designing menu systems for any platform, including desktop systems, can be challenging, but for cell phones the problem is made more difficult by several factors: -- Discrete selection actions in the form of button presses1 are usually needed to move between menu items, because most cell phones lack more direct selection capabilities (e.g., a mouse or touch screen). -- Cell phone displays are small, allowing only a few menu items to be displayed at a single time. Many cell phones lack functionality for paging up or down, making display limitations even more significant. -- There is less standardization in hardware supporting menu traversal for cell phones than for desktop machines. Some phones have two-way directional buttons, others four-way; some have a labeled "Menu" button, while others rely on a button with overloaded functionality. Button placement can vary significantly, with "Cancel" and "OK" buttons reversed from one phone to another. If interfaces are developed for the lowest common denominator, independently of specific hardware (which is common practice at the mobile application level), then even cell phones with elaborate interaction support become less efficient. 1 We use the terms "button presses" and "key presses" interchangeably.

These factors suggest that cell phone menu interfaces deserve close analysis, and that they need specialized techniques for their development and evaluation, which this article takes up in two parts. In Section 2, we describe an empirical study of the traversal of cell phone menus, along with three models for predicting user performance: a Fitts' law model [Fitts 1954], a GOMS model [John and Kieras 1996a, 1996b; Kieras 1999], and an ACT-R model [Anderson et al. 2004]. All the models give good accounts of qualitative patterns in user behavior, and the latter two models give good to very good quantitative predictions of behavior, at both aggregate and detailed levels of analysis. In Section 3, we use our empirical results to define a novel evaluation metric for the efficiency of cell phone menu traversal. We define a search procedure that generates improvements to a menu hierarchy with respect to a given set of characteristic user profiles. This article makes several contributions to the HCI literature: a novel and timely study of a very common new HCI task (menu use on cell phones), new models for accurately predicting performance on this task, and a simple, theoretically motivated search procedure that generates menu hierarchies that reduce traversal time in simulation studies by a third, which should be generally applicable to all menu-based systems.

2. A PERFORMANCE STUDY 

Our interest is in expert (i.e., practiced and error-free) use of cell phone menu systems. For control purposes, it was not feasible to collect data from experienced users on their own cell phones, with all the potential differences in hardware and software. As a compromise, we had users practice a small number of tasks, so that all tasks could be remembered easily, and then carry them out on a single cell phone. Though restrictive, these conditions give a reasonable starting point for an empirical study and model validation. We used a Kyocera 2325, as shown in Figure 1. At the top level of its internal menu, the Kyocera display shows a single selectable icon. The OK button selects the current item; on the four-way scrolling button, RIGHT and LEFT move through the item list horizontally. For lower-level menus, three items are displayed at a time, oriented vertically. Each new menu list is displayed with the top item highlighted. The OK button, on the left, is used to select the currently highlighted item in these menus, while the CLR button, on the right, returns to the previous level in the hierarchy. The UP and DOWN regions of the four-way button move through the menu. Downward scrolling is incremental, with items appearing one at a time at the bottom of the screen. 2.1 Procedures We recruited fourteen experienced cell phone users for our study, students who took part for course credit. The first two users acted as participants in a pilot phase of the experiment, in which software for data collection and analysis was tested and procedures were refined; their data were also used in developing (but not validating) the models described in later sections. The remaining twelve users, male undergraduates in computer science, provided the main body of data for the study. All were right handed. All but one used their right hand to hold the cell phone, and all used the thumb of the right hand to press keys. To collect data, we recorded the tone produced by each key press as transmitted through the earphone jack of the cell phone. Collection was initiated by the first key pressed by the participant and ended with the last key pressed. The onset of each key press is detectable by a threshold test on the audio output waveform from the earphone jack, using software we wrote for this purpose. Each tone lasts approximately 0.095 s, during which time the display changes, before the key is released. System responses are much faster than key presses and are treated as occurring within elementary key press actions and not contributing to the duration of user actions. Participants started with a practice stage, in which they familiarized themselves with the cell phone and its menu system. We gave each participant a paper form describing how five terminal menu items were to be reached, as shown in the first column of Table II. Each ">" represents a scrolling action, with commas separating consecutive selection actions. Reaching each of the terminal items (those at the end of each sequence) constituted a task in the study. Participants practiced each task until they could carry it out three times in a row without error. Each trial in the study required reaching one of the five target terminal items without access to the paper form. Tasks were presented to participants in a randomized order. We obtained five correct trials per participant (i.e., without errors or extraneous actions), discarding fewer than 10 trials across all participants, less than 3% of the data. This means that our cleaned dataset contains only OK and RIGHT/DOWN key press actions, 2,280 observations in total (2,280 = 12 users × 5 repetitions × (10 + 9 + 3 + 8 + 8) actions per task). Table II shows the mean duration per task, over all participants in the study. User performance is much slower than for single-level menu selection with a mouse on a standard desktop platform [Byrne 2001], which highlights the importance of specialized models for this task, as we discuss below.

2.2 Models of User Behavior We predicted user performance with three models, each supported by a considerable background literature. A Fitts' law model, a GOMS model, and an ACT-R model were developed independently of each other, based on data from one task carried out by one of the users in the pilot stage of the experiment.2 The three models run in the same software framework that evolved over the course of this research. The framework provides a common specification of the Kyocera cell phone, including the sizes and positions of keys and the distances between them, as measured on the physical device. The framework also supports a common representation of the menu hierarchy shown in Table I. The models use the same software environment that includes a simulation of the cell phone's interface and produces output in a consistent form. 2.2.1 A Fitts' Law Model. Our model is based on MacKenzie's [2003] version of Fitts' Law for one-finger typing for text entry on mobile phones. Movement time in seconds for thumb input is where D represents the distance (amplitude) of the movement and W the width of the target. The value for D in our study was 14.5 mm, which separates the OK button and the DOWN button area, with widths W of 6 mm and 10 mm, as provided by the cell phone specification. This model, as with the other models described below, makes the simplifying assumption that all scrolling actions can be represented by DOWN key presses, even though the first action is a RIGHT key press, with a slightly different size and distance from the OK button. To execute the Fitts' law model for each of the five tasks, a path is generated from the root of the menu hierarchy to the terminal item for the task. Each step on the path is associated with a movement action or a key press action. Durations for all the steps are accumulated to produce an overall task duration. 2.2.2 A GOMS Model. The second model is a GOMS model [Kieras 1999; John 2003]. GOMS methods for task analysis produce hierarchical descriptions of methods and operators needed to accomplish goals; some GOMS models have been strikingly successful in critical HCI domains [Gray et al. 1993]. In our model, a method is defined for each task in the study. All of the methods are automatically generated from the menu hierarchy specification, based on the same path traversals used for the Fitts' law model. Within a method, each step corresponds to the selection of a menu item. The GOMS method for selecting the terminal item Ringer Volume is shown at the top of Figure 2. Each of the steps in this method in turn decomposes into a selection method, such as Select Menu or Select Sound, which involves scrolling until a specific item in the sequence is reached--selection in a menu at a single level. There is one selection method for each menu item, from Select Settings to Select Ringer Volume. All of the selection methods have the same form, as shown in the example at the bottom of Figure 2. Specifications of these lower-level methods are created automatically from a generic template. 2 Preliminary versions of the GOMS and ACT-R models described in an earlier conference paper [St. Amant et al. 2004b] contained minor inconsistencies; these inconsistencies were removed in revision. Performance was altered by no more than a few percentage points. The qualitative behavior of the models and comparisons between them remain unchanged from their earlier description.

Processing in a selection method involves iterating through a sequence of four exhaustive tests of whether or not the target intermediate or terminal item is currently highlighted and whether the finger is on the appropriate key for selection or scrolling. The durations of the steps follow the guidelines established by Kieras [1999] in his work on GOMSL and GLEAN3 (including a version of Fitts Law). Each test in a decision step requires 0.050 s, plus the time to execute any actions in the body of the decision if the test succeeds. Steps that involve key presses last 0.280 seconds plus the duration of tests or auxiliary operations (0.330 seconds in total). Moving to the DOWN key lasts 0.083 seconds (0.133 seconds in total); moving to the OK key lasts 0.113 seconds (0.163 seconds in total). Movement times are based on the movement component of the Fitts' law model in the previous section. The model assumes negligible system response time and that there are no verification steps. Further, the initial visual action to acquire the first menu item occurs before the first key press (timing begins at the first key press), and as the highlighted menu item changes no visual re-acquisition is needed during selection or scrolling activity. Processing is entirely sequential, with no overlapping of steps. Modeling results, based on the description above, are generated by a GOMS interpreter that we implemented specifically for this project. While there would have been some benefit to using existing GOMS modeling tools and environments (e.g., GLEAN3 [Kieras 1999]), we judged that the value of a single simulation and modeling framework (despite its limitations), for the Fitts' law, GOMS, and ACT-R models, and the phone simulation would provide a worthwhile degree of consistency across our evaluation. 2.2.3 An ACT-R Model. The third model is based on the ACT-R 5.0 cognitive architecture [Anderson et al. 2004]. We picked ACT-R as a representative cognitive modeling architecture and as a common choice in HCI work. ACT-R integrates theories of cognition, visual attention, and motor movement and has been the basis for a number of models in HCI (e.g., Ritter and Young [2001]). ACT-R models simulate the time course and information processing of cognitive mechanisms, such as changes of attention and memory retrievals, as well as external actions, such as movement of the fingers. Roughly speaking, ACT-R models provide details that can explain behavior in cognitive terms at a level not addressed by the coarser GOMS representation. In our ACT-R model, a virtual (simulated) display maintains a representation of the current items in the cell phone's menu interface hierarchy. Menu items are presented in a vertical list, and one of the menu items is always highlighted. All items are presented for each list, independent of the physical display size. When an item is selected in the virtual display, the list is refreshed with the appropriate submenu. The model's memory is initialized with a set of declarative memory chunks that represent the parent-child relationships between the intermediate menu items needed to reach terminal items. For example, for the Ringer Volume task, pairs of declarative memory chunks for the Menu/Settings, Settings/Sounds, and Sounds/Ringer Volume relationships are included. Chunks representing the parent­child relationships are generated automatically via traversal of the menu hierarchy specification. ACT-R's model of the hand is initialized with the thumb on the OK button. Procedural knowledge in the ACT-R model consists of eleven productions: -- Find-top-item searches the visual field for a highlighted menu item immediately after a selection action. -- Find-next-item searches for the next item below the one currently attended, immediately after a scrolling item. -- Attend-item causes the location of the highlighted item to be visually attended. -- Encode-item encodes the text for the attended menu item, so that its content (i.e., the name of the item in text form) becomes accessible to the model. -- Respond-select-target fires when the currently highlighted item is recognized as the terminal item. -- Recall-item-association retrieves an association, if it exists, between the currently highlighted menu item and its subordinate item along the path to the terminal item. -- Respond-select-ancestor recognizes an intermediate menu item along the path to the terminal item (i.e., one of its ancestors). -- Respond-continue-down fires when the highlighted item is neither the next item to be selected nor along the path to the terminal item. -- Move-down causes the motor module to press the Down key. -- Select-target causes the motor module to press the OK key on the terminal menu item, ending model execution. -- Select-ancestor causes the motor module to press the OK key on an intermediate menu item.

The model starts with the goal of selecting a specific terminal menu item. The simulation environment shows a single highlighted item. The model first retrieves a target intermediate item to be selected. It then searches for the currently highlighted menu item in its field of view. Once found, the visible item is attended and then encoded, so that its text representation becomes accessible. If the text matches the target item and this is the same as the terminal item, then the model initiates motor actions to move the thumb to the OK button (if necessary) and press it. Model execution completes at this point. If the text matches the target item but it is not the terminal item, then this means that the currently highlighted item is on the path to the terminal. The OK button is pressed and another target item is retrieved from memory. Visual processing repeats as before. If the text of the highlighted item does not match the target item, then motor actions are initiated to move the thumb to the DOWN button and press it. Control is transferred to the visual search action, as before. In the model, manual scrolling actions can trail behind visual processing by an unspecified amount (determined by processing in the model such as memory retrievals); the visual and manual modules become synchronized when a new menu is presented. User errors, such as pressing an incorrect key, are not modeled. Model execution is deterministic, with no noise parameters used. Our model is defined in the ACT-R modeling language, but its execution depends on an extension to perceptual-motor processing in the architecture. The perceptual and motor components of ACT-R 5.0 have some bias toward desktop activities, such as selecting menu items with the mouse and navigating through windows and dialog boxes [Anderson et al. 2004; Byrne 2001]. In ACT-R, the keyboard is represented as an array of locations. Neighboring keys are a unit distance apart in a rectilinear arrangement, and each key has unit width and height. Standard key presses are modeled as finger movements from locations on the home row to the location of a target key. To handle interaction with a cell phone keypad, more flexibility is needed in models of finger movements and the keyboard. We extended the ACT-R environment representation to support a layout-based keypad in which the size and placement of keys can be specified individually. The new representation allows us to build specifications of different cell phone keypads that can be integrated with ACT-R motor processing in a straightforward way. Fingers are modeled as moving between locations, which, in the case of this experiment, are key locations, but may be arbitrary if needed. 2.3 Model Performance We can describe the performance of the models at two levels: the accuracy with which the models predict the overall duration of tasks, and the accuracy of their predictions of the duration of individual actions. These two levels are discussed in the sections below. Other factors commonly explored by modeling, such as learning behavior and the occurrence of errors, are excluded by the design of the experiment. 2.3.1 Task-Level Performance. Table III shows summary model performance and user data broken down by task. Figures 3 through 7 show more detailed views of the same data in graphical form. Both the GOMS and ACT-R models give good approximations of user performance. GOMS predictions are within the 99% confidence interval for mean overall task duration for all target items except View Day. ACT-R predictions are within this interval for two of the target items. The Fitts' law model does less well, for reasons that are worth discussing. Many models of cell phone interaction, such as keypad dialing and one-finger text entry [MacKenzie 2003], have been based on Fitts' law, which motivated this aspect of our evaluation. Our Fitts' law model performs relatively poorly, despite the success of such models elsewhere. The Fitts' law model produces times that are about half of the observed times. This is not surprising--much of the activity of this menu selection task is outside the scope of the model. Silfverberg et al. [2000] describe a comparable example of where Fitts' law models break down, in a discussion of text entry on mobile phones. For some cell phones, text entry is aided by lexicon-based word disambiguation. While typing, the user ordinarily refers to the display in order to decide whether the system has correctly disambiguated the word being typed. In text entry, such cognitive processing may not be needed by expert users familiar with the disambiguation system. In this menu selection task, however, we assume that users confirm their actions. In other words, significant visual and cognitive processing is necessary at each step in the process, but this is not captured by the Fitts' law model (though it is represented in the GOMS and ACT-R models). The consistent linearity of user performance across tasks in Figures 3 through 7 suggests a straightforward way to measure the predictive power of the GOMS and ACT-R models through comparison with a least squares linear model. Because the models' predictions apply to each task, a harsh way to test them is to compare them to a linear regression model fit to data from each task. Table IV shows the coefficient of determination, R 2 , for these linear models for each button press. The linear model predicts time T by the number of key presses k plus a constant. The remaining columns show analogous values for the ACT-R and GOMS models. A linear model can also be fit to the aggregation of performance data for all tasks and all users. This aggregate linear model has an R 2 of 0.834. Although neither the GOMS nor the ACT-R model accounts for as much variance as a general linear equation, both are close; the comparable values are 0.809 for the GOMS model and 0.796 for the ACT-R model. The aggregate linear model has appealing conceptual and computational simplicity; we use Eq. (2) in Section 3 for just this reason. As a general model of performance, though, it has several shortcomings in comparison with the GOMS model, the ACT-R model, and even the Fitts' law model. First, the latter are a priori models--they were not tuned specifically to the data. Second, as we discuss later in this section, the models give predictions at a more detailed level than the linear model can provide, including the ability to carry out actions to produce this behavior. Third, and most important, the GOMS, ACT-R, and Fitts' law models have theoretical underpinnings that give them explanatory power. In the case of GOMS, performance is explained by the specific tasks that are represented, the hierarchical structure in which they are combined, and dependence on a cognitive processing framework that provides specific timing predictions (e.g., for Fitts' law movements). The ACT-R model extends the level of detail in its explanations, in accounting for the interval between actions by explicit visual processing and memory retrievals, and in modeling visual processing and motor actions as proceeding in parallel for scrolling actions but synchronizing with selection actions. This allows performance on multiple tasks to be based on models of single tasks (e.g., Salvucci [2001]). Like all model-generated explanations, these are provisional and subject to further testing. In particular, the predictiveness of a linear model raises a warning flag: because task-level performance accumulates the durations of actions in sequence, almost any reasonable cumulative function is likely to have the same qualitative shape. The accuracy of the models over tasks of different durations suggests that the models have some generality, but this is far from conclusive. If, as with computing, the purpose of modeling is insight rather than numbers [Hamming 1962], we should look more closely at our results. 2.3.2 Action-Level Performance. Table V shows the predictions of the mean time between user key presses that each model makes over all the menu selection tasks. There are three different categories: all actions aggregated over all tasks, only selection actions, and only scrolling actions. The ACTR and GOMS models both provide good predictions at this level, with differences of at most 0.045 s, about 8% error. Although the Fitts' law model is qualitatively correct in predicting that selection actions take longer than scrolling actions, it underpredicts user reaction time in all categories--our discussion in this section will therefore mainly focus on the ACT-R and GOMS models. The results in Table V are limited in two ways. First, they distinguish only between classes of actions in the abstract, independent of the task context in which they are executed. Second, the results neglect the inherent variance in user performance. Even under identical task conditions, the actions of different users (or a single user in different trials) may have different durations. We address these two limitations in the remainder of this section. One way to describe the execution of a menu navigation task is as a simple repeated pattern: each task is carried out through a number of scrolling actions followed by a selection action, repeated until a terminal item is reached. We define a selection run as a sequence of scrolling actions leading up to a selection action. As in our per-task analysis, user performance is basically linear for the overall duration of selection runs and is well predicted by both the GOMS and ACT-R models with respect to overall duration. What is more interesting is differences in duration for actions of the same type. In selection runs, the first scrolling action after a selection action lasts much longer than subsequent scrolling actions, as shown in Figure 8. A general linear model (see Eq. (2)) would be a straight line at 531 ms per action. Two factors appear to contribute to the longer duration of the first scrolling action. One factor is movement time, in that for the first scrolling action, the thumb must move from the OK key to the DOWN button; movement between keys is unnecessary for further scrolling. The other factor is visual processing: each time that a selection action takes place, a new set of menu items appears on the display and must be read. All of the models include a movement component and thus reflect the general qualitative pattern, but they vary in how they handle visual processing. The ACT-R model carries out an explicit visual search, which occurs in parallel with the motor movement. The GOMS model does not include an explicit visual processing step, but each selection entails decision-making tests as well as a call to a new method, adding to the duration of motor movement. Both models produce slight underpredictions of the duration of the first scrolling action. (An alternative explanation for this pattern is offered by the EPIC architecture but is based on different low-level cognitive assumptions [Hornof and Kieras 1997].) A less obvious pattern is also present in Figure 8. For users, the first scrolling action lasts the longest, the second the shortest, and all succeeding actions in between. We believe that three factors explain the increase in duration between the second and remaining actions. The first factor is an environmental constraint on users' visual processing. Because only three menu items are presented on the display at a time, we can expect the duration of the fourth and succeeding scrolling actions to be slower than the second and third, because the items to be traversed are not immediately available for visual processing. The second factor is a possible strategy that users took in dealing with menus that are known, via practice, to be long: users may quickly execute several scrolling actions with less attention to the display, until the approximate region of the target item is reached. The third factor is parallelism in visual processing and motor action. Related experiments on menu selection with a mouse [Byrne 2001] suggest that eye movement is not strictly synchronized with motor activity in the discrete menu traversal actions of our domain; this means that the eyes may scan ahead of the items being highlighted by button presses. The first factor is not reproduced by our simulation environment and the second is not yet included in the model. The third factor, parallelism in motor and visual behavior, is represented in the ACT-R model. This parallelism accounts for the increase in duration after the fourth scrolling action, as the duration of motor actions dominates that of visual processing. The exact point in the user data at which the increase occurs is not captured by any of these models; neither is the remaining variability in the duration of scrolling actions. Finally, we note that the cognitive and visual processing component of actions in selection runs is much higher than the movement component. The Fitt's law model provides a baseline for movement-only duration, but underpredicts the duration of user actions by a factor of two to three. The GOMS and ACT-R models, for reasons discussed above, come much closer to user performance. Overall, the mean error for mean action duration during all selection runs is about 14% for the GOMS model, 19% for the ACT-R model, and 60% for the Fitts' law model. Our results so far show that the GOMS and ACT-R models give accurate predictions of the mean duration of user actions, when actions are grouped into classes or, as in the analysis of selection runs, associated with a specific task context. These predictions, however, give no information about the accuracy of the models for specific instances of actions. In evaluating model predictions at the action level, the issue of inherent variability in user performance is perhaps the most important. Two common measures of error that give insight into this issue are root mean squared error (RMSE) and mean absolute error (MAE).

Table VI shows RMSE and MAE for the three models, for all actions and for the categories of scrolling and selection actions. The prediction error for the GOMS and ACT-R models, per action duration, is much higher than for mean duration, about 42% of the mean action duration (as given in Table V) for the RMSE measure and 32% for MAE. The values are similar for the subcategories of scrolling and selection actions. By these measures, the models are very close in performance, with neither GOMS or ACT-R having an obvious advantage. While these values of RMSE and MAE are disappointingly high for the GOMS and ACT-R models, it is worth asking how much they might be improved. As in our task-level analysis, we can define a post hoc model, based on the user data, for comparison with the models we have built. We begin by observing that the predictions of the models abstract away performance differences between individual users and across trials. For example, a model will give the same predicted duration for the sixth action in the Ringer Volume task, regardless of which of the twelve users or which of their five trials is involved. The variance in the 60 data points per unique action, in context, gives rise to the error measured in Table VI. How well would an optimal model perform? For a sample of data points, the best estimator with respect to mean squared error is simply the mean of the sample. The best post hoc model (with respect to least squares error) thus returns the mean duration for each unique action, over users and trials. The error for this model is shown in the MD (Mean Duration) columns of Table VI. These errors are lower than but still relatively close to those produced by the GOMS and ACT-R models. As percentages of mean action duration, the models might improve from 42% to 35% with respect to RMSE and from 32% to 26% for MAE. This comparison suggests that the GOMS and ACT-R models are performing almost as well as is possible in predicting user behavior at the action level, given the variance in the user data. 2.4 Discussion All of the models we have presented have proved robust in our analysis, though at a sufficiently detailed level they break down (as all models do). Our results indicate that detailed, rigorous models of low-level interaction with cell phones is possible, and that such models make good predictions. Aside from the use of this work as a possible exemplar of the application of cognitive modeling techniques to HCI evaluation, we can note a few observations. Modelers need to consider the trade-off between modeling effort and the value of increasingly veridical results. The GOMS model developed here is as good as or better than the ACT-R model, and was much cheaper to build. For modeling efficiency, a reasonable heuristic is to apply simple formalisms to model simple procedures. This is especially relevant if the simple formalism can predict all the observable information or all the needed behavior. All the data we have in this study (keystroke times by expert users) can be predicted by both ACT-R and GOMS, though in other situations (e.g., if we had eye-tracking data and wanted to predict eye movements or to model concurrent tasks), GOMS would be at a distinct disadvantage. Further, GOMS offers considerable flexibility in modeling. A coarser formalism does not necessarily imply stricter constraints on modeling, which is perhaps an unintuitive observation; rather, the reverse can be the case. In our GOMS model, for example, the specific ordering of decision steps, as shown in Figure 2, is not governed by cognitive constraints. A different ordering (e.g., one that tested whether an OK key press was appropriate before rather than after scrolling) would have produced different predictions. It turns out, in our case, that user behavior is sufficiently regular that the GOMS model we developed for a single user's behavior generalized very well to a larger sample; if this had not been the case, the modeling flexibility we describe would not have been helpful. Our ACT-R model, for the same task, does not allow such direct fine-tuning to be carried out in the same way, because of tighter architectural constraints on the interactions between visual and motor actions. The remaining differences between the models' predictions and the data suggest further improvements to the models are possible. Most importantly, the comparison in Figure 8 shows that only the ACT-R model starts to account for the faster second keystroke, and none of the models predict this (or the later changes) very well. There are limitations to this work so far, aside from model performance. For example, many cell phones have additional interaction features, such as shortcut menus and non-linear graphical icon displays, that are not captured by the models we have built. Further studies, perhaps extending to include novice users, could take error types and error distributions into account, to help extend the range of application of these models. We believe, nevertheless, that our work lays out clear directions for future research. One issue we have begun to explore is the performance differences between the GOMS and ACTR models. As can be seen in the evolution of cognitive modeling architectures such as ACT-R and EPIC [Kieras and Meyer 1997], there is considerable overlap in basic assumptions about the way that perceptual-motor constraints should be modeled [Byrne 2001; Kieras 2002], and so it is not unreasonable that the models produce similar predictions. Nevertheless, because ACT-R represents behavior at a greater level of detail than GOMS, the ACT-R model is capable of more detailed performance predictions than the GOMS model. That GOMS outperforms ACT-R in some areas of our study is disappointing from a cognitive modeling standpoint, but not entirely unexpected, for the reasons described above. Further, the models were developed independently of each other, and different modeling paradigms and modelers can lead to different opportunities for errors in modeling to occur [Ritter 1991]. There has been recent work toward automatically translating between models at different levels of abstraction, which would help reduce or at least formalize such errors, but this research is in its early stages [John et al. 2004; St. Amant et al. 2004a; Ritter et al. 2006; Salvucci and Lee 2003]. 

3. USER PROFILES AND SEARCH 

Once models of menu traversal have been built, the models can be applied toward improving menu hierarchies so that end users can traverse menus more quickly. This is a key concern for developers who may be less interested in modeling theory or model development than in the pragmatic issues of increasing usability. An evaluation of a menu hierarchy independent of usage patterns would be uninformative: different users choose different items, and items are chosen with varying frequency. In other words, different usage patterns favor different designs. We define a user profile to be a probability distribution over the set of terminal items in a menu hierarchy that specifies the probability of each terminal being chosen, relative to the entire set. Each user profile is also associated with the frequency that the menu system is accessed. For the entire population of users of the menu hierarchy, there may be many different user profiles, some more common than others, a distribution captured by the coverage of individual profiles. As an example, imagine that 20% of the users of a given cell phone access only two items, Recent Calls and View All Contacts, each on average twice a day. In the probability distribution of the profile for this set of users, these two items have probability 0.5 and all others have 0.0; the coverage of the profile is 0.20; and its frequency is 4 (a value that becomes meaningful in the context of the per-day usage values of other profiles). In formal terms, the design problem involves the construction of a mapping (in the form of a hierarchical ordering h) between T , the set of terminal menu items, and U , the set of all user profiles defined on T . A reasonable evaluation measure for a given menu hierarchy h is its efficiency: the expected cost of reaching a terminal item. This turns out to be straightforward to represent. Expected cost is given by where p(t) is the probability of the occurrence of a specific terminal t, and ch (t) is the cost of reaching terminal t in hierarchy h. In some situations, it may be possible to estimate p(t) directly through usage statistics across user profiles. This would mean maintaining a local log of menu selections on individual cell phones, to be uploaded opportunistically to a central repository, or making these local actions visible remotely as they are carried out. If this is not practical due to storage or bandwidth constraints, an alternative is possible. We can express the probability p(t) as follows. That is, the probability of the occurrence of t is the conditional probability of its occurrence in a specific user profile u, scaled by the probability of u and summed over all user profiles. The conditional probability p(t|u) is given by the distribution associated with each user profile as described above. Values for p(u) can be estimated from the coverage and frequency of a profile at the time the profile is assigned to a user. In practice, we can imagine individual users being asked questions about their cell phone usage when they are assigned to a specific user profile: how often they will access their cell phone's menu system and the types of functions they expect to use. The trade-off, compared with direct sampling of p(t), is between accuracy and resource demands. All that remains is to define a specific cost function ch , which we can do with our study results. For pragmatic reasons, we use the easiest metric available to compute cost, the linear regression given in Eq. (2) (the GOMS or ACT-R model could have been used, with comparable accuracy but with a significant increase in processing time). The factors that make the linear regression less appropriate for modeling do not apply here. Our choice for ch means that EC(h) produces the expected duration of choosing an arbitrary terminal menu item in hierarchy h. This measure can be used by an automated search algorithm to identify alternative designs of the menu hierarchy that improve user performance. A complication is that the automated modification of a menu hierarchy cannot arbitrarily rearrange structure purely for efficiency. Changes should respect the semantic relationships between the items. That is, the item Ringer Volume is under the Settings category rather than vice-versa for good reason. To avoid the difficulties of representing and reasoning about menu item semantics (we leave this for future work), we rely on two search operators that produce only small changes. For a terminal item with non-zero probability, these operators can be applied: -- Promote item moves an item to the beginning of its menu list, to reduce scrolling time. -- Promote subtree moves an ancestor of the item up one level in the hierarchy, to reduce the number of intermediate items that must be selected to reach the terminal. An item or subtree rooted at an ancestor may only be promoted once. Even with these constraints, the search space size is exponential in the number of target items with non-zero probability in any profile (e.g., if all non-zero items in a user profile are in one menu list, then all permutations of these items will be considered). Exhaustive search is thus impractical for the phone hierarchy shown in Table I; for just the menu containing 12 items mentioned in Section 1, half a billion permutations are possible. A best-first search algorithm, however, gives good results after as few as 100 steps. 3.1 Results Ideally, we would be able to validate the search procedure based on real user profiles found in the most commonly used cell phones. We have been unable to acquire such data, unfortunately. Lacking real cell phone user profiles, we can only illustrate the search procedure in practice, but our results are promising.

Based on the Kyocera menu hierarchy, we defined random profiles of different sizes, where size refers to the number of non-zero probability menu items contained in the profile. The probabilities for each profile were drawn from a uniform random distribution and normalized. Because these profiles were randomly generated, we used only a single profile for the search, rather than composing arbitrary probabilities from different random profiles. These profiles approximate profiles for spreadsheet usage [Napier et al. 1992] and modeling languages [Nichols and Ritter 1995]. Table VII shows the results for user profiles of size 20, 30, and 40 terminals. In each case, 10 different random profiles were generated for each size, and a best-first search, bounded at 500 steps, was applied to produce improvements. The cost values are means of the time estimates produced by the linear model. The last column gives the time savings in traversing the reordered menus, as a percentage of the duration of the traversals in the original menu hierarchy. Because these results are based on random probabilities of accessing menu items, rather than actual user experiences, they can only be viewed as suggestive. Anecdotal evidence from industry contacts indicates that performing usability studies on menu hierarchies is not common practice. We expect that with improvements in data collection, however, this approach may help to make cell phones more efficient in the future. Targets for future research include examining the plausibility of a uniform distribution for selectable menu items in user profiles, more efficient search to optimize menu layouts, application to other types of menu layouts, and the inclusion of other factors (e.g., profile size) in cost computations. 3.2 Discussion We have presented formulas and a search algorithm to show how menu efficiency can be improved by about a third. The modifications to the menu hierarchy produced by the search have the effect of reducing the depth of the hierarchy and increasing the length of individual menus. This was a simple change, but clearly one that could be applied to at least one commercially available phone. It could plausibly be applied to other systems with similar menu structures. The general approach laid out in this section is related to two areas of HCI other than cognitive modeling, both of which provide opportunities for further research. The first area is adaptive user interfaces. The issue of finding the best menu hierarchy for a given user profile is separate from that of deciding when the menu structure should be put in place. Our discussion in this section assumes that a static hierarchy is associated with each user profile, even if new usage data were to become available over time. If such data were recorded over time for individual users, then a new search could be carried out incrementally to find improved menu hierarchies. This function should not be performed lightly, but one now quite real possibility is to treat the automated adaptation of the menu hierarchy as a customization option that users can select at their own discretion, whenever they choose. It should also be possible to incorporate a theory of learning that could predict when to do this and the costs involved in learning the new menu structure. The second related area is support for navigation. A menu hierarchy is a small, restricted information space in comparison with other spaces such as the World Wide Web. The modifications explored by the search procedure are only a small subset of possible transformations that might be applied to an interface. Nevertheless, some of the same conceptual issues apply to the analysis of navigation in general. For example, usage frequency could be used for improving navigation on a web site by promoting links upward toward the site entry page and move specific links closer to the top of their pages [Ritter et al. 2005]. In practice, the most effective approach to navigation redesign addresses the semantics of the information space rather than focusing only on its surface organization and presentation [Young 1998]. For menu hierarchy modification, this implies that greater potential benefits can be gained from examining the semantic relationships between menu categories and menu items than their ordering. The most relevant research along these lines is Pirolli's work on optimalforaging theory and information scent [Pirolli 1997, 2003]. Optimal-foraging theory explains behavior adaptations in terms of resource availability and constraints. In its application to menu navigation, information scent is a metaphor for visible semantic cues that lead users to information they seek. Pirolli has developed an extension of ACT-R, called ACT-IF, to evaluate a foraging model of information navigation. ACT-IF relies on a spreading activation network to capture associations in memory processing. The models described in our article are based on the assumption that associations between menu items such as Sounds and Ringer Volume can be directly retrieved from memory by an expert user. A more general model, based on ACT-IF, might be able to explain the strength of these associations, based on measures of semantic distance. With such flexibility in representation, it would be possible to explore additional modeling issues, such as how novice users might traverse an unfamiliar menu hierarchy, which paths through the hierarchy are more likely to result in errors, and how renaming or recategorizing menu items could influence navigation performance more than just reordering. 

4. CONCLUSION 

In this article we have described a set of evaluation concepts and tools to support cell phone menu design. The GOMS model is able to predict user performance very well. The ACT-R model performs almost as well. It took more effort to create, but also provide more detailed predictions and could be used for a wider range of analyses. Although our work has relied on a simpler performance model, both of these models could be used by a simple, efficient algorithm to optimize the redesign of cell phone menus. The redesign could let users on average perform their tasks about 30% faster, based on plausible assumptions about usage. This menu redesign approach is simple; we believe it is simple enough to be taught to and used by designers. This approach is based on knowing users (through the models) and knowing their tasks. In its simplest form, the approach is to reorder the menu items to put the most commonly used tasks earlier and higher in the hierarchy. Where users' task frequencies are not known or vary widely between users, it appears reasonable to allow the system to reorder itself upon a user's request after a sufficient break-in period. Of course, the semantics of the task and the semantics of the task titles will have a role to play as well, which we did not explore here. Others are working with ACT-R to create models that start to take account of this aspect of interaction [Pirolli 2003]. These models and the optimization algorithm bring together several interesting aspects of human behavior and show how a simple AI algorithm can help in HCI design. It also gives rise to both theoretical and practical implications. Theoretically, novice user actions, learning, error recovery behavior, performance under stress, and generality across different devices are now areas ripe for further exploration. Having the models in hand also let us explore and explain new regularities in user behavior, such as the variations in key press time shown in Figure 8. From a practical standpoint, developers have models that are ready for use -- these models are general enough that they do not require cognitive modeling expertise or programming skill to apply them to different traversal tasks, in different menu hierarchies, or on different cell phones. Our longer-term goals for this research include the application of modeling techniques to provide insights into usability issues [Nichols and Ritter 1995] and the development of better cognitive modeling tools for evaluating and designing more general classes of user interfaces [Ritter et al. 2006; St. Amant et al. 2004a]. We believe that as modeling concepts and techniques become more accessible to HCI developers, they will become increasingly significant in their contribution to improving user interfaces. Wide application of the menu design approach in this article could, for example, save significant amounts of time. If 2 billion users were to use their cell phone menus every day for just three seconds, our improvements could save almost 30 years of user time per day.

An Integrated Approach for Modeling Learning Patterns of Students in Web-Based Instruction: A Cognitive Style Perspective. SHERRY Y. CHEN and XIAOHUI LIU Brunel University

Web-based instruction (WBI) programs, which have been increasingly developed in educational settings, are used by diverse learners. Therefore, individual differences are key factors for the development of WBI programs. Among various dimensions of individual differences, the study presented in this article focuses on cognitive styles. More specifically, this study investigates how cognitive styles affect students' learning patterns in a WBI program with an integrated approach, utilizing both traditional statistical and data-mining techniques. The former are applied to determine whether cognitive styles significantly affected students' learning patterns. The latter use clustering and classification methods. In terms of clustering, the K-means algorithm has been employed to produce groups of students that share similar learning patterns, and subsequently the corresponding cognitive style for each group is identified. As far as classification is concerned, the students' learning patterns are analyzed using a decision tree with which eight rules are produced for the automatic identification of students' cognitive styles based on their learning patterns. The results from these techniques appear to be consistent and the overall findings suggest that cognitive styles have important effects on students' learning patterns within WBI. The findings are applied to develop a model that can support the development of WBI programs. 


1. INTRODUCTION 

With the rapid development of information technology, the World Wide Web (Web) contains an enormous amount of information [Liaw and Huang 2006]. In particular, there has been considerable growth in the use of instructional materials over the Web [Yen and Li 2003]. Web-based instruction (WBI) has become increasingly attractive to educational settings both for financial and technological reasons [Brotherton and Abowd 2004]. These include easy updating of the material [Scarsbrook et al. 2005], remote access from everywhere and at any time [Anido et al. 2001], presentation with multiple media such as text, audio, graphics, video and animation [Masiello et al. 2005], and the realization of a learner-centered design approach [Jolliffe et al. 2001]. The learner-centered design is especially important because WBI programs are used by a diverse population of learners who have far more heterogeneous backgrounds in terms of their background, skills, and needs [Soloway and Pryor 1996; Chen and Macredie 2004]. This type of design argues that the development of an instruction program should be based on the learners' point of view [Soloway et al. 1996] and should address the needs of learners [Quintana et al. 2000]. Paying attention to learner diversity has been shown to increase student motivation to learn which, in turn, may lead to improved learning performance [Larkin-Hein and Budny 2001]. Therefore, individual differences arguably become an important consideration. A number of learnercentered studies have shown that individual differences have a strong impact on the use of instruction technology [Marchionini 1995]. An analysis of existing pedagogical studies also confirms that the successful usage of instructional technology depends on the technology itself and the learners' individual characteristics [Chou and Wang 2000]. For these reasons, research into individual differences has mushroomed over the past decade. The examined differences include cognitive styles [Workman 2004; Chen and Macredie 2004], gender differences [Beckwith et al. 2005; Roy and Chi 2003], and prior knowledge [Wang and Dwyer 2004; Mitchell et al. 2005]. Among these differences, cognitive style has been identified as one of the most pertinent factors because it refers to a user's information processing habits, representing an individual user's typical mode of perceiving, thinking, remembering, and solving problems [Messick 1976]. It has also been suggested that teachers should assess the cognitive styles of their students in order to design instructional strategies for optimal learning [Lee 1992]. In this vein, this study investigates a specific research question, "what are the effects of students' cognitive styles on their learning patterns within a Webbased instruction program?". Over the past five years, this issue has been investigated by a number of studies (e.g., Calcaterra et al. [2005]; Liegle and Janicki [2006]). While their results are useful, they only represent the tip of iceberg of what might be obtained by using advanced intelligent technologies, one of which is data mining. Data mining, also known as knowledge discovery in databases [Fayyad and Uthurusamy 1996], is an interdisciplinary area that encompasses techniques from a number of fields, including information technology, statistical analyses, and mathematic science [Bohen et al. 2003]. A major function of data mining is to help analyze, understand, or even visualize the huge amount of data stored in databases, data warehouses, or other information repositories [Li and Shue 2004]. Recent studies suggest that data mining is a useful tool for analyzing Web usage data because it can discover regularities and hidden patterns in data [Cho et al. 2003; Ozmutlu et al. 2002].Therefore, we choose to use data mining to analyze the learning patterns of different cognitive styles in our study. The article is structured as follows. In Section 2, we briefly highlight previous work on cognitive styles in WBI and present related research on data mining. Section 3 describes the methodology used to conduct the experiment and the techniques applied to the analysis of corresponding data. Subsequently, the experimental results are presented in Section 4. It then progresses to Section 5 where we discuss the effects that cognitive styles have on students' learning patterns. Section 6 presents a mechanism to help designers develop WBI programs that can accommodate the preferences of both field-independent and field-dependent learners. Finally, conclusions are drawn and possibilities for future work are identified in Section 7.

2. BACKGROUND 

This section starts with a summary of the empirical findings about the effects of cognitive styles on WBI, followed by an explanation of the rationale for data mining and a review of its existing approaches and applications. 2.1 Cognitive Styles Traditional computer-based instruction programs present information in a linear fashion. WBI programs employ hypermedia techniques, which have great potential for education [Chen and Macredie 2004] and permit much more flexibility in the delivery of instruction [Yamda et al. 1995]. Learners are offered a rich exploration environment and have freedom for navigation. It can be argued that when learners are given the opportunity to move freely through a WBI program, they are able to develop their own learning patterns whose features can reflect their cognitive styles [Chen and Ford 1998; Terrell 2002]. In other words, differences in cognitive styles may lead to distinctive learning patterns. Cognitive styles refer to how individuals prefer to organize and represent information [Riding and Rayner 1998]. There are many dimensions to cognitive styles, such as visualized versus verbalized, right-brained versus left-brained, global-holistic versus focused-detailed, or field-dependent versus field-independent. Among these dimensions, field dependence/-independence has emerged as one of the most widely studied dimensions with the broadest application to problems of education [Messick 1976; Witkin et al. 1977] because it reflects how well a learner is able to restructure information based on the use of salient cues and field arrangements [Weller et al. 1994]. The key issue of field dependence lies in the differences between field-dependent and field-independent learners, which are presented here: --Field Independence. These individuals tend to exhibit more individualistic behaviors since they are not in need of external referents to aide in the processing of information. They are more capable of developing their own internal referents and restructuring their knowledge, are better at learning impersonal abstract materials, are not easily influenced by others, and are not overly affected by the approval or disapproval of superiors. --Field Dependence. These individuals are considered to have a more social orientation than field-independent persons since they are more likely to make use of externally developed social frameworks. They tend to seek out external referents for processing and structuring their information, are better at learning material with human content, are more readily influenced by the opinions of others, and are affected by the approval or disapproval of authority figures [Witkin et al. 1977]. A number of studies investigate the relationships between the degree of field dependence and students' leaning patterns within WBI. Table I presents a list of 11 recent studies published from 2000 to 2005. As shown in this table, it is still inconclusive whether field-independent and field-dependent students have different learning patterns. In addition, these studies mainly relied on traditional statistics which, in turn, might produce insufficient data analysis. A lack of deep analysis might cause some relevant relationships to be ignored, for example, what dominant learning patterns appear in WBI and whether cognitive style is a major factor influencing students' learning patterns or if other human factors, such as prior knowledge and gender differences. To conduct such a deep analysis, there technology influence learning patterns is needed that combines statistical rigor and computational power to analyze, summarize, and interpret data effectively. Data mining is one such technology. 2.2 Data Mining Data mining is the process of extracting valuable information from large amounts of data [Hand et al. 2001]. The main difference between statistical analyses and data mining lies in the goal that is sought. The former is often used to verify prior hypotheses or existing knowledge in order to prove a known relationship [Moss and Atre 2003], while the latter is aimed at finding unexpected relationships, patterns, and interdependencies hidden in the data [Wang et al. 2002]. As opposed to traditional experiments designed to verify a priori hypothesis with statistical analyses, data mining uses the data itself to uncover relationships and patterns. In doing so, hidden relationships, patterns, and interdependencies can be discovered, predictive rules can be generated, and interesting hypotheses can be found. These are the advantages of data mining [Hedberg 1995; Gargano and Ragged 1999]. Data mining can be used to achieve different types of tasks. Based on the nature of the information extraction, these tasks can be broadly divided into three major categories: clustering, classification, and association rules [Chen
ACM Transactions on Computer-Human Interaction, Vol. 15, No. 1, Article 1, Pub. date: May 2008.

Results Field-independent students favored using the index. Conversely, field-dependent students preferred to use the map. Field-dependent learners in the breadth-first version performed better than those in the depth-first version. Conversely, field-independent students in the depth-first version outperformed those in the breadth-first version. Field-independent students had better performance in the Internet treatment than in the CD-ROM and text treatments. Field-independent students did not differ significantly from field-dependent students in their learning patterns. Field-independent students achieved superior scores in the long-page condition, whereas field-dependent students were superior in the short-page condition. Field-dependent subjects hit more often on teaching notes and other class resources than field-independent subjects. Field-independent students appreciated the fact that they could study topics in any order. However, field-dependent students felt confused over which options they should choose. Field-independent students tended to have higher online technologies self-efficacy, but they did not receive higher grades than field-dependent students. Field-dependent students used linear learning. Conversely, field-dependent students preferred nonlinear learning. There was no significant difference between field-independent and field-dependent students in their quantity of annotation. Cognitive style is a significant factor in terms of Instructional delivery modes and experience with online learning.

Clustering, a major exploratory data analysis method (EDA) [Tukey 1977], is concerned with the division of data into groups of similar objects. Each group, called a cluster, consists of objects that are similar among themselves and dissimilar to objects of other groups [Roussinov and Zhao 2003]. This approach has the advantages of uncovering unanticipated trends, correlations, or patterns, and no assumptions are made about the structure of the data. Wang et al. [2004] have developed a recommendation system for the cosmetic business. In the system, they segmented the customers by using clustering algorithms to discover different behavior groups so that customers in the same group would have similar purchase behavior. The goal of classification is to construct a classification procedure from a set of cases whose classes are known so that such a procedure can be used to classify new cases [Liu and Kellam 2003]. In other words, classification can be used both to understand existing data and to predict how new cases will behave. For example, Ng and Tan [2003], Brown et al. [2000], and Mateos et al. [2002] used classification to infer the functions of genes. Association rules that were first proposed by Agrawal and Srikant [1994] are mainly used to find out the meaningful relationships between items or features that occur synchronously in databases. This approach is useful when one has an idea of the different associations that are sought. This is because one can find all kinds of correlations in a large dataset. Cunningham and Frank [1999] applied the association rules to the task of detecting subject categories that co-occur in transaction records of books borrowed from a university library. A number of techniques are available to support these tasks, including decision trees, the K-means algorithm, the support vector machine, and selforganization maps. Among these, the K-means algorithm and decision trees have been widely used to analyze Web usage data. The K-means algorithm is an algorithm that can support tasks for clustering (see Section 3.3.2 for the details). This algorithm was applied in the study of Wiwattanacharoenchai and Srivihok [2003] that clustered customer segments from Web logs of various Internet banking Web sites. Consequently, their results showed that there was a clear distinction between the segments in terms of customer behavior. A similar study was conducted by Chaimeun and Srivihok [2005] who combined the K-means algorithm with self organization maps to cluster handicraft customers. In addition, Baeza-Yates et al. [2004] used the K-means algorithm to discover clusters that define textual contexts for the images on the Web. Decision trees can easily be used for performing classification tasks (see Section 3.3.3 for the details). This technique was used in the study of Lee and Yang [2003], which developed a learning agent to model a user's interests for TV program selections and to explore different program features for better recommendations. Kim et al. [2002] also used the decision tree to develop personalized recommendation procedure for an Internet shopping mall. Furthermore, Ayan et al. [2002] used the decision tree to develop a technique for automatically defining logical domains on the basis of Web page metadata. In addition to the aforementioned evidence that suggests the K-means algorithm and decision trees can provide effective approaches to examining the Web usage data, another advantage of these two techniques is that it is easy to understand how they work (see Section 3.3) and the generated results are easily understood [Mertik et al. 2004]. Moreover, they are supported by many commercial data mining software packages and thus are easily implemented in real-world applications [Ahola and Rinta-Runsala 2001]. In this article, an integrated approach that combines traditional statistics with the K-means algorithm and decision trees is selected to analyze students' learning patterns in an empirical study conducted in a WBI program. The purpose of using such an integrated approach is to bring together confirmatory analysis and exploratory analysis. In doing so, we can obtain a more complete picture, which not only shows the predefined relationships between the students' cognitive styles and their learning patterns, but can also reveal the unexpected but valuable patterns hidden in data [Hand 1999].

3. METHODOLOGY 

To understand the differences in learning patterns among different cognitive style groups, we conducted an experiment in a UK university. This section describes the research instruments, the experimental design, and the methods of data analyses. 3.1 Research Instruments Research instruments work as a guide in order to make sure that the same information is obtained from different students. The research instruments used in this study included a WBI program to teach students computational algorithms and Riding's Cognitive Style Analysis to measure students' cognitive styles. 3.1.1 Web-Based Instruction Programs. A WBI program was created containing materials from the Computation and Algorithms module. The program included about 75 pages and the content was divided into six sections. Interface elements included (a) a title bar located at the top of the screen showing the section name being viewed, (b) a control panel with choices for menu, map, index, and the other available sections, and (c) the main body of the tutorial, providing referenced links and subject categories for selection. Figure 1 shows the screen design of this WBI program. The design of this WBI program was underpinned by considerations of free exploration of the instructional material. The WBI program provided the students with rich links within the text as well as a variety of navigation tools, including a hierarchical map, an alphabetical index, and a main menu. In addition, each topic was further split into four display options comprising (a) To allow students to control the selection of the contents they wish to learn To allow students to choose one of the display options that covers the same concept overview, (b) details, (c) examples, and (d) references. There were two types of overview, a general content overview and an overview for each specific topic. In this way, the learners were given control of deciding their own learning paths and choosing their favorite navigation tools and preferred presentation formats. Three types of learner control were available in the programs, as shown in Table II. 3.1.2 Cognitive Style Analysis. The cognitive style dimension investigated in this study was the level of field dependence. A number of instruments have been developed to measure field dependence, including the Group Embedded Figures Test (GEFT) by Witkin et al. [1971] and the Cognitive Styles Analysis (CSA) by Riding [1991]. The main advantage of CSA over GEFT is that fielddependent competence is positively measured rather than being inferred from poor field-independent capability [Ford and Chen 2001]. In addition, the CSA offers computerized administration and scoring and consequently has been selected as the instrument for measuring field dependence in this study. The CSA includes two subtests (Figure 2). The first presents items containing pairs of complex geometrical figures that the individual is required to judge as either the same or different. The second presents items, each comprising a simple geometrical shape, such as a square or a triangle, and a complex geometrical figure as in the GEFT. The individual is asked to indicate whether or not the simple shape is contained in a complex one by pressing one of two marked response keys [Riding and Grimley 1999]. These two subtests have different purposes. The first subtest is a task requiring the field-dependent capacity. Conversely, the second subtest requires the disembedding capacity associated with field independence. The CSA measures what Riding and Sadler-Smith [1992] refer to as a wholist/analytic (WA) dimension, noting that this is equivalent to field dependence/independence. As Witkin et al. [1971] argued, a field-independent individual is capable of a more analytical cognitive function than a field dependent individual who uses a more global approach. Riding's [1991] recommendations are that WA scores below 1.03 denote field-dependent individuals; WA scores of 1.36 and above denote field-independent individuals; and students scoring between 1.03 and 1.35 are classed as intermediate. In this study, categorizations were based on these recommendations. As reported by Peterson et al. [2003], the reliability is r=0.81, p<0.00. 3.2 Experiment Design The experiment took place in a UK university. A request was issued to students in lectures and further by email, making clear the nature of the experiment and their participation. The students took part in the experiment a voluntary basis. The experiment consisted of two phases held on different days. Seventy-six participants took part in the first phase of the experiment, and they were asked to take Riding's CSA test to classify their cognitive styles. Once this was done, they were requested to fill out a questionnaire which was designed to identify their personal background, including their gender, the levels of their prior knowledge to subject content, and the levels of their previous system experience in using computers, the Internet, and computer-aided learning. The purpose of these activities was to select the sample for the next phase based on the participants' previous system experience and their prior knowledge of the subject content. Sixty-five students who had the basic computing and Internet skills necessary to operate the WBI program and were inexperienced in the subject content were selected to participate in the second phase. Details of the distribution of the participants for the second phase are presented in Table III. To help them use the WBI program, the second phase began by giving the students an introduction to the functionality of the WBI program. Subsequently, all of the participants interacted with the WBI program and their interactions with the program were recorded in a log file. The students were given a task sheet containing various exercises related to the content of the WBI program. The purpose of doing this is to ensure that the participants browse all contents within the WBI program. They were informed that they could use the WBI program for a maximum of 90 minutes. Due to the relatively small sample size, a within-subjects design was selected for experiment design [Reips 2000]. In this way, participants were free to choose preferred navigation tools and display options by themselves so that how cognitive styles influence their choices could be examined.

3.3 Data Analysis Raw experimental data were in the form of Web access logs and initially went through a preprocessing phase in which all damaged links were traced and removed from the database. In order to discover the relationships between students' cognitive styles and their learning patterns, it is necessary to extract the potentially useful information from raw data. The rationale of selecting useful information is based on a comprehensive review by Chen and Macredie [2002], which examined the effects of cognitive styles on student learning. In addition to their findings, the characteristics of the raw data and Web site structure were also taken into account to extract the attributes that reveal students' learning patterns in WBI. In total, there are eight attributes, including the total number of pages each student browsed, the total number of visited pages, respectively, describing overviews, examples, detailed descriptions, the total number of times each navigational tool was used, including main menu, hierarchical map and alphabetical index, and the number of repeated visits the students made. It is worth noting that the measurement of these eight attributes is based on the students' choices. In other words, this study emphasizes on students' preferred learning patterns instead of the best learning patterns because cognitive style is not a measure of intelligence or ability [Rose 1988]. In order to conduct a comprehensive evaluation, the students' learning patterns were analyzed using traditional statistical and data mining techniques. The former were applied to determine whether cognitive styles had significant effects on students' learning patterns. The latter used both clustering and classification methods. In terms of clustering, the K-means algorithm was employed to produce groups of students that shared similar learning patterns, and subsequently the corresponding human factors for each group were identified. As far as classification is concerned, the students' learning patterns were analyzed using decision trees with which rules were produced for the automatic identification of students' cognitive styles based on their learning patterns. 3.3.1 Traditional Statistics. To determine if there are statistically significant differences among the learning patterns of three cognitive style groups, the data collected from the log file were also used for analysis with the Statistical Package for the Social Sciences (SPSS) for Windows (release 10.0). The independent variable was the participants' cognitive styles. The dependent variables were the eight attributes described in Section 3.3. A series of ANalyses Of VAriance (ANOVA) was used to test for differences in each of the dependent variables because it is suitable to test the significant differences of three or more independent groups. Pearson's correlation, which is appropriate to reveal the nature and extent of associations between two variables [Stephen and Hornby 1997], was applied to find the correlations between attributes and the correlations between WA scores and the eight attributes. A significance level of p < 0.05 was adopted for the study. The mean value and standard deviation of each dependent variable are also presented in a table. 3.3.2 Clustering. K-means is one of the simplest clustering algorithms for grouping objects with similar features. In K-means, the number k of clusters is fixed before the algorithm runs. The algorithm randomly picks k cluster centers, assigns each point to the cluster whose mean is closest in a Euclidean distance sense, then computes the mean vectors of the points assigned to each cluster and uses these as new centers in an iterative approach [Hand et al. 2001]. In detail, the first step is to define k centroids (centers), one for each cluster. Another parameter called seed (S) is used to generate the random numbers for the assignment of the initial centroids. Following that, the algorithm takes each datapoint and associates it with the closest centroid. The next step starts when all the datapoints are assigned to clusters; it is the recalculation procedure of k new centroids. For these k new centroids, a new binding has to be done between the same dataset points and the closest new centroid. These two steps are alternated until a stopping criterion is met, that is, when there is no further change in the assignment of the datapoints [Evgenia et al. 2004]. The outcome of the algorithm reveals the centroid or means vector of each cluster as well as statistics on the number and percentage of instances assigned to different clusters. Thus, centroids can be used to characterize the behavior of each of the formed clusters. One of the challenging issues in using the k-means algorithm for data clustering is that one needs to decide a suitable number of clusters, k, in advance. This prerequisite can be a drawback to other experiments and domains but not for this particular experiment. Since the dataset is not large, we ran the algorithm for k=2,. . . ,10. The other issue is that K-means is sensitive to how clusters are initially assigned; this can be overcome by trying different values for the seed number S and evaluate results in order to determine which combination fits the data better [Bandyopadhyay and Maulik 2002]. Another evaluation was needed to compare various combinations of the attributes that characterize each user. The goals of these evaluations were to minimize variability within clusters and maximize variability between clusters. In other words, the similarity rules will apply maximally to the members of one cluster and minimally to members belonging to the rest of the clusters. More specifically, the evaluations will emphasize how similar the behavior of a particular user is to user of the same cluster and how different the behavior of users in that clusters from the behavior of users of all other clusters. The results of the evaluations indicated that there were no significant differences among various attribute combinations and that the most efficient outcome was obtained with a value of k = 3 according to the mean values and standard deviations of attributes and the percentage of instances of each cluster. 3.3.3 Classification. Among data mining techniques, decision tree is one of the most frequently used methods for classification. A decision tree is used to discover rules and relationships by systematically subdividing the information contained in data [Chou 1991; Chen et al. 2003]. Typically, a decision tree algorithm begins with the entire set of data, splits the data into two or more subsets based on the values of one or more attributes, and then repeatedly splits each subset into finer subsets until the size of each subset reaches an appropriate level. The entire modeling process can be represented in a tree structure, and the model generated can be summarized as a set of "if­then" rules [Li 2005]. The value of the decision tree reflects its easy understanding and a simple top­down tree structure [Hsu et al. 2003]. In tree-structured representations, a set of data is represented by a node, and the entire data set is represented as a root node. When a split is made, several child nodes, which correspond to partitioned data subsets, are formed. If a node is not to be split any further, it is called a leaf; otherwise, it is an internal node. In this study, we deal with binary trees where each split produces exactly two child nodes. Some popular algorithms of decision trees are Classification & Regression Trees (C&RT) [Breiman et al. 1984], Chi-Squared Automatic Interaction Detection (CHAID) [Kass 1980], and C4.5 [Quinlan 1993]. In this study, we have used the C4.5 because it is a well-known classification algorithm and can generate easily understandable rules [Ding et al. 2002]. The main goal of the algorithm is to discover relationships between a given classification of objects and a set of attributes. The output of the algorithm is a classification tree showing how objects may be assigned to the given classes on the basis of the values of these attributes [Andrienko and Andrienko 1999]. Tree production has three phases in this algorithm. In Phase I, an initial and large tree is created from the sets of examples according to attribute selection measures. In Phase II, an error-based pruning method is used to simplify the tree by discarding one or more sub-trees and replacing them with leaves or branches [Quinlan 1993]. In Phase III, the classification performance of the decision tree is tested by analyzing the number of correctly and incorrectly classified instances. The number of correctly classified instances determines whether the decision tree can be applied to the datasets or whether further preparation will be necessary. 

4. RESULTS 

On the basis of Pearson's correlation, there is no significant negative relationship between the attributes. The frequency of using the hierarchical map and the number of pages browsed have a significant positive correlation (r=.304; p<0.05; N=65), followed by the frequency of using the examples and the number of repeated visits with a significant positive correlation (r=.260; p<0.05; N=65). However, these correlations are less than 0.5 so it seems unlikely that there is a possible replication of measures among the attributes that are positively correlated. Therefore, all attributes were considered in the aforementioned three-data analysis approaches. Section 4.1 described results obtained from traditional statistical tests (i.e., ANOVA and Pearson's correlations). The results of clustering (K-means algorithm) and classification (decision tree) are presented in Section 4.2 and Section 4.3, respectively. Section 4.4 discusses the similarities and differences of the results gained from these three approaches. 4.1 Traditional Statistics To identify the statistical significance, an ANOVA was conducted to compare the differences among three cognitive style groups for each attribute (Table IV). In addition, Pearson's correlation was used to find whether there was a significant relationship between the WA score and each attribute (Table V). The results appeared to be consistent. Five attributes were found to be significant in both analyses: the frequencies of looking at examples and details and the frequencies of using main menu, hierarchical maps, and alphabetical index. No significant differences were found in these two analyses for two attributes, that is, the frequency of using overview and the number of repeated visits. However, the number of pages browsed was significant in Pearson's correlations, but there was no significance based on the analysis with ANOVA.

A possible explanation for this inconsistency is that three intermediate students had WA scores near a border with field-dependent students, with whom they might share some similar learning patterns. Post-hoc analysis of mean differences using the Tukey test showed significance differences for the frequencies of using the hierarchical map and the main menu between field-dependent students and field-independent (p<0.01) or intermediate students (p<0.05) but not between field-independent and intermediate students (p>0.05). Tukey test analyses also revealed that fieldindependent students significantly chose the alphabetical index and examined the details many more times than field-dependent (p<0.01) and intermediate students (p<0.01) but there were no significant differences between fielddependent and intermediate students (p>0.05). Table VI presents the mean and standard deviation of the frequency of each attribute for each cognitive style group. 4.2 Clustering The purpose of clustering is to group learners based on their similar learning patterns. Based on this rationale, three clusters were produced. The percentage of learners within each cluster is satisfactory for the total number of 65 instances. Clusters can be characterized as well balanced: Cluster 0 (N = 22): 34%, Cluster 1 (N= 23): 35%, Cluster 2 (N = 20): 31%. The mean value and standard deviation of each attribute for each cluster are shown in Table VII, where learners are grouped according to the following trends. --Cluster 0 (C0). Learners visited few pages, made few repeated visits and seldom accessed any navigation tools and subject categories. --Cluster 1 (C1). Learners visited some pages, made some repeated visits, frequently accessed the alphabetical index, and looked at the detailed descriptions. --Cluster 2 (C2). Learners visited many pages, made many repeated visits, frequently accessed the hierarchical map and looked at the examples. When analyzing the WA Score of each cluster, we found that there is a significant difference (F (2, 62)=5.878; p<0.01). Cluster 1 has the highest WA score (Mean=1.4; Standard Deviation=0.38), while Cluster 2 has the lowest WA score (Mean=1.0; Standard Deviation=0.24). In other words, the learners of Cluster 1 have a field-independent tendency, whereas the learners of Cluster 2 have a field-dependent tendency. These results seem consistent with the distribution of cognitive styles among the three clusters (Figure 3). Fieldindependent learners (N=14; 56%) mainly emerge in Cluster 1 in which the learners frequently examined the detailed descriptions, accessed the alphabetical index many more times, and browsed fewer pages. Conversely, the majority of field-dependent learners (N=12; 57%) appear in Cluster 2 in which learners often accessed the hierarchical map, browsed many more pages, and favored learning from examples. Unlike field-independent and field-dependent learners, intermediate learners of each cluster are almost balanced (Cluster 0: N=7, 36%; Cluster 1: N=6, 32%; Cluster 2: N=6, 32%). To identify whether cognitive style was the only one human factor that directed students' learning patterns, the significant differences of other human factors among the three clusters were also examined. None of them are significant, including their previous experience using computers (F (3, 61)=2.008; p>0.05), the Internet (F (3, 61)=0.126; p>0.05), and computer-aided learning (F (4, 60)=2.685; p>0.05). 4.3 Classification Figure 4 shows the decision tree that was generated for characterizing cognitive styles. According to this decision tree, the frequencies of using the examples and the hierarchical maps are the most important and second most important features to classify the cognitive styles, respectively. On the third level, the frequency of using the alphabetical index and the number of pages browsed are also relevant. As shown in this decision tree, the examples were mainly chosen by field-dependent students, and this result is coherent with those from clustering and traditional statistics. It is worth noting that the hierarchical map and alphabetical index could have been used by both fielddependent and field-independent students but they used these navigation tools in different ways. Field-dependent students, who either frequently used the hierarchical map or frequently used the alphabetical index, browsed many more pages. Conversely, field-independent students, who frequently used the hierarchical map, visited fewer pages. Field-independent students, who rarely used the hierarchical map and the alphabetical index, looked at the detailed descriptions many more times. In other words, such detailed description may be the major source for their learning. These learning patterns are not derived from traditional statistical and clustering approaches. In addition to illustrating the students' learning patterns, the decision tree can also be converted to create rules for the predictions of the students' cognitive styles, that is, identifying the cognitive styles of new students based on their learning patterns. In total, there are eight rules drawn from the decision tree. Different rules lead to different cognitive styles. The first rule obtained from the right side of the second level of the tree indicates that students who visit the examples more than 11 times are classified field-dependent students. The second rule taken from the right side of the fourth level shows that students who use the map more than 10 times and browse less than or equal to 57 pages are classified as field-independent students. The remaining rules were acquired from the bottom level of the tree. The third to the sixth rules were gained from the left side, while the seventh and eighth rules were from the right side. In terms of the former, the third rule is that students who use the map less than or equal to 10 times and use the index less than or equal to 9 times and visit the detailed descriptions less than or equal to 12 times are classified as intermediate students. The fourth rule is that students who use the map less than or equal to 10 times and use the index less than or equal to 9 times and visit the detailed descriptions more than 12 times are classified as field-independent students. The fifth rule is that students who use the map less than or equal to 10 times and use the index more than 9 times and browse less than or equal to 45 pages are classified as intermediate students. The sixth rule is that students who use the map less than or equal to 10 times and use the index more than 9 times and browse more than 45 pages are classified as field-dependent students. The seventh rule is that students who use the map more than 10 times and browse more than 57 pages and use the main menu less than or equal to 6 times are classified as intermediate students. The eighth rule is that students who use the map more than 10 times and browse more than 57 pages and use the menu more than 6 times are classified as field-dependent students. As shown in these eight rules, a cognitive style could be reached by more than one rule. In this case, three rules are connected with field-dependent students, three are associated with intermediate students, and the other two are related to field-independent students. These rules can be applied to replace the CSA or other cognitive style tests and work as criteria for automatic identification of the students' cognitive styles. In other words, the students' cognitive styles can be automatically recognized based on their learning patterns which can contribute to the development of personalized learning environments on the Web. 4.4 Summary Regarding whether these attributes are related to cognitive styles, the results obtained from the four data analysis approaches are rather similar (Table VIII). The common relevant attributes are the frequencies of looking at examples and details and frequencies of using the main menu, hierarchical maps, and alphabetical index. The common irrelevant attributes are the frequencies of looking at the overview and the number of repeated visits. The number of pages browsed is recognized as a critical attribute based on the results obtained from the K-means algorithm, the decision tree, and the Pearson's correlations, but the results from ANOVA indicate that there is no statistical significance. 

5. DISCUSSIONS 

As shown in the previous section, cognitive style has significant effects on student learning patterns in the WBI program. In particular, they have different preferences in the selection of navigation tools and display options. The difference in their preferences may be a reflection of differences in the students' cognitive styles [Wildemuth et al. 1998]. For example, intermediate learners of each cluster are almost balanced. It suggests that they are equally comfortable using each learning pattern. One possible interpretation is that individuals possessing an intermediate cognitive style combine the characteristics of both field independence and field dependence and employ a more versatile repertoire of learning strategies. Versatile learners, who have acquired the skill to move back and forth between different learning strategies, are more capable of adapting themselves to the learning systems [Ford 2000]. In addition, the different learning patterns showed by field-independent and field-dependent learners also echo their characteristics which are discussed in the following. 5.1 Analytical vs. Global A major issue in identifying differences between field-independent and fielddependent students is their separate tendencies to adopt analytical and global approaches. This difference may explain the results of this study that field-independent students tend to browse fewer pages than field-dependent students. A possible interpretation of this finding is that field-independent students tend to be more analytical [Ford et al. 1994] and they are very task oriented [Witkin et al. 1977]. Hence, they only pay attention to particular topics related to their learning, and thus browse fewer pages. On the contrary, field-dependent students perceive objects as a whole and process information in a global fashion [Goodenough 1976]. Therefore, they tend to browse many more pages to build an overall picture of the subject content. Such findings strengthen the claim of previous research that field-independent people are good at analytical thought, whereas field-dependent people have global perceptions [Witkin et al. 1977]. 5.2 Active vs. Passive In this study, field-dependent and field-independent students show different preferences for navigation tools. Field-independent students often access the alphabetical index, which provides users with means to locate particular information without going through a fixed sequence [Chen and Macredie 2002]. This may be due to the fact that field-independent students actively segment information into relevant parts [Goodenough 1976]. Conversely, fielddependent students often use the hierarchical map, which applies a graphical representation to illustrate the relationships among different concepts [Turns et al. 2000] and reflects the conceptual structure of the subject content [Nilsson and Mayer 2002]. This might be caused by the fact that fielddependent students tend to take a passive approach by relying on salient cues [Anastasi 1988], and they have more difficulties in separating the individual parts within a whole [Witkin et al. 1977]. With the extra guidance provided by the hierarchical map, field-dependent students can more easily access meaningful information. The findings, in general, support previous studies [Evans and Edwards 1999; Hsu and Schwen 2003]. 5.3 Internal vs. External The results of our study have shown that different cognitive style groups tend to favor different display options. Field-dependent students often look at examples, while field-independent students frequently examine the detailed descriptions. Examples are down-to-earth visual material that provides illustrations with practical cases to be helpful to students in deepening their understandings. This approach is beneficial to field-dependent individuals who rely more on external frames of reference and operate best when analyses are already provided [Lyons-Lawrence 1994]. The detailed descriptions presents text-based explanations of theoretical concepts which are very detailed, but they do not illustrate the concepts with practical cases. Therefore, there is a lack of concrete guidance for students; students need to transfer their knowledge into an activity by themselves. This may not interfere with field-independent students who use an internal frame of reference to organize information [Reiff 1996] and structure problems [Davis and Cochran 1989]. Therefore, they are more able to reorganize information from the detailed descriptions and create useful cues on their own. This finding is in accordance with that of previous work that field-independent individuals have more theoretical and abstract interests [Miller and Escolme 1990].

6. IMPLICATIONS FOR SYSTEM DESIGN 

The previous discussion suggests that student learning patterns are significantly influenced by their cognitive styles. Field-independent and fielddependent learners have different characteristics that affect their learning patterns. This implies that WBI programs should be developed to support the unique needs of each cognitive style group. More specifically, the WBI programs should be flexible enough to offer multiple options tailored to the distinctive cognitive styles. To this end, a model (Figure 5), which was developed using the results presented in Section 4, can be considered as a mechanism to help designers develop WBI programs that can accommodate the preferences of both field-independent and field-dependent learners. The core of the model consists of three key design elements of WBI programs: navigation tools, display options, and content scope. 6.1 Navigation Tools In this study, field-independent students tend to actively group relevant concepts from the alphabetical index by themselves, while field-dependent students tend to be passive and rely on the hierarchical map to build the relationships among different concepts. One of the solutions to accommodate their different preferences is to allow the learners to see both navigation tools at the same time by using frames. Frame uses an underlying grid layout which allows multiple documents to be shown in a window at once [Milunovich 2002]. Navigation control is one of the benefits of using frames. One frame can always stay visible even if the contents of the other frame change [Hobbs 1999]. In this case, there are two frames: the hierarchical map frame and the alphabetical index frame. The hierarchical map frame is always visible and it corresponds to information selected from the alphabetical index frame. In other words, the selected topic in the alphabetical index frame would be highlighted in the hierarchical map frame. In doing so, field-independent students can use the alphabetical index frame to select a relevant topic and field-dependent students can identify the relationships between this topic and other topics from the hierarchical map frame. 6.2 Display Options The results of our study have shown that field-independent students are capable of extracting relevant information from the detailed description which lacks practical instruction because they have a tendency to use their own internal references. On the other hand, field-dependent students rely more heavily on external cues, thus, they prefer to get concrete guidance from examples. One of the possible ways to address their different needs is to show both of the display options, detailed description and concrete examples, within a table. By using a table, all of the relevant information about a particular case can gathered together in one place. For example, one column can be used to present the detailed descriptions of a particular topic, while the other column provides the illustration with examples for that topic. This approach not only lets field-independent students have a look at detailed descriptions, but also provides field-dependent students with concrete examples. In other words, the information needs of both cognitive style groups can be met simultaneously. 6.3 Content Scope Field-independent students approach an environment in an analytical fashion so they tend to focus on information that is relevant to their tasks. Hence, they browse fewer pages to directly get to relevant topics for completing their tasks. Conversely, field-dependent students use a global approach to process information so they tend to build an overall picture by browsing more pages. One of the potential solutions to deal with their different requirements is to use a pop-up window, which is a secondary window to provide additional information about selected objects by clicking a hypertext link [Bell et al. 2002]. In this case, a pop-up window can be used to show additional topics for field-dependent students who would like to get a global picture of the subject content. However, these windows can be switched off, lest they irritate field-independent students who have a tendency to focus on specific relevant topics. In other words, information that is related to tasks is put in the main window for field-independent students, while unrelated information is displayed with a pop-up window for field-dependent students. 7. CONCLUSIONS WBI creates learning opportunities for everyone if suitable considerations are made in the design process. Otherwise, they can impose needless barriers to equal participation in educational settings. The experimental results obtained in this study suggest that cognitive style plays an influential role in student learning patterns within WBI. Field-independent and field-dependent learners have different preferences for locating information, especially for the selection of navigation tools and display options. Thus, there is a need to be aware of cognitive styles when planning to improve the usability and functionality of WBI programs. The contribution of this study includes three aspects: theory, methodology, and applications. In terms of theory, this study deepens the understanding of the importance of cognitive styles in the development of WBI programs by providing empirical evidence. Cognitive styles, gender differences, and system experience are factors that are frequently considered in the literature of individual differences but it is inconclusive as to their relative importance. The findings of this study indicated that cognitive style is a major factor that influences student learning patterns. However, it was only one relatively small study. Further work needs to be undertaken with a larger sample to provide additional evidence. With regard to methodology, this study analyzed the experimental data with a data mining approach, which used both clustering and classification techniques. These two techniques are complementary in that they integrate the analysis of macro and micro levels. The results from clustering present an overall picture of the students' learning patterns, whereas those from classification provide the detailed rules for the automatic identification of students' cognitive styles based on their learning patterns. However, this study only used two methods, that is, K-means (clustering) and decision trees (classification). Given any dataset, there are often no strict rules that impose the use of a specific method over another in its analysis. Therefore, it is necessary to conduct further work to analyze student learning patterns using other clustering or classification methods, for examples self-organizing maps and support vector machines. It would be interesting to see whether similar results can be found by using these methods. As far as the application is concerned, this study recognized the importance of versatility in the development of WBI programs and developed a model to illustrate the needs of different cognitive styles. In addition, several design approaches were proposed to accommodate the preferences of both fieldindependent and field-dependent learners. In the future, the rationale of the model and the design approaches can be used to improve the development of existing WBI programs and other Web-based applications such as digital libraries, search engines, and electronic journals. Finally, it would be valuable to see whether such WBI programs and the Web-based applications can promote learners' performance and increase their satisfaction.
Repeatable Evaluation of Search Services in Dynamic Environments.
ERIC C. JENSEN Summize, Inc. STEVEN M. BEITZEL Illinois Institute of Technology ABDUR CHOWDHURY Summize, Inc. and OPHIR FRIEDER Illinois Institute of Technology and Georgetown University

In dynamic environments, such as the World Wide Web, a changing document collection, query population, and set of search services demands frequent repetition of search effectiveness (relevance) evaluations. Reconstructing static test collections, such as in TREC, requires considerable human effort, as large collection sizes demand judgments deep into retrieved pools. In practice it is common to perform shallow evaluations over small numbers of live engines (often pairwise, engine A vs. engine B) without system pooling. Although these evaluations are not intended to construct reusable test collections, their utility depends on conclusions generalizing to the query population as a whole. We leverage the bootstrap estimate of the reproducibility probability of hypothesis tests in determining the query sample sizes required to ensure this, finding they are much larger than those required for static collections. We propose a semiautomatic evaluation framework to reduce this effort. We validate this framework against a manual evaluation of the top ten results of ten Web search engines across 896 queries in navigational and informational tasks. Augmenting manual judgments with pseudo-relevance judgments mined from Web taxonomies reduces both the chances of missing a correct pairwise conclusion, and those of finding an errant conclusion, by approximately 50%. 

1. INTRODUCTION 

Evaluating the effectiveness of information retrieval systems, in terms of relevance, requires a large amount of human effort. Many environments, such as the World Wide Web, grow and change too rapidly for a single evaluation to carry meaning for any extended period. Changes in their document collection, query population, and set of search services demand the repetition of evaluations over time. In these environments, static test collections become outdated too quickly and require too much effort to reconstruct. Rather, practitioners often compare a small number of live engines by judging every result retrieved at a shallow depth--without system pooling. The number of queries necessary for such an evaluation to be reliable1 must be determined, however. We hypothesize that combining automatic evaluation techniques with a smaller set of manual relevance judgments can provide more reliable pairwise conclusions ("engine A outperforms engine B") than the manual set alone. We propose a semiautomatic framework for combining manually judged queries with automatically evaluated ones, our ultimate goal being to reduce manual evaluation effort by finding reliable conclusions using less manually judged queries. To test our hypothesis, we adopt the reproducibility probability (". . . probability of observing a significant clinical result from a future trial . . . " [Shao and Chow 2002]) as our estimate of reliability. We then compare conclusions drawn with high reproducibility probability from semiautomatic evaluations against those from a manual evaluation of the top ten results of ten Web search engines over 896 queries. 2 The available content on the Web changes 8% every week, along with dramatic changes in the number of servers and pages [Cho et al. 2000; Ntoulas et al. 2004]. In our experimentation, we found that only 61% of Web search engines' top ten results remained the same three months later on average, and only 38% for the most changed engine [Jensen 2006]. Searchers' interests and the popular queries they use to express them are also in a constant state of flux, with 20% of even the 30,000 most popular queries changing from one week to the next, and less than half remaining the same after six months [Pass et al. 2006]. Even the topical categories these queries fall into have changing relative popularities within days, weeks, months, and years [Beitzel et al. 2004b, 2006; Jansen and Spink 2005]. Not only is the query population rapidly changing, but its size and diversity also indicate that a large number of queries are required to construct a representative random sample [Pass et al. 2006]. Popular queries and even popular query terms make up only a small portion of the total query stream, with approximately half of all queries being repeated ten or fewer times over a week [Beitzel et al. 2006; Jansen et al. 2005]. Developing new algorithms, or even tuning traditional retrieval strategies for emerging applications (image search, blog search, etc.) requires reliable, repeatable3 evaluations on their respective dynamic environments. Static test collections, such as those constructed for the Text Retrieval Conference (TREC), become outdated too quickly to address these changes in popular queries and their associated relevant results. With typical TREC evaluations requiring well over 500 assessor-hours (see Section 2), these sorts of collections are too expensive to reconstruct when changes in effectiveness over time must be measured. This effort is exacerbated by rapidly growing collection sizes, as the reusability of such collections depends on the depth of their pooled evaluations (also detailed in Section 2). Therefore, practitioners in dynamic environments often dispense with efforts to build reusable test collections in favor of reevaluating each engine as decisions are required. However, based on analysis of our manual evaluation, we find that such shallow judgments demand a large number of queries to provide reliable conclusions (as many as 650 in our environment). A method of reducing the effort needed to draw reliable conclusions in such an environment is needed. To make the repetition of such large evaluations over time feasible, we propose a semiautomatic framework that incorporates automatically evaluated queries (using pseudo-relevance judgments) with manually judged ones. This provides insight into conclusions earlier in the evaluation process so that poorly performing engines can be eliminated before judging every result from every engine over a large query sample. We identify two methods for integrating automatic judgments. Each provides a different form of guidance for evaluators to reach reliable conclusions with less effort than manual judgments alone: Semiautomatic Filtering: Verify conclusions drawn from a smaller number of manual judgments based on their agreement with automatic techniques. Semiautomatic Prediction: Directly combine automatically judged queries with manual ones to yield samples of larger sizes whose conclusions can be used as an estimate of those that might be found with that many manual judgments. To test our hypothesis that this semiautomatic framework yields more reliable conclusions than those available from the manually judged sample alone, we must adopt a specific method of estimating reliability. We use reproducibility probability (how likely a pairwise conclusion is to hold across any query sample of a given size) as our estimate of reliability for two reasons. First, measuring changes in performance over time in a dynamic environment demands conclusions that generalize to the query population as a whole at the time of evaluation. If applying an identical evaluation methodology to different query samples from the same time period yields inconsistent conclusions, nothing can be concluded about changes in engine performance over time. Comparing the conclusions from any two evaluations that use different query samples would be impossible. Implicit in this assertion is our view of the query stream at a given point in time as a hypothetical infinite population, in following with the frequentist approach we adopt (well reviewed recently for information retrieval in Cormack and Lynam [2006]). Second, we seek to reduce manual evaluation effort by exploiting the fact that larger differences in evaluation scores are detectable with smaller query sample sizes, possibly available from an evaluation in progress. Information retrieval traditionally uses a priori heuristics for determining the necessary query sample size to yield a desired level of reliability, such as TREC "rules of thumb" about the minimum absolute difference between scores often derived from empirical meta-evaluation (see Section 2.2.3). However, these do not address the problem of detecting reliable conclusions from an evaluation in progress. We leverage the pointwise bootstrap estimate of reproducibility probability of hypothesis tests that quantifies the reliability of conclusions from any pairwise evaluation, without the prerequisite of a sufficient query sample size to estimate parameters such as the mean score difference or a context of meta-evaluation over a diverse set of engine pairs. The ability to develop intelligent evaluation strategies, such as discarding results from an engine that is clearly inferior based on a small number of judgments, is largely unexplored because the "running averages" available from evaluations over small query sample sizes have been shown to be unreliable when viewed as whole [Voorhees and Buckley 2002]. Quantifying the utility of intelligent evaluation strategies is also difficult using existing methods of comparing evaluations (meta-evaluation). For example, prior automatic evaluation and implicit preference research (reviewed in Section 2.3) focuses on optimizing the correlation of engine rankings from a purely automatic evaluation to a manual one. However, critical decisions such as which search service to employ, and so forth, demand a more rigorous comparison of conclusions drawn by these methods with those from manual judgment. By leveraging reproducibility probability, we ensure only conclusions with high reproducibility probability are compared; those that would not generalize to other query samples using the same evaluation technique are considered "ties." Next, we review related work in information retrieval evaluation and reliability estimation. In Section 3, we show that our manual Web search evaluation is reliable and we validate reproducibility probability estimation techniques on it. Having established that prerequisite, we propose and validate our semiautomatic framework in Section 4 using two simple automatic evaluation techniques. Even with these na¨ve techniques, errors are often reduced by half i compared to using small sets of manual judgments alone. More importantly, metrics for comparing evaluations and measuring the utility of semiautomatic methods are developed. 

2. RELATED WORK 

First, we review evaluation of information retrieval systems on the Web. We then examine four methods for estimating the reliability of evaluations: hypothesis testing, confidence intervals, empirical meta-evaluation, and reproducibility probability estimation. Finally, we review prior work in automatic evaluation techniques.

2.1 Web Search Evaluation Evaluating the effectiveness (relevance) of live Web search engines provides many unique challenges because they operate on data that are continually changing [Hawking et al. 1999; Savoy and Picard 2001]. The set of popular Web queries and the relevant documents for those queries changes dramatically over time [Pass et al. 2006]. Previous studies concluded that overlap among results from different Web search engines was too high for them to be deemed significantly different [Ding and Marchionini 1996]. However, when a decision must be made, some form of reliable evaluation is necessary. Most of the work in evaluating search effectiveness follows the Text Retrieval Conference (TREC) methodology for constructing reusable test collections. TREC holds constant the document collection and query set, pooling the top ranked results up to a given depth (typically 100) from each engine and manually judging each document in this pool as relevant or not relevant. If this judgment depth is large enough, these collections are reusable, in that the relative effectiveness of runs from new engines over the same documents and queries can be evaluated simply by applying the existing judgments and assuming documents that are not judged are not relevant [Zobel 1998]. Studies of evaluation in TREC (meta-evaluations) have shown that although relevance is an ambiguous concept [Borlund 2003], variations in relevance judgments due to assessor disagreement do not destabilize evaluation [Voorhees 1998]. The TREC Web track applies this methodology to static Web document collections. The recognition that Web search users perform tasks other than the TREC standard informational task (searching for many relevant documents topically related to the query) has led to the incorporation of navigational homepage or namedpage-finding evaluations that assume there is a single best-known item (sans duplications) the searcher wants to find. Most recently, TREC has begun to address the question of whether building reusable collections through pooled evaluation is scalable to terabyte-sized collections [Clarke et al. 2005]. Recent work by Sanderson and Zobel [2005] shows that judging only the top ten results of each engine provides reliable evaluation for less effort than system pooling. Evaluations are very labor intensive. Our own precision oriented evaluation of the top ten results of ten Web search engines over 896 queries required 225 assessor-hours to complete [Jensen et al. 2005; Jensen 2006]. This is approximately 15 minutes per query, to assign binary relevance and choose the best result from an average of 43 distinct results. A previous navigational evaluation we performed, selecting only the best page and its duplicates from a pool of six Web search engines' results (about 25 on average) over 418 queries, required 87 hours, or approximately 12 minutes per query on average [Beitzel et al. 2003b]. Creating reusable test collections such as those developed in TREC requires a larger amount of effort. Recent TREC efforts have employed six assessors generally working 20 hour weeks for over a month [Soboroff 2006]. The TREC 2001 Web ad hoc search task required 761.25 assessor-hours to perform judgments over 50 topics, and an additional 283 hours to develop those topics. The 2004 terabyte track ad hoc task required 1037.5 hours total, with over half spent performing judgments over 50 topics. Even in the 2003 homepage/named page task where the query was developed for a prechosen best document, the process of simply checking shallow retrieved pools for duplicates over the 300 queries required as many as 100­120 hours. 2.2 Estimating Evaluation Reliability The ultimate goal of evaluation is to facilitate the construction of engines that are a "meaningful" improvement over the state of the art. However, this improvement (often characterized as a level of difference discernable to users) may be achieved through several iterations of reliable improvements. We specialize on Tague-Sutcliffe's [1996] definition of reliability for the case of a pairwise conclusion from an information retrieval evaluation as its reproducibility probability across any random query sample of equivalent size. We focus only on the reliability of conclusions, as minimum levels of difference could easily be incorporated into such analysis by selecting a different null hypothesis, and would only increase the required sample sizes. Next, we review several methods of estimating reliability. 2.2.1 Hypothesis Tests. Applying statistical hypothesis tests to information retrieval evaluations has a history of controversy, as most tests rely on observations conforming to continuous, often particular, distributions, but typical information retrieval evaluation metrics are bounded, discrete and often non-normal in nature [van-Rijsbergen 1979]. Bootstrap hypothesis tests, such as those applied to information retrieval evaluation by Savoy [1997] or Sakai [2006], do not require these assumptions because they estimate the empirical distribution by resampling thousands of times. When performing a handful of these tests, this computational cost is not of consequence, but estimating their power is computationally and theoretically challenging [Davidson and MacKinnon 2006]. Therefore, we choose the Wilcoxon signed rank test with standard corrections for noncontinuity in our experimentation because its nonparametric nature does not require assuming a particular distribution, but it is easily calculable and maintains higher power than very simple tests such as the sign test [Hollander and Wolfe 1973]. Although the reproducibility probability of any test could be estimated with the nonparametric bootstrap we leverage below, the distribution of scores in our evaluation motivated this decision. They failed a Shapiro-Wilk test for normality but did appear to be symmetric, as required for the Wilcoxon test [Jensen 2006]. Our own and others' experimentation with the t-test, the sign test with and without the "zero fudge," Wilcoxon test with both the continuity correction and normal approximation, and also an exact version of the Wilcoxon test that computes every permutation in the case of tied ranks, found none that resulted in substantially more reliable reproducibility probability estimates than others [Jensen 2006; Sanderson and Zobel 2005]. Our same prior investigation showed that reliable reproducibility probability estimates with a 95% confidence level would have required more queries, so we chose  = 0.10. The fundamental problem with relying on the p-value from a single hypothesis test is that it does not address the problem of adequately representing the query population to ensure reproducibility [Goodman 1992]. Because of this, it is possible to find statistically significant differences over a particular sample of queries that may not generalize to the query population. Another factor that must be considered in applying hypothesis tests to information retrieval evaluation is that performing multiple tests with the same null hypothesis requires simultaneous testing procedures (such as the commonly used Bonferroni correction) to account for the overall larger probability of finding a significant result by random chance. Miller distinguishes between experiments designed for "uncovering leads that can be pursued further to determine their relevance to the problem" versus those that report final conclusions, suggesting that multiple test procedures are more important in the latter case [Miller 1981]. Our primary endpoint is comparing conclusions that result from pairwise hypothesis tests of semiautomatic evaluations versus those of benchmark manual ones. Because each comparison between a pair of engines has its own hypothesis, differing from others, multiple testing procedures are not required in our analysis. However, if the primary endpoint is to find the best engine or to rank the engines, multiple testing procedures for step-wise and pairwise comparisons should be considered, to ensure conservative estimates [Munzel 2001]. 2.2.2 Confidence Intervals. Many advocate reporting confidence intervals for the parameter of interest (which in evaluation is typically the score difference) rather than hypothesis testing because they are easier to interpret correctly. Cormack and Lynam [2006] construct confidence intervals of average precision over varying document collections in the TREC informational task, using the bootstrap. If we use a confidence interval to decide whether or not one engine significantly outperforms another (by checking whether the null hypothesis, typically zero difference in scores, lies outside the interval), we are performing exactly the same analysis as the equivalent hypothesis test. Again, this does not address the problem of adequately representing the query population or quantifying reproducibility. 2.2.3 Empirical Meta-Evaluation. Empirical meta-evaluation (studying the results of an evaluation over a large number of engines) focuses on estimating the reliability of an evaluation as a whole. This sort of analysis is a key component of TREC, where it is almost exclusively applied due to the lack of such a large, diverse set of engines in proprietary environments. In empirical meta-evaluation, reliability is defined as the stability (consistency) of the ranked list of engines across query sets. Kendall's Tau or Spearman's rank correlation measures are often used to compare evaluations based on their ranking of engines. However, the most relevant metric to our work is the error rate (probability of a pair of engines flipping positions relative to one another in the ranked list of engines when using a different query sample) as defined in Buckley and Voorhees [2000]. It is estimated post hoc by counting the number of pairwise flips in the rankings of a large number of engines across varying query samples by resampling the pilot query set (total available judged queries) into smaller samples. In Voorhees and Buckley [2002], they focused on performing this calculation for several different query sample sizes up to half the size of the pilot sample and then extrapolating to estimate the error rate at the total pilot set's size. By also calculating the error rate for several different fuzziness (minimum difference in average scores to not be considered a tie) values and leveraging the extrapolations to the pilot set size, these estimates can be used to devise a priori heuristics sometimes cited when planning or analyzing experiments at TREC. Typically, these take the form of "an X% difference in mean average precision is needed to ensure an error rate of less than 5% with 50 queries." However, these heuristics do not account for the differences in distribution of a particular pair of engines' scores (their variance, for example). While error rate is useful for post hoc comparison, these general heuristics derived from it are only applicable to a completed evaluation. Applying these heuristics to an evaluation in progress is questionable, as preliminary differences in average scores are subject to influence from outlying scores, such as zero and one. Recent work builds on error rate with improved theoretical foundations. Sanderson and Zobel [2005] mitigate distributional issues by requiring both that a pair of engines pass a hypothesis test, and have a difference in average scores large enough to correspond with a low error rate. Lin and Hauptmann [2005] derive error rate from statistical principles, showing that variance in engines' scores dramatically impacts the reliability of evaluations. Sakai [2006] uses bootstrapping to find the score difference required to achieve a given significance level in bootstrap hypothesis testing. Each of these methods has in common the use of a large number of diverse runs to provide a general rule for the difference in average scores required. They do not address the problem of reducing the effort needed to answer specific questions without such a context, such as "does engine A outperform engine B?" from an evaluation in progress. 2.2.4 Reproducibility Probability. Although we are unaware of its application to information retrieval evaluation, we adopt reproducibility probability because it directly measures the likelihood that a particular pairwise conclusion generalizes to the query population as a whole, while its generality enables its application to evaluations in progress and does not require a large number of diverse engines as a context. Shao and Chow [2002] analyze several methods of estimating reproducibility probability. We follow their first in which "the reproducibility probability can be defined as an estimated power of the future trial using the data from the previous trial(s)." For clarity, we briefly diverge to differentiate between the true power (typically referred to simply as the "power"), or probability of rejecting a legitimately false null hypothesis, and this "estimated power" described by Shao and Chow. The true power (defined as an expectation in Equation 1 borrowing notation from Lehmann [1986]) is commonly used a priori in experimental design to determine what sample size will be large enough to detect a significance difference if one exists. However, calculating the true power depends on specifying a particular distribution, F, that satisfies the alternative hypothesis. This can be inaccurate when little is known about the actual distribution of observations [Bacchetti 2002]. When F is unknown, the true power of nonparametric tests is most accurately estimated using the bootstrap by creating artificial subsamples of a pilot sample in which the alternative hypothesis is enforced [Troendle 1999]. We are primarily concerned with comparing evaluations based only on their conclusions with high reproducibility probability, not the true power of hypothesis tests, which determines the likelihood of detecting a significant difference where one exists. 

True power of a hypothesis test. The "estimated power" method of estimating reproducibility probability described by Shao and Chow differs from this true power in that it comes from observed experimental data where the truth-value of the null hypothesis is unknown. For this reason, it is also known as the "observed power." Like Shao and Chow, however, we prefer the term "reproducibility probability" to avoid any implication about the truth of the null hypothesis (inference). Post hoc power analysis has drawn criticism for the way it has been misinterpreted as evidence against the null hypothesis for tests that do not reject the null hypothesis (since the observed power is greater than zero, we must simply not have a large enough sample to support our conclusions) [Hoenig and Heisey 2001]. This is the trap of the large sample; that any two nonidentical engines are significantly different with a large enough sample. We focus only on tests that do reject the null hypothesis and have high reproducibility probability at sample sizes just large enough to reliably estimate reproducibility probability (as analyzed in Section 3.3). Although their definition is general, Shao and Chow [2002] only apply the "estimated power" approach to reproducibility probability for the parametric t-test. However, it can be applied in the general case (to include nonparametrics) using the point-wise bootstrap estimate, i.e. as done by De Martini [2006]. This pointwise estimate (Equation 2) is based on Efron and Tibshirani's [1993] nonparametric bootstrap, "A preliminary data set, datan , is used to estimate a ^ probability distribution, in this case F . Then the desired power or sample size ^ calculations are carried out as if F were the true distribution," as discussed in their example of estimating the true power of a bioequivalence test. We provide an algorithm implementing Equation 2 specifically for information retrieval in Section 3.2. Note that this is in the same spirit as the error rate heuristic discussed in Section 2.2.3, but formalizes reproducibility probability of a particular pairwise conclusion without requiring a completed evaluation over a large variety of engines.  

Point-wise nonparametric bootstrap estimate of reproducibility probability. However, these reproducibility probability estimates are just that, estimates that are influenced by the variability in the observed data (pilot sample). The necessary pilot sample size required to reliably (reproducibly across pilot samples) estimate them must be established. We leverage graphical methods for comparing true power to estimates that have been developed for just this purpose [Collings and Hamilton 1988]. Aggregated numerical methods have also been introduced, but they are targeted at relative comparison of power estimation techniques rather than our focus of determining necessary sample sizes [De Martini and Rapallo 2003]. One method of making more conservative bootstrap estimates is to perform a double bootstrap, essentially performing secondary bootstrap replications of each of the bootstrap samples [De Martini 2006; Hall and Martin 1988]. When performing a hypothesis test for each bootstrap sample of reasonable size, this is computationally prohibitive. Our validation of the reliability of reproducibility probability estimates in Section 3.3 follows a limited version of this procedure, visualizing differences in estimates over several pilot samples. 2.3 Reducing Evaluation Effort Two aspects of reducing evaluation effort have been studied in prior work: evaluation strategies that reduce the number of judgments needed in a manual evaluation, and automatic evaluation techniques that heuristically infer pseudo-relevance judgments. Studies in each of these areas suffer from a difficulty in comparing conclusions drawn from one evaluation to another: to ensure lower-effort techniques provide correct conclusions with respect to more thorough methods. Simply knowing that the engine rankings of one evaluation correlate with another does not address differing levels of confidence in conclusions and the associated issue of whether too many errant conclusions are being drawn or too few correct conclusions (too many ties) are found. Several evaluation strategies are proposed as extensions or alternatives to the TREC pooling methodology to reduce manual effort. Soboroff [2006] focused on the problem of changes in the document collection, proposing to maintain existing TREC collections to limit the impact of these changes over time. Recent work dramatically improves on the evaluation effort required in TREC by intelligently selecting results to be evaluated [Aslam et al. 2006; Carterette et al. 2006]. Cormack et al. [1998] proposed interactive searching and judging, in which no system pooling is used; evaluators simply perform various queries for a topic, marking relevant documents as they proceed. Sanderson and Joho [2004] analyze methods of producing test collections without any system pooling and find that their quality correlates with that of TREC collections. Sanderson and Zobel [2005] quantified the relative advantage of not pooling in terms of the evaluation effort required to achieve a desired error rate. Fully automatic evaluation techniques are widely employed in domains where manual evaluation would require a prohibitive amount of effort [Goldstein et al. 2005]. Two categories of automatic evaluation techniques proposed for information retrieval are inferring pseudo-relevance judgments from the retrieved documents themselves, and using external resources to aid in this inference. Several approaches randomly sample the documents from the retrieved pools, based on known statistics about the typical distribution of relevant documents, as pseudo-relevant documents, but find that the effectiveness of only typical engines, but not the best engines, can be predicted [Aslam et al. 2003; Nuray and Can 2006; Soboroff et al. 2001; Wu and Crestani 2003]. Others use similarity functions between documents and the query to automatically estimate relevance [Shang and Li 2002]. Several methods of leveraging external resources to infer pseudo-relevance judgments have been proposed. Some advocate the use of click-through data (tuples consisting of a query and a user-clicked result) for automatic assessment. However, there is a well-known presentation bias inherent in these data: users are more likely to click on highly ranked documents regardless of their quality [Boyan et al. 1996]. Joachims et al. [2005] find that clickthrough data can, however, be used to infer relative preferences between documents. Others have made use of taxonomies to fuel automatic evaluation, such as the Open Directory Project (referred to as DMOZ or ODP), Yahoo's directory, and Looksmart [Haveliwala et al. 2002; Srinivasan et al. 2005]. These taxonomies divide the Web into a hierarchy of categories, with some pages placed in multiple categories. Each category has a title, and a path that represents its placement in the hierarchy. They also typically have editor-entered page titles that do not necessarily correspond to the titles of the pages themselves. 

3. RELIABLE MANUAL EVALUATION 

Evaluating our semiautomatic framework requires a reliable manual evaluation for comparison. We are unaware of currently available large manual evaluation in a dynamic environment, such as the Web. Therefore, we performed our own evaluation of ten Web search engines over 896 queries (based on the assessor time we allocated, with no preference for this particular number). We briefly review this experimental environment, with more details available in Jensen [2006]. We then examine the question of reliability of conclusions drawn from such an evaluation. With prior techniques for estimating reliability inapplicable, we review reproducibility probability, and specifically the pointwise bootstrap estimate that we leverage. As with any reliability estimate, the conditions for the estimate itself to be reliable must be verified. We therefore continue by validating the reliability of these reproducibility probability estimates themselves, finding the minimum query sample size necessary in our environment to ensure that high reproducibility probability estimates from a sample correspond to similarly high levels on larger samples. 3.1 Experimental Environment We manually evaluated the top ten results of ten Web search engines over 896 queries without system pooling. The engines evaluated (AltaVista, AllTheWeb, Gigablast, Google, Lycos, MSN, MSN Tech Preview (now their main engine), Teoma, Wisenut, and Yahoo) are anonymized in no particular order as E1, E2,. . . E10. We randomly sampled 896 distinct queries from an AOL Search query log consisting of the entire search traffic, hundreds of millions of queries, for the two days 9/17 and 9/18, 2004. Queries in the log are lowercased and stripped of most punctuation. We were careful to randomly select from the true distribution of queries, creating a sample that approximates the frequency distribution of the query population [Beitzel et al. 2004b, 2006]. Results from all ten engines were pooled in a uniform interface based on canonicalized URL, including the surrogate document representations consisting of title, snippet, and the link to the page that assessors could optionally click through. For each query, a group of AOL editors, undergraduate and graduate computer science student assessors manually assigned each result as relevant or not relevant and selected a single best result from the entire pool. Assessors were instructed to imagine they had posed the query to determine the most likely information need based on only the typically short query from the log. Of course, this environment may suffer from problems of assigning navigational and informational interpretations to each query, shifting definitions of relevance, or differing perceptions of relevance based on the quality of surrogates. Our focus on finding conclusions that generalize to the query population as a whole, motivated us to use the limited number of assessor hours available to us to judge more queries rather than attempt to reduce such sources of random error. A more controlled evaluation environment would likely reduce the number of queries required, but at a cost of higher effort per query. Detailed statistics about this evaluation, including score distributions, and so on, are available in Jensen [2006]. For each engine over each query, we calculated three evaluation metrics: average precision at ten (precision averaged at each retrieved relevant document, limiting the denominator to the maximum number of retrieved results, ten), denoted as AvgP, precision at ten, denoted as P@10, and reciprocal rank of the best page, denoted as MRR for familiarity despite our point-wise use of it. See Table I for mean and median scores, ranked by mean. Next, we performed pairwise hypothesis testing for significant differences in median score using the Wilcoxon test as motivated by Section 2.2.1. While overall median is not terribly descriptive in Table I due to the discretized nature of a top-ten evaluation, simply the discrepancies between means and medians are indicative of non-normal distributions. We visualize the significant differences found as a hierarchy, where any path to a lower node represents that the higher node significantly outperforms the lower one (Figure 1). These hierarchies are simply a visualization conveying the same information as more common textual approaches to represent groups, such as those provided in TREC using IR-STAT-PAK [Blustein and Tague-Sutcliffe 1995]. We believe they are more readable than purely textual approaches when engines are not strictly ranked by their average scores. Nodes are collapsed together when they have an equivalent set of relationships. For example, E1 significantly outperforms every other engine under the MRR evaluation metric because there is a path from E1 to E2 to E3 and E10, and so on. E6 and E4 under MRR, by contrast, neither outperform nor are outperformed by E8, but they both significantly outperform E9. We make our best effort to place engines with larger scores higher, but favor readability over enforcing this strictly. As one would hope for any measure of reliability, all of our results produce figures that are associative, never requiring more than one node to represent an engine. Precision at ten is not shown, as it is nearly identical to average precision over the top ten results, with only two differences in significant conclusions, E6 > E8 with AvgP (read "engine six significantly outperforms engine eight") and E9 > E6 with P@10. While average precision is not typically used for retrieved sets of ten, it does help to reduce the number of tied scores across engines, compared to the more discretized P@10 (see Jensen [2006]), which we hypothesized would increase reliability. However, we do not find any meaningful differences in either the reliability or conclusions of AvgP versus P@10 (see Section 3.3), so for the remainder of this article we simply choose AvgP. 3.2 Bootstrapping Reproducibility Probability The algorithm we employ for bootstrap estimates of reproducibility probability in pairwise information retrieval evaluations is detailed in Figure 2. This is a specialization of the nonparametric bootstrap (from prior work described in Section 2.2.4, particularly an implementation of Equation 2) for the pairwise information retrieval evaluation problem. We first analyzed point-wise estimates such as this in a preliminary investigation [Jensen et al. 2005]. For generality, we leave the hypotheses stated as E A > E B ("engine A significantly outperforms engine B"), and the converse, because the specific hypotheses depend on the test chosen. The null hypothesis for both tests is that there is no difference between the two engines. We favor one-sided tests because the conclusions we are ultimately interested in are whether one engine outperforms another, not simply whether they differ. Implicit in deciding the direction of differences is the risk of type III error ("actually drawing firm but incorrect conclusions"), but for even minimal differences in engines this risk is small [Spiegelhalter and Freedman 1986]. For the conclusions included in our comparisons, calculating reproducibility probability for each direction makes this choice abundantly clear: we compare only conclusions with at least 90% reproducibility probability, in which case the converse conclusions typically have reproducibility probability less than 1%. Performing this procedure for every pair of k = 10 engines results in k(k - 1) = 90 reproducibility probability estimates, from which we simply discard the weakest estimate of each pair E A > E B or E B > E A leaving k(k -1)/2 = 45 estimates in our analyses. Throughout our experimentation, we set the number of bootstrap iterations, B = 2,401 (where B has no relation to engine B which we always represent as E B ). We have no preference for such an odd number, except that it is larger than the recommended minimums for bootstrap calculations, including those for bootstrapped hypothesis tests [Davidson and MacKinnon 2000]. Preliminary experimentation also confirmed this was more than sufficient. 3.3 Reliability of Point-wise Bootstrap Power Estimates The margin of error for reliability estimates due to variability in their pilot samples is rarely studied. Since we cannot evaluate the entire query population, any estimate of reliability is biased by the pilot query sample used to calculate it. We focus on determining the sample size required to ensure that high reproducibility probability estimates from any pilot sample correspond to similarly high reproducibility probability estimates for the same engine pair from our entire sample of 896 queries. Although this analysis must be performed separately in each evaluation environment, it serves as a simple method for establishing that high reproducibility probability estimates converge, in that they remain high across pilot samples at a particular pilot sample size. Requiring a certain number of pilot queries simply to estimate reproducibility probability would seem to dissolve all hope of reducing evaluation effort, but, as we demonstrate in Section 5, incorporating automatic judgments allows us to meet this minimum sample size without manually evaluating each query. In Figure 3, we provide an example of the growth of reproducibility probability for two example engine pairs with increasing bootstrap sample size m (and corresponding size of pilot samples n ). Hereafter, the Wilcoxon test with  = 0.10 is assumed. The scores for these three engines and their associated rankings are detailed in Section 3.1. The points on the lines of Figure 3 provide relatively smooth curves because they are estimates from the same pilot sample Q of all 896 queries. The error bars, however, represent the range of reproducibility probability estimates calculated using several other pilot samples Q created by randomly sampling m + 50 queries from Q. Throughout, we use a bootstrap sample size of 50 less than the pilot sample to dampen the issue of tied score differences due to duplicated queries created by sampling with repetition. Equivalent score differences result in tied ranks in the Wilcoxon test that reduce its accuracy. Error bars are not shown for m = 850, because creating pilot samples that vary substantially out of the 896 queries available is not possible. With over 600 queries, we are able to conclude that E2 reliably outperforms E3 (their median AvgP scores are .676 and .646, respectively). The candidate conclusion E5 > E3, however, clearly lacks the reproducibility probability to support it with these sample sizes. With a very large number of queries, we might expect to be able to distinguish between E3 and E5 reliably. As discussed in Section 2.2.4, increasing the sample size until significant differences are found is a dangerous and inefficient method of comparing engines. Any nonidentical engines can be declared significantly different with a large enough sample size. As our goal is to compare evaluations that use differing query samples, the sample sizes used in our analysis are determined by the reliability of reproducibility probability estimates for any engine pair, not the significance or reproducibility of particular conclusions. How can we use reproducibility probability to determine the sample size necessary to ensure reliable conclusions? One option would be to extrapolate reproducibility probability estimates from smaller sample sizes to project the sample size at which a conclusion will be reliable, as is often done for error rate. However, we can see from the error bars in Figure 3 that estimates based on small pilot samples vary wildly. Having only evaluated 450 queries, for example, we might extrapolate that with 650 we would find a reliable difference between E5 and E3. Instead, we favor a conservative approach of evaluating enough queries to make it clear that reproducibility probability estimates are converging to similar values across varying pilot samples for all pairs of engines. The discrepancies between reproducibility probability estimates from one pilot sample to another can be dramatic, even with substantial numbers of evaluated queries. For example, in Figure 4 we plot reproducibility probability estimates over all 45 pairs' candidate conclusions (E A > E B ) at bootstrap sample size 450 from varying pilot samples of 500 queries (created as described for Figure 3) versus identically sized estimates using all 896 queries as the pilot sample. Just as varying pilot samples produced large error margins in Figure 3, here we see that reproducibility probability estimates above 0.9 from a pilot of 500 queries might correspond to estimates as low as 0.4 for the same conclusion when using all 896 queries. To determine the minimum query sample size necessary to ensure that highly reproducible probability estimates from a given pilot sample will correspond to similarly high estimates from other samples, we employ a simple metric: the minimum reproducibility probability estimate from a pilot sample to ensure a reproducibility probability of at least 90% using our entire sample of 896 queries as the pilot. In Figure 4, for example, we would judge that 500 queries are insufficient because only sample estimates very near 1.0 meet this criterion. The corresponding y-axis estimates from all 896 queries are below 0.9 for even high sample estimates. As our metric decreases with larger sample sizes, the entire discrepancy graph continues to grow tighter to the diagonal. This analysis is a limited version of the conservative double bootstrap method proposed by De Martini [2006], which is computationally infeasible for our sample sizes. While such analysis could be performed on each engine pair individually, or by bucketing pairs by levels of difference, this creates the same dependencies that make error rate difficult to apply in new environments: defining the level of difference from unreliable preliminary values and an exaggerated dependence on the diversity of engines evaluated. Ensuring that none of the pairs of engines (especially those with small differences) yields a falsely high reproducibility probability estimate removes the dependence on determining levels of differences from small query sets. Rather than generating synthetic differences or engines, this analysis provides a minimum sample size that makes false positive errors unlikely for any new engine with similar score distribution in the given environment.

In Table II, we detail this analysis for our web search evaluation, presenting the minimum pm=n -50,0.10 from 20 varying pilot samples of size n to ensure pm=n -50,0.10  0.90 using all 896 as the pilot sample. For P@10 with samples of 300 queries and MRR with 450, even an estimate of 1.0 does not guarantee the estimate from all 896 is above 0.90 for the same candidate conclusion. Because of the margin of error for bootstrap estimates from a single pilot sample (which depends on B), minimums of 0.99 and above are difficult to enforce. With pilot samples of size 650, however, estimates begin to converge to ensure that high reproducibility probability from a sample corresponds to a high reproducibility probability estimate using all 896. Therefore, we conclude that 650 queries are necessary to estimate reproducibility probability reliably in our environment. Because this convergence takes place nearly 250 queries below the size of our entire sample of queries, we conclude that it is not an artifact of pilot sample size approaching that of our entire sample. This convergence takes place near the same size for each evaluation metric, leading us to hypothesize that the size of the pilot samples has more impact than the distributions under evaluation, and providing further evidence that even different engines would likely have reliable reproducibility probability estimates with this number of queries. We performed this same analysis on several TREC collections in Jensen [2006], finding conclusions difficult to generalize here, as such collections are not intended to represent a query population. 3.4 Conclusions From Manual Web Search Evaluation Having established that the point-wise bootstrap estimate of reproducibility probability is reliable for high reproducibility probability estimates on large enough sample sizes, we conclude by applying it to our manual evaluation. The metric we chose for measuring reliability is also convenient for providing an ad hoc correction to our reproducibility probability estimates. While we are interested in conclusions with at least 90% reproducibility probability, we saw that estimates from a pilot sample of even 800 queries must be above 98% to ensure this is valid for the population. To ensure we only examine reliable conclusions, therefore, we only include those with reproducibility probability of at least 99% for the remainder of our investigation. These benchmark high reproducibility probability conclusions from Wilcoxon tests using  = 0.10 are shown in Figure 5. While many conclusions are significant based on a Wilcoxon test (see Figure 1), approximately half of these have high reproducibility probability. 

4. SEMIAUTOMATIC EVALUATION 

We have shown that evaluations in dynamic environments are capable of yielding conclusions that are reproducible across query samples. However, the sample sizes necessary to ensure this are large, demanding substantial effort to evaluate each query manually. To reduce the required manual judgment effort so that evaluations can feasibly be repeated as the environment changes, we propose a semiautomatic evaluation framework for integrating automatic judgments with manual ones. Whereas small numbers of manually evaluated queries are of little use on their own due to the large number of false positives we saw in Section 3, combining them with automatic evaluation provides insight into conclusions. Although any automatic evaluation technique; using implicit preferences such as clickthrough data, fusion or metasearch based approaches, and so on, could be applied in this framework, we leverage the resource-based approach we developed in prior work: mining pseudo-relevance judgments from taxonomies such as the Open Directory Project (referred to as DMOZ) [Chowdhury 2005]. This serves as both an analysis of the utility of our resource-based automatic evaluation technique, and more importantly, a vehicle for developing our semiautomatic framework and demonstrating how to apply and validate it. First, we provide an overview of mining pseudo-relevance judgments from taxonomies and give conclusions derived from its automatic judgments alone. Next, we present the two basic ways in which automatic techniques can augment manual ones: by predicting conclusions that are likely to be found with larger query sets, by using a combination of a smaller number of manual judgments with automatic ones, and by filtering conclusions from small manual evaluations to improve their reliability. Finally, we present simple methods for leveraging each of these two aspects. We compare the reliable pairwise (engine A vs. engine B) conclusions they provide, with those drawn from our manual evaluation. Our analysis serves as an example of that which would be required using any automatic evaluation technique in a given environment, thus illustrating our framework and corresponding metrics for analyzing the utility of semiautomatic methods. 4.1 Mining Automatic Relevance Judgments To validate our semiautomatic framework, we employ automatic evaluation techniques developed in our previous work that address both the informational and navigational tasks [Beitzel et al. 2003a; Chowdhury 2005; Jensen 2006]. These automatic techniques leverage two types of resources that are likely to be available in most dynamic search environments: a log sufficiently representing the population of queries, and a human-edited taxonomy of documents in the collection that is large enough to include a representative sample of the collection. This could be any form of taxonomy, such as a corporate intranet directory, Web taxonomy, or large collection of categorized bookmarks, but it must represent human matches of topics to documents and not be biased towards particular search services. Our initial investigations into automatic evaluation used the DMOZ and LookSmart taxonomies to show that on the Web these techniques are not biased towards particular engines by the choice of taxonomy to mine judgments from, finding a 0.931 Pearson correlation between MRR1 scores (the reciprocal rank of the first relevant result in the retrieved list) of automatic evaluations using each [Beitzel et al. 2003b; Chowdhury and Soboroff 2002]. These purely automatic techniques have correlations in the 0.7 range with manual evaluation scores [Beitzel et al. 2003a; Chowdhury 2005]. For the following experimentation, we repeated our automatic evaluations on the Web using more recent DMOZ data (downloaded on 12/8/2004) applying their judgments to queries from the same log and results from the same set of ten Web search engines as in our manual evaluation. Details of this process are provided in Appendix A.1. An example of each technique is provided in Figure 6. For the navigational homepage/named page-finding task, we mine pseudo-relevance judgments using a technique we term Title-Match. It collects documents from the taxonomy whose editor-supplied titles exactly match a given query. These documents are treated as the "best" or "most relevant" documents for that query. For the informational, topical search task, we use a technique termed Category-Match. If the most specific component of a category name exactly matches a given query, all documents from that category are used as the pseudo-relevant set. Scores for the ten engines using these automatic techniques are available in Appendix A.1. As with manual evaluations, ranking engines by their average score and comparing rankings using correlations is insufficient. To compare only the reliable conclusions drawn from automatic evaluations with those from manual ones, we apply the same reproducibility probability analysis. Using the randomly selected Title-Matched queries as the pilot sample, and setting the bootstrap sample size equivalent to that of our manual evaluation, so that we would detect differences of comparable magnitude, we found those diagrammed in Figure 7. Comparing these conclusions with those of our manual evaluation in Figure 5 (duplicated for convenience), the automatic technique ranks E10 and E6 relatively lower, while it ranks E4 and E5 higher. Category-Match has a similar correlation, (see Appendix A.1). Although our focus is on demonstrating our framework, we investigated several methods of improving this correlation, including correcting for query popularity distribution, topical category distribution, and number of relevant results. None of these preliminary investigations substantially improved correlation [Jensen 2006]. 4.2 Integrating Manual and Automatic Judgments Although they are useful in examining evaluation characteristics over query sample sizes difficult to evaluate manually, we have seen that these purely automatic techniques are often inaccurate. We have also shown, in Section 3.3, that evaluation of search engines in dynamic environments demands a large query sample size even to estimate reproducibility probability. Incorporating automatic techniques with smaller numbers of manual judgments provides a sort of evaluation roadmap where there would otherwise have been little information about engines' relative performances. We focus on providing guidance for developing an intelligent evaluation strategy without having to manually evaluate the requisite number of queries for a reliable evaluation over every engine. We examine the two basic advantages semiautomatic methods can offer towards this goal: expanding the set of conclusions by predicting which will have high reproducibility probability with more manual evaluation, and pruning the set of conclusions from a manually judged query sample by removing those that do not seem to be reproducible across samples of this size. 4.2.1 Semiautomatic Prediction. To aid evaluators in focusing on conclusions that are likely to be reliable with further manual evaluation, we propose the technique detailed in Figure 8. Although automatic and manual judgments could also be combined per-result rather than on a query-by-query basis, we hypothesized that evaluating only some of the results from a query is not dramatically less effort than evaluating all of a query's results. We employ this probabilistic sampling rather than simply using the same, entire Q man sample in each bootstrap replication to reduce false positives by increasing the diversity of the samples. We assume the number of queries with automatic judgments is much larger than that used in each bootstrap replication to prevent a large number of tied scores. The primary goal of the following experimentation is to determine the range of rman and nman parameters at which the semiautomatic method predicts more of the correct conclusions than simply using Q man alone, while maintaining a relatively low probability of finding errant, false positive, conclusions. 4.2.2 Semiautomatic Filtering. To finalize conclusions from manually evaluated query samples too small to provide reliable conclusions on their own (removing the need for further judgments of the associated engines), we propose the technique detailed in Figure 9. This technique leverages the large sample sizes possible using automatic techniques to reduce the likelihood that initial conclusions are simply artifacts of the insufficient manual sample size. For sizes nman too small to yield reliable conclusions on their own (as discussed in Section 3.3), we hypothesize that filtering their conclusions with those from an automatic evaluation can reduce false positive errors enough to allow them to be accepted. The primary goal of our experimentation with this technique is to determine the range of sizes nman for which this effect is achieved, while not discarding too many of the conclusions from the purely manual evaluation that are actually correct. 4.3 Utility of Semiautomatic Evaluation The primary goal of these semiautomatic methods is to make repeating evaluations feasible in large, dynamic environments. They address this by providing insight into conclusions before completing an evaluation of every engine's results over the entire query sample size required to ensure reliability. This enables the development of intelligent evaluation strategies that reduce manual effort by removing engines from an evaluation in progress. However, acceptable levels of error for making decisions such as discarding an engine, depend on factors specific to evaluation goals, making conclusions about total effort difficult to generalize. The level of investigation (are we trying to divide the best engines from the worst, or determine whether one of the top two is truly better than the other?), or even the relative efficiency, monetary cost, and so on, of the engines considered to be likely, determines whether we are willing to tolerate some false alarms or missed conclusions. This is outside the scope of comparing the relative utility of various semiautomatic techniques. Therefore, we focus only on the general utility of these semiautomatic techniques versus manual judgments at finding the correct pairwise E A > E B conclusions using only a small pilot sample of manually evaluated queries. We quantify this utility by measuring the number of errant pairwise conclusions each of them yield and the number of correct conclusions they miss. This is a typical method of evaluating pairwise conclusions in filtering and categorization [Beitzel et al. 2004a; Manmatha et al. 2002]. Our motivation for focusing on binary pairwise conclusions themselves, as opposed to the underlying reproducibility probability estimates is twofold. First, we found in Section 3.3, that for reasonable sample sizes, only very high reproducibility probability estimates are reliable. Based on that analysis, throughout the following evaluation we only treat reproducibility probability estimates greater than 99% as asserting a conclusion. Second, practitioners are likely more concerned with making errant conclusions, rather than the accuracy of actual values of reproducibility probability estimates. For the same reasons, we provide the raw counts of errors rather than their percentages, as the magnitude of number of errors is often of at least as much concern as their proportions. Unlike using only the correlation of engine rankings to compare evaluations, this framework focuses on conclusions with high reproducibility probability, accounting for ties, and exposing whether an evaluation is too weak to find correct conclusions, or too confident in errant conclusions. Comparing evaluations is complicated by the need to define the "correct" conclusions. For example, if an evaluation of 300 queries finds that E A outperforms E B , and a larger evaluation of 800 queries finds the same thing, but if it also shows that 300 was not enough to reliably conclude that, is the conclusion E A > E B based on the initial 300 queries "errant?". To mitigate these issues each of our analyses spans several benchmark query sample sizes (most easily characterized by the bootstrap sample size, m, since we vary the size of the pilot samples). Because our baseline is purely manual judgments, the following analysis also provides an interesting corollary to our investigation into the reliability of reproducibility probability estimates from manual judgments as it further describes the type of errant conclusions they cause. 4.3.1 Results of Predicting from Auto-Manual Mixed Samples. First, we evaluate the utility of the prediction procedure described in Section 4.2.1 against simply using the pilot sample of manually evaluated queries alone. In the task of predicting what conclusions will be found with larger query sample sizes than those that have been evaluated, we seek to determine the range of r man (the ratio of manual to automatically judged queries) and nman (the size of the pilot sample) parameters for which the semiautomatic procedure substantially reduces errors compared to the manual. As we did in Section 3.3, we analyze the manual method by finding the set of conclusions from each of 20 different distinct query samples, Q man , with 50 more queries than the size we bootstrap. With a mixture of automatically and manually evaluated queries in the semiautomatic method, the need for a larger pilot manual sample than the bootstrap sample size needed to prevent a large number of ties is diminished. To ensure a conservative evaluation, we therefore used sets Q man of size nman = E(m ) for the semiautomatic method. man We begin with an examination of the navigational evaluation, using the best page MRR manual evaluation and the Title-Match automatic approach. In Figure 10 we compare the correct set of manual conclusions based on our benchmark pilot of all 896 queries bootstrapped into sets of 850 (a copy of Figure 5 for convenience) to those from one of the twenty semiautomatic prediction runs. This is in fact the worst case (the largest number of missed conclusions) out of the twenty pilot Q man samples of size 350 for the m = 850, E(m ) = 350 test. man Comparing these example semiautomatic conclusions in Figure 10 to those of the purely automatic technique in Figure 7, shows that the same general discrepancies exist, but their severity is markedly decreased. The semiautomatic still ranks E10 and E6 relatively too low and E4 and E5 higher than the manual, just as the automatic method did. However, the number of errors is dramatically fewer because it commits these infractions in only a small number of engine pairs, whereas the automatic method is certain of its incorrectness in many more cases. This serves as an illustrative example of how the aggregated errors in the following tables, such as Table III, are counted. Recalling that any path from a higher node to a lower one implies that engine outperforms the lower one, each of these sets contain 16 distinct conclusions (by chance). As recorded in the final row of Table III, this case of the semiautomatic technique misses 7 of the 16 correct conclusions, the largest absolute number of them across all 20 pilot samples. The missed conclusions are: r {E1, E2} > E4 r E1 > {E5, E7, E8, E10} r E6 > E9 Of the 16 conclusions this case draws, 9 are false alarms (errant false positives): r r r r {E2, E5} > {E10, E6} {E3, E4, E7} > E6 E5 > E8 E8 > E9. The first column is the benchmark bootstrap sample size taken from the pilot of all 896 that we compare with both the small manual and semiautomatic. The expected number of manual queries in each test bootstrap sample for the semiautomatic approach is given in the second column. This is equivalent to the test bootstrap sample size for the purely manual approach, as we are interested in how well a small number of manually evaluated queries predict the conclusions of a larger number. The probability of a false alarm is expressed as the ratio of the average number of false alarms to the average number of conclusions drawn. The maximum absolute number of false alarms across all 20 runs is given with its associated number of conclusions on that pilot sample. The probability of a miss is based on the number of correct conclusions, which is constant for each benchmark m (the same for the manual and semiautomatic method). There is one special case, E(m) = 0, where a purely automatic apman proach is provided. That case, does not make use of any pilot manual samples so there is only a single result. Table III includes selected rows where the semiautomatic approach reduces errors dramatically compared to the manual. Complete results for these and other ratios of manual to automatic results are provided in Appendix A.2. Predictions based on expanding the small manual sample with queries automatically evaluated using Title-Match typically miss approximately half as many of the correct conclusions as those from the manual sample alone. We examine predictions to four larger sizes: 300, to investigate our ability to predict dramatic differences with very few judgments, 450, the first point when high reproducibility probability estimates in the manual case begin to become reliable (see Table II), 600, where manual conclusions are reliable, and 850, the most detailed conclusions our set of judgments can support. The small number of correct conclusions (three) in the 300 queries case makes it difficult to choose one over the other as both the manual and semiautomatic methods have difficulty. The manual one often misses all three, while the semiautomatic one draws far too many conclusions in general, with over half of them being false alarms. Random performance, however, would draw nearly all false alarms, as only three conclusions of 45 are correct. Across the other prediction sizes, the manual method often misses nearly all the correct conclusions at a maximum; the semiautomatic often cuts this by half. Its number of false alarms, however, is greater than when using manual queries alone. This can be mitigated by incorporating a large enough ratio of manual queries (see Appendix A.2), which also reduces the number of conclusions it draws in general (the denominator of the probability of false alarm). Of course, larger available pilot samples for the manual method increase the number of conclusions it draws on average, subsequently decreasing the average number of misses with little increase in false alarms. When a larger number of manual judgments are available, the semiautomatic method may not be justified. Compared to not being able to draw any conclusions at all, even a prediction method prone to some degree of false alarms is likely useful; but how do we determine the bounds of this utility? Clearly, we need a combined metric to compare these two methods and determine when the semiautomatic method's relative benefits justify its use.

To directly compare the cost of errors in the manual and semiautomatic methods, we leverage a standard cost function (Equation 3) adopted from the Topic Detection and Tracking (TDT) conference [Manmatha et al. 2002]. A lower cost indicates fewer errors were made. This combines the ratios of errors shown in the table with relative costs for each type of error, and normalizes them by the relative number of correct conclusions in general. In our calculation of P(rel), we assume the maximum number of pair-wise conclusions that could be found among our 10 engines, with 45 as the denominator. Because the actual numbers of correct conclusions for our four prediction sizes are much less than 45, this may inherently provide extra weight to the false alarm errors. Equation 3: The TDT cost function. In the prediction task, we set Cmiss = 5  Cfa to reflect the importance of finding correct conclusions over suggesting errant ones. With the cost of misses twice, or equal to, false alarms, the manual method typically outperforms the semiautomatic, although this may be inflated by the aforementioned bias from P (rel ). We hypothesized that the key parameter was the ratio of manually judged queries in the bootstrap samples, regardless of the overall magnitude of the sample. In Figure 11, we show the costs for the manual and semiautomatic methods at various ratios of manually evaluated queries to the predicted query sample size when predicting sizes of 450, 600, and 850. This, and each of the following cost graphs include trend lines for readability, created using a second order polynomial regression since that yielded the largest R 2 fitness measure for each graph. We do not intend to make any general assertions about the shape of such curves, as it is obvious they differ depending on the automatic technique used. Errors are very highly correlated to the ratio of manual judgments regardless of total sample size. When less than 50% of the sample size to be predicted has been manually evaluated, the semiautomatic technique is more effective at predicting conclusions than the smaller number of manually judged queries alone. When roughly half of the sample size to be predicted has been manually evaluated, the cost of false alarms introduced by the semiautomatic method outweighs the reduction in missed correct conclusions compared to using the manual sample alone. We also hypothesized that conclusions with dramatically differing engines could be predicted with very few manual judgments. As we saw in the raw error counts of Table III, however, predicting conclusions at sample size 300 using the semiautomatic technique results in so many false alarms that its cost is higher than using the small manual sets alone, despite their propensity to miss many relevant conclusions (see Figure 12). When no manual judgments are available, however, the cost of errors from the purely automatic method is not terribly high. Again, it is likely to be useful compared to not being able to predict any conclusions whatsoever. The proposed semiautomatic framework and metrics enable us to compare the effectiveness of different automatic judgment techniques, in the hopes of moving beyond these na¨ve ones. We performed the same experiments and i analysis with combining the average precision at 10 manual judgments and Category-Match automatic judgments. The complete error counts are included in Appendix A.2. Errors in the semiautomatic informational evaluation are also very highly correlated with the ratio of manual results, as evidenced by Figure 13. As with Title-Match, predicting distant conclusions is more useful than nearer ones. However, integrating the Category-Match judgments does not offer as much benefit as those of Title-Match in the navigational evaluation. The number of false alarms does not decrease as quickly with larger ratios of manually evaluated queries, and the number of misses actually increases slightly; whereas it decreases with Title-Match. We believe this is because the Category-Match evaluation has more disagreement with the manual AvgP evaluation, causing the integration of more manual judgments to reduce the number of both correct and incorrect Category-Match predictions. Like the navigational evaluation, however, the manual samples alone often miss nearly all of the correct conclusions. Just as with the navigational evaluation, predicting conclusions for small sample sizes such as 300, is better achieved with very few manual judgments than with the semiautomatic technique, due to the large number of false alarms (see Appendix A.2). 4.3.2 Results of Filtering Conclusions from Small Manual Samples. Next, we evaluate the utility of the filtering procedure described in Section 4.2.2 as opposed to simply using the manually evaluated queries alone. The intent here is to reduce the number of false alarms from sample sizes too small to ensure reliability (as per Section 3.3). We seek to determine the range of manually evaluated queries nman for which the semiautomatic technique is beneficial. As in our analysis of prediction, we create 20 distinct query samples, Q man , of size nman = m + 50, and compare the set of conclusions from each to that of bootstrapping our entire pilot sample of 896 into sets of size m. We begin with an examination of the navigational evaluation, using the best page MRR manual evaluation and the Title-Match automatic approach (see Table IV). Using the same metrics as in the previous section, it is clear from Table IV that semiautomatic filtering reduces false alarms by approximately half throughout the experiments, while not substantially increasing misses, especially the maximum number of them. At nman = 500 there is a dramatic decrease in the number of false alarms. Interestingly, this correlates with the smallest size at which reproducibility probability estimates begin to become reliable across all metrics in Table II. To compare the semiautomatic method to the manual, with a single metric, we again use the TDT cost function defined in Equation 3. In contrast to predicting conclusions, filtering increases reliability of candidate conclusions, so we set the cost of false alarms to be twice that of misses. With the costs set equal, the manual approach is preferred for some sample sizes. In Figure 14, we show the cost of the manual and semiautomatic methods at increasing sample sizes. Here, the steep drop in false alarms causes the corresponding total cost to drop dramatically with samples of size 500 and above. By 600, the costs are roughly equivalent, but filtering can still be useful to ensure the reliability of a conclusion to a stricter standard, as evidenced by the raw counts in Table IV. The results for the informational search task and Category-Match automatic judgments are similar. Unlike the navigational evaluation, however, the number of false alarm and miss errors for the semiautomatic technique increases consistently with sample size. However, it still cuts the average number of false alarms by approximately half. Like the navigational evaluation, there is a drop in cost (see Figure 15) with samples of size 500 and above, but unlike it, the utility of filtering is also immediately diminished at that same point. 

5. CONCLUSIONS 

Dynamic environments such as the World Wide Web demand frequent repetition of costly search effectiveness evaluations. We have detailed a semiautomatic framework that combines automatic evaluation with manual judgments to make this feasible. We employ methods for comparing conclusions of one evaluation to another that go beyond simple correlation of engine rankings. Compared to small numbers of manually judged queries alone, semiautomatic prediction often reduces the number of missed correct conclusions by half, and semiautomatic filtering reduces the number of errant conclusions by half. This provides evaluators with insight into conclusions before naively evaluating every engine over the requisite number of queries for a reliable evaluation. To validate this framework, we leveraged reproducibility probability to determine which conclusions generalize to the query population as a whole. Applying this method to our own precision-oriented manual Web search evaluation over 896 queries shows that the query sample sizes required to ensure reliability in such evaluations are often much larger than those previously studied (650 in our environment). Because precision-oriented evaluations are performed without system pooling, they do not depend on the number of engines being judged, enabling evaluation strategies that reduce effort by discarding poorly performing engines early. However, semiautomatic methods such as those proposed, are needed to exploit this by building query samples of sufficient size before manually evaluating each one. In a conservative example from our navigational evaluation, a combination of semiautomatic filtering and prediction using only 300 manually judged queries would enable us to reliably conclude that E6 and E9 are indeed the worst performing engines. Removing them from the evaluation would reduce the size of the result pools in the following 350 queries left to evaluate by 19% based on overlap analysis in Jensen [2006]. There is a great deal of future work in this area. Using this framework, we will evaluate and refine other automatic evaluation techniques, especially implicit preferences such as clickthrough data, to determine which (or what combination) best enables semiautomatic methods to determine the correct conclusions with fewer manual judgments. We will also further investigate manual judgment techniques for those that optimize the effort required to reach a desired level of reliability, such as judgments with varying levels of relevance beyond binary. In addition, each automatic evaluation technique has its own spamming issues that need to be investigated.

Strategy-Based Instruction: Lessons Learned in Teaching the Effective and Efficient Use of Computer Applications
SURESH K. BHAVNANI University of Michigan FREDERICK A. PECK University of Colorado and FREDERICK REIF Carnegie Mellon University

Numerous studies have shown that many users do not acquire the knowledge necessary for the effective and efficient use of computer applications such as spreadsheets and Web-authoring tools. While many cognitive, cultural, and social reasons have been offered to explain this phenomenon, there have been few systematic attempts to address it. This article describes how we identified a framework to organize effective and efficient strategies to use computer applications and used an approach called strategy-based instruction to teach those strategies over five years to almost 400 students. Controlled experiments demonstrated that the instructional approach (1) enables students to learn strategies without harming command knowledge, (2) benefits students from technical and nontechnical majors, and (3) is robust across different instructional contexts and new applications. Real-world classroom experience of teaching strategy-based instruction over several instantiations has enabled the approach to be disseminated to other universities. The lessons learned throughout the process of design, implementation, evaluation, and dissemination should allow teaching a large number of users in many organizations to rapidly acquire the strategic knowledge to make more effective and efficient use of computer applications. 

1. INTRODUCTION 

This article reports our experience in the design, implementation, evaluation, and dissemination of a teaching approach called strategy-based instruction. This approach, evolved over five years through teaching almost 400 students, is designed to teach effective and efficient strategies to use complex computer applications such as spreadsheets and Web-authoring tools. Strategy-based instruction is motivated by several empirical studies which have reported that users have difficulty in acquiring effective and efficient strategies to use computer authoring applications. These empirical studies in the use of UNIX [Doane et al. 1990], word processors [Rosson 1983], spreadsheets [Nilsen et al. 1993; Cragg and King 1993] and computer-aided drafting (CAD) systems [Bhavnani and John 2000] have shown that, while most users can easily learn how to use basic commands, few of them acquire the knowledge to use the commands effectively and efficiently. For example, Nilsen et al. [1993], observed experienced spreadsheet users perform a task requiring a change of width of several adjacent columns with the exception of one. They found that most of the users modified the column widths one-by-one in order to avoid modifying the exception. However, another method of performing this task is to aggregate all the columns (including the exception), modify their widths, and then modify the exception back to its original width. This method avoids the time-consuming and error-prone steps of changing the width of each column. The method is therefore efficient because it reduces task time, and effective because it reduces errors in the end product. While some users do in fact acquire efficient and effective methods to become experts, why do many other users persist in using inefficient and ineffective methods to perform common computer tasks? Analyses of tasks like the previous one have led researchers to conclude that users are likely to change a method to perform a task if that method fails to achieve the intended goal. However, users are more likely to not change their methods if they succeed in achieving goals, even if the methods are inefficient. For example, Singley and Anderson [1989] state: "productions which produce clearly inappropriate actions contribute to poor initial performance on a transfer task but are quickly weeded out. Productions that generate actions which are merely non-optimal, however, are more difficult to detect and persist for longer periods," [p. 119] More recently, Fu and Gray [2004] suggest that most users persist in using suboptimal methods (e.g., using spaces to center a word on a page) because they are general purpose and therefore useful for a wide range of similar tasks. Furthermore, because such suboptimal methods provide immediate incremental feedback about progress towards the user's goal, they become preferred methods over time. Unfortunately for the user, these preferred methods are highly inefficient when used for complex tasks. Other reasons (for a review see Bhavnani and John [2000]) that might conspire against users becoming more effective and efficient in using computer applications include prior knowledge dominating current performance (thus leading to the Einstellung effect [Luchins and Luchins, 1970; Flemming et al. 1997]), a production bias [Carroll and Rosson 1987] which results in users focusing on the task at hand rather than on learning to use the system more efficiently, few opportunities for acquiring effective methods in work environments [Bhavnani et al. 1996], and the lack of effective and efficient methods made explicit in instructional material [Bhavnani and John 1996; Bhavnani 1998]. Furthermore, sources of knowledge to use computer applications, like help and user manuals, either focus on command instructions for simple tasks or focus on methods to perform complex tasks methods that are task-specific and difficult to generalize [Bhavnani 1998]. Given the many reasons that conspire against users acquiring efficient and effective methods, can explicit instruction address this issue? Over the last 20 years, researchers have stressed the need for computer literacy in undergraduate education [Sellars 1988], identified the different stages of computer literacy [Halaris and Sloan 1985], and designed approaches to teach application commands such as through minimalist documentation [Carroll et al. 1987]. More recently, researchers have explored extensions of computer literacy to new uses of technology, (e.g., as a communication device) and to social aspects of technology (e.g., ethical computing guidelines) [Goldweber et al. 1994; International Society for Technology in Education 1999; Hoffman and Blake 2003]. Finally there have been numerous approaches for teaching computer skills through online tutoring systems (e.g., Shaw and Polovina, [1999]), and comparisons of different instructional approaches such as tutors and discovery learning (e.g., Charney et al. [1990]). While this research provides important insights into the need and process of teaching users how to use computer applications, they have mostly focused on teaching how to use basic commands. However, several studies have shown that most users do not acquire efficient and effective methods just by learning how to use commands. For example, architects, despite formal CAD training to use commands and many years of experience using the CAD system, did not use effective and efficient methods in real-world tasks [Bhavnani et al. 1996]. To address this situation, we hypothesized that users might benefit from explicit instruction on effective and efficient methods to use computer applications. For example, in addition to learning how to select and modify columns in a spreadsheet, we hypothesized that users might benefit by also learning the method of dealing with exceptions. This method is general because it can be used to deal with a wide range of tasks involving different information objects (e.g., words, graphics, formulas). We refer to such general and goal-directed methods as strategies. To teach efficient and effective strategies, we first need to know what they are. Unfortunately, there has been relatively little research in identifying effective and efficient strategies1 for using computer applications. For example, as discussed earlier, Nilsen et al. [1993] identified a few efficient methods to perform spreadsheet tasks, and Lee and Barnard [1993] discuss a method to compare different parts of a document by using the SPLIT WINDOW command. In neither case has there been an attempt to generalize these methods, nor have they been organized in a framework. Furthermore, while much research has focused on teaching computer commands, we have found no attempts to teach and evaluate strategies to use computer applications. This article describes our experience over five years to (1) develop a framework to identify and organize strategies that generalize across computerauthoring applications and (2) design, implement, evaluate, and disseminate an instructional framework to teach these strategies. Section 2 reviews our prior research that focused on the design of the aforementioned two frameworks and how they were used to implement and evaluate a prototype for strategy-based instruction. Section 3 discusses our more recent research which evaluated the robustness of the prototype as it was extended to teach new applications to new populations and in a new context. Section 4 generalizes our experiences by indicating five lessons learned in teaching strategy-based instruction. We conclude with reflections on our experience in designing, implementing, and evaluating the strategy-based instruction to almost 400 students over five years and with research questions that need to be addressed to make more users effective and efficient in the use of computer applications. 

2. DESIGN, IMPLEMENTATION, AND EVALUATION OF A STRATEGY-BASED INSTRUCTIONAL PROTOTYPE 

Our research on strategy-based instruction began with the design of the following two frameworks: (1) a strategy framework that specified the general powers of computers which were used to identify efficient and effective strategies and (2) a strategy-based instructional framework that specified the general principles of instruction which were used to identify an approach for teaching strategies to novice users. These two frameworks were used to implement a prototype of strategy-based instruction, which was then evaluated in controlled classroom experiments. 2.1 Design of a Strategy Framework The strategy framework was developed through a literature review of effective and efficient methods for using computer applications [Bhavnani and John 2000], observation of users performing real-world tasks [Bhavnani et al. 1996], analysis of the features of applications [Bhavnani and John 1998], and a GOMS [Card et al. 1983; John and Kieras 1996] analysis of key strategies [Bhavnani and John 2000]. This process led us to identify a strategy framework consisting of 9 strategies based on 4 general functions or powers of computer applications: iteration, propagation, organization, and visualization.2 The first column of Appendix-A shows the 9 general strategies, and the remaining columns show how those strategies can be instantiated across different authoring applications. For example, the strategy (described earlier) to modify many columns with an exception (an instantiation of the "handle exceptions last" strategy in Appendix-A) is efficient because it exploits the power of iteration provided by most authoring tools. Instead of the user modifying each column, the strategy enables the iterative task to be delegated to the computer, given some constraints. Such strategies are critical to learn because real-world users (as shown by Nilsen et al. [1993]) typically miss opportunities to use such strategies. Furthermore, a GOMS analysis of such iteration strategies found that they could lead to a reduction in time of between 40-70%, and to a reduction in the probability of errors [Bhavnani and John 2000]. Similarly, propagation strategies exploit the power of computers to modify objects that are connected through explicit dependencies. These strategies allow users to propagate changes to large numbers of interconnected objects. For example, the strategy "make dependencies known to the computer" is useful in word processors in the use of style. Here a user can create paragraphs that need to share a common format specification; when the specification is modified, all the dependent paragraphs are automatically changed. Similarly, formulas in a spreadsheet can be linked to dependent data or graphic elements in a CAD system can be linked to a common graphic definition of objects. Organization strategies exploit the power of computers to construct and maintain the organization of information such as tables and lists. For example, the strategy "make organizations known to the computer" is useful in a word processor in the use of a table. In contrast to using tabs to construct a table (whose organization may not be maintained when the contents are modified), using the INSERT TABLE command in MSWord enables the computer to maintain the tabular organization under any modification of its contents. Similarly, data for different years in a spreadsheet can be organized in separate sheets for easy access and manipulation. Finally, visualization strategies exploit the power of computers to display information selectively without altering its content. For example, the strategy "view parts of spread-out information to fit simultaneously on the screen" addresses the limited screen space of most computer screens. For instance, a user might need to compare the contents of a table at the beginning of a long word processing document to the contents of a table in the middle of the same document. In such cases, instead of scrolling back and forth between the tables, it is more efficient and less error-prone to set up distinct views (e.g., through the use of the SPLIT-WINDOW command) that focus on each table and that can be viewed simultaneously on the screen. This strategy is clearly useful in large documents containing text, numbers, or graphic elements and is therefore generally useful across applications using such objects. As in most performance-improvement methods, these strategies trade off the effort to use a strategy and the realized benefits. For example, iteration strategies are more beneficial when they are used for many elements rather than a few [Bhavnani and John 1998]. Furthermore, it would not be compelling to use a strategy that saves time when time is not a critical factor to a user. Therefore, strategies are more cost effective for complex tasks and where the performance gains are of value to the user. It is therefore as important to know when to use a strategy as it is to know how to execute it. 2.2 Design of a Strategy-Based Instructional Framework As suggested by many researchers (e.g., Klahr and Carver [1988], Gong and Elkerton, [1990]), we decided to model the knowledge required to use the strategies before we designed the instruction. This approach enabled us to gain a precise understanding of the knowledge to be imparted. A GOMS analysis of the strategies [Bhavnani and John 2000] revealed that each requires three knowledge components. (1) Command knowledge that includes knowledge of the existence of commands, their location, and the methods to use them. In GOMS terms, there must exist a method with operators to execute the command. (2) Application-specific strategic knowledge that includes knowledge of the existence of efficient strategies within an application, conditions of when to use a strategy, and the method to execute the strategy by sequencing different commands. In GOMS terms, there must be a selection rule that recognizes when to use this strategy, and an associated method to sequence different commands to execute the strategy. (3) Application-general strategic knowledge that provides knowledge of how particular applicationspecific strategies can be applied across applications. In GOMS terms, the selection rules for strategies are generally stated and can be instantiated in different task situations. While the GOMS modeling guided us towards a more precise understanding of what to teach, it did not provide guidance on how to teach the above three knowledge components. Therefore, we exploited existing educational research to understand how to teach the knowledge components. We now describe how we designed an instructional framework by combining our understanding of the knowledge components required to use effective and efficient strategies with the existing research on how best to teach different types of skills. 2.2.1 Command Knowledge. Our approach of when and how to teach command knowledge was guided by previous research in the psychology and education literature. Anderson [2000, p. 387] recommended that it was important to teach component skills before teaching high-level skills that included those component skills. This suggested to us that command knowledge should be taught before strategies that used those commands. This was further verified in our early pilots [Bhavnani et al. 1999] where we attempted to first teach general strategies as a unifying framework for later teaching the commands. However, this approach resulted in a course that was not motivating for the students because the strategies were too abstract without the command knowledge. Our final prototype, therefore, taught commands before teaching the strategies. Prior research has also shown the importance of active processing, whereby students are made to engage in a task instead of merely observing passively how others perform the task. Such active processing has been shown to result in a deeper understanding of the imparted knowledge [Nicholls 1989; Nolen 1996, 2003]. We therefore designed the instruction in two parts. (1) Demonstration of commands where the students watched the instructor execute the steps of a command. While this first step was passive, it enabled the student to begin to acquire the declarative knowledge of the location, goal, and process of using the command. (2) Practice of commands where the students performed on their own a task that was different from the one demonstrated but required the same commands. Such an approach was used to encourage active processing. For example, the students were shown how to use commands to view a document such as SPLIT WINDOW, SCROLL, and NEW WINDOW, and then given an opportunity to practice the commands in the context of a new task. Prior research also suggested that students typically have higher intrinsic motivation when they are taught with examples that are relevant to them and to the real-world [Pintrich and Schunk 1996; Myers 1989; McCade 2001; Eisenberg and Johnson 2002]. The in-class tasks for demonstration and practice (as shown in Appendix-D) were therefore carefully designed to be meaningful and relevant to the students. For example, the tasks for the technical students included organizing information related to salaries for teaching assistants, and tasks for the art students included organizing information related to art and music. These tasks were designed based on input from the student instructors who had experience in teaching the CMU freshman students in previous years, and therefore had first-hand knowledge of the tasks that were relevant to these students. 2.2.2 Application-Specific Strategic Knowledge. Our approach to teaching application-specific strategic knowledge was guided by research which has shown that higher retention of knowledge can often be achieved when students construct knowledge through the process of guided discovery (e.g., Brown and Palinscar [1989]). In addition to using the notion of guided discovery, our approach was also informed by the importance of making explicit the conditions under which a strategy is useful [Singley and Anderson 1989]. We implemented the notion of guided discovery by engaging the students in an interactive session where they were asked to describe how they would use the commands just practiced to efficiently perform a complex task. For example, after being introduced to the SPLIT WINDOW command, the students were shown a long document with many short bulleted lists and asked to discuss a method for bringing three nonadjacent items from the last list to the third list. The instructors provided feedback for the methods suggested by the students by discussing the trade-offs, and then demonstrated the efficient strategy of splitting the window before moving the items. These discussions made explicit the conditions which best motivated the use of the applicationspecific strategy. 2.2.3 Application-General Strategic Knowledge. Our approach to teaching application-general strategic knowledge was guided by two important findings in the acquisition of knowledge: (1) the transfer of strategies can be achieved by teaching the general form of a strategy [Bossock and Holyoak 1989; Fong et al. 1986] and through multiple examples of the same strategy [Gick and Holyoak 1983], and (2) higher retention can be achieved by revisiting the same knowledge at regular and reasonably extended intervals in a phenomenon called the spacing effect (e.g., Hintzman [1969]; Underwood [1969], Anderson and Milson [1989]; Anderson [2000]). We implemented the notion of teaching application-general strategic knowledge in multiple contexts by first presenting the general form of the strategy, and then showing how it could be used across many computer applications. For example, after the split-window strategy was discussed and demonstrated within an application (as discussed above), the strategy was generalized to "view parts of spread-out information to fit simultaneously on the screen" by pointing it out in a strategy handout (similar to the table shown in AppendixA). This handout contained all the strategies and examples of their instantiation across the applications taught. Similarly, the application-specific strategy of using the STYLES command in Word to efficiently and effectively modify text was generalized to the strategy "make dependencies known to the computer" by pointing it out in the handout. To leverage the spacing effect to enhance retention, we taught the same strategy in subsequent applications. 2.3 Implementation of the Strategy-Based Instructional Framework The strategy-based instructional framework was implemented as a prototype in the context of an existing seven-week required course for freshman students at Carnegie Mellon University (CMU). This course focused on teaching a set of commands to the freshman students. To provide an experimental comparison, our implementation taught the same commands, and the same sequence of applications (UNIX, MSWord, then Excel), and took the same instruction time as the regular CMU instruction (3 classes of 50 minutes each for UNIX, MSWord, and MSExcel). The strategy-based implementation followed the template shown in Figure 1. The command instruction began with a demonstration of a small set of commands in the context of simple tasks (Step 1). For example, the instructor introduced different ways to view a document in MSWord through the use of SPLIT WINDOW and SCROLL. The students were then told to practice the commands just taught with a new task (Step 2). This demonstration and practice was followed by instruction for the next set of commands (Step 3). In this case, these commands involved using NEW WINDOW and ZOOM. All the commands taught until then in the class were summarized (Step 4). The command instruction was followed by application-specific strategy instruction. For example, the instructor opened a 3-page document that had 11 different bulleted lists. The students were asked how they would move 3 nonadjacent bulleted items from the last list to the third list in the document. Here the instructor encouraged the students to discuss alternate methods to do the task by using the commands they had just learned (Step 5). Then the instructor stated that the advantage of using the SPLIT WINDOW or NEW WINDOW to perform the task was to avoid having to repeatedly scroll up and down between the lists (Step 6). The instructor demonstrated this method in a practice document and contrasted it with the inefficient method of scrolling (Step 7). The students were then given the strategy handout containing the strategies and their instantiations across the applications (as previously described). The application-specific strategy just taught was abstracted to the general strategy "view parts of spread-out information to fit simultaneously on the screen". The students were asked to locate the strategy just taught in their handout and were shown how they generalized across applications (Step 8). Steps 5-8 were repeated for other complex tasks demonstrating the utility of other strategies (Step 9). All the strategies presented in the class were then summarized by explicitly pointing them out in the handout (Step 10). The steps were repeated for each application (UNIX, MSWord, and Excel). The presented approach contrasts with the traditional approach of teaching such applications. For example, instructors of the existing CMU course are taught to teach commands in the context of simple tasks (Steps 1-3). However, the students never receive instruction on how to assemble the commands to perform complex tasks effectively and efficiently. Thus they do not receive any instruction on the general nature of effective and efficient methods and do not acquire strategic knowledge that they can use across applications. Both versions of the course were taught by undergraduate students who were trained to teach the respective courses. All the time in the traditional course was spent on teaching pertinent commands and on examples illustrating their use. In the strategy-based course, the time was spent teaching both commands and general strategies. But, because the teaching of these was tightly integrated, the total time spent by students was the same in both courses. 2.4 Evaluation of the Strategy-Based Instruction Prototype We conducted two experiments to test the strategy-based instruction with two different populations. The first experiment (CMU-1) was conducted with science and engineering students and was designed to address the question "Does the proposed strategy-based instructional approach help the acquisition of strategic knowledge without harming the acquisition of command knowledge?" The second experiment (CMU-2) was conducted with a population of art students and addressed the question "How effective is strategy-based instruction for teaching students with non-technical majors?" 2.4.1 Method for CMU-1. The students were divided into two groups. The command group received the instruction ordinarily provided by CMU, and the strategy group received the experimental strategy-based instruction. Students were randomly assigned across both treatments, and then balanced by major (i.e., each treatment had equal numbers of students from each technical discipline). This assignment resulted in 87 students in the command group and 84 students in the strategy group. Instructor training. Each group had a main instructor and a secondary instructor, both of whom were undergraduate students at the university. The main instructor taught the course content in front of the classroom through a desktop computer connected to an overhead projector. The role of the secondary instructor was to provide assistance to students who had difficulty following the instruction or had trouble with the computers. The instructors in both conditions had taught the existing CMU course before, had equivalent experience in teaching and in the use of commands, and were considered to be effective instructors. All the instructors therefore had received the same instruction on how to teach commands, but the strategy-group instructors got extra instruction to teach the general strategies. Instructors in both groups were given teaching guides to help teach content. The teaching guides for the command group consisted of a list of commands and practice files developed by a commercial company. This instructional approach is typically used to teach computer applications in educational and commercial organizations and therefore represented a realistic comparison condition. The teaching guides for the strategy group included the same commands as those taught in the command group, but in addition contained instruction on how to teach the general strategies with appropriate demonstration and practice examples. Furthermore, the strategy instruction included problemsolving requiring interaction with the students. Our guides provided the overall structure for instruction but excluded the actual words to be used during instruction. Thus the guide allowed situated elaboration and improvisation by the instructors. While such teaching guides provide structure to scaffold new instructors and enable teaching consistency across instructors, they also allow for creative instructor elaboration which could lead to improved learning by students [Bereiter 2002; Borko and Livingston 1989; Palinscar 1998; Yinger 1987]. This balance of structure and improvisation is similar to how experienced teachers typically design their instruction [Brown and Edelson 2001]. The instruction in the strategy group could be done in the same amount of time as in the command group because the strategies were tightly integrated into the teaching of the commands and concretely illustrated in the strategy handout. In addition, we created handouts to explicitly show students how the strategies in Appendix A generalized across applications.3 Posttest tasks. The posttest was given in a computer laboratory and consisted of three sets of tasks (one each in UNIX, MSWord, and MSExcel as shown in Appendix B). The students were presented with tasks and instructions on paper. These tasks required them to use online applications and files which consisted of a UNIX directory populated with files, MSWord to create a new file, and an MSExcel file containing a spreadsheet. The instructions also required the students to complete, after each task, a brief questionnaire in which they were asked to explain their method for completing the task and their rationale for using that method. Finally, the students were instructed to save the resulting directories and files which were checked before the students left the computer laboratory. The tasks were designed to take a maximum of 1.5 hours, but there was no time limit given to the students. The students were spaced out in the computer laboratory to ensure that they could not see the details of the computer screen of other students in the experiment. Embedded in the previous three sets of tasks were 13 opportunities4 (shown in the first column of Table I to use the 9 general strategies (shown in Appendix A). For example, Task 3A in MSExcel required the students to find which of two pairs of days had the lowest temperature in a large spreadsheet containing temperature data. One way to perform this task was to scroll up and down the spreadsheet in order to compare the dates. Another way to perform the task was to split the screen into two panes so that the top pane would display at all times the column headings containing the dates, and the bottom would be used to scroll through the temperature data. This approach provided a quicker and more accurate comparison of dates. Each of these strategy opportunities was different in content from the tasks taught in the experimental course. Participation in the posttest was voluntary. Students were requested to participate in the posttest for $25 and were informed that their performance on the posttest would not affect their grade. This recruitment yielded 42 of the total 87 students from the command group, and 48 of the total 84 students from the strategy group.

3 See

Bhavnani et al. [2001] for a detailed description of the course implementation, and http://www.si.umich.edu/StrategyCourse/ for the teaching guides and handouts (also available in the ACM Digital Library). 4 One MSWord task did not motivate the use of the strategy that it was designed to test, and therefore was excluded from the analysis. This left 12 opportunities to use 8 strategies. 

2.4.2 Method for CMU-2. The method for CMU-2 was similar to CMU-1 except that the population consisted of only art students. The command group and the strategy group consisted of 24 and 25 art students, respectively. Similar to the CMU-1 experiment, students were requested to participate in the posttest for $25. The recruitment yielded 17 students from the command group and 19 students from the strategy group.

2.4.3 Data Collection. We collected and analyzed five types of data. (1) Screen-capture videos, which recorded the computer interaction of each student. (2) Command logs, which consisted of UNIX history files and MSExcel Macro files. These files contained a list of commands executed by the students in a format that could be automatically analyzed by computer scripts. (3) Completed task files, which were collected in Word and Excel. (4) Qualitative descriptions, authored by each of the students, in which they explained how they completed each task, and the rationale for their method were collected. (5) Exam scores, which were based on an exam required by all students enrolled in the course and tested only command knowledge were used to check whether the strategy instruction harmed command knowledge. 2.4.4 Analysis. The focus of our study was to analyze whether or not students recognized the opportunity to use a strategy. Therefore, for each student, we analyzed each of the 12 strategy opportunities for evidence of strategy use. For each opportunity, students were given a binary score indicating whether or not they used the strategy in that particular opportunity. This led to nominallevel data for each strategy (i.e., a student either used a strategy or did not). Table I shows the strategy opportunities for each task, the criteria for strategy use, and the method used to analyze the strategy. Where possible, we used computational methods to analyze the data in order to reduce errors. In all cases, whenever there was any chance for ambiguity, the data were double-checked in another form such as the screen capture video or the written descriptions. For example, as shown in row E of Table I, Task 2A in MSWord included an opportunity to use the strategy "make dependencies known to the computer". Students were given credit for having used this strategy if they explicitly defined and used at least one style using the MSWord STYLE command. Strategy use was assessed by first looking at each student's completed task file for evidence of style use, and then confirmed through analysis of the screen capture videos. This confirmation was necessary because MSWord sometimes automatically assigns styles to text. 2.4.5 Results: CMU-1. We performed a two-step analysis of the data. First, we tested for an overall effect by analyzing the proportion of strategy opportunities used by each student in the command and strategy groups. For the command group, the mean proportion of strategies used was 42.66% [SD=.16], while for the strategy group, the mean proportion of strategies used was 68.58% [SD=.21]. Therefore, a typical student in the command group used 42.66% of the strategy opportunities shown in Table II, while a typical student in the strategy group used 68.58%. This difference was statistically significant based on a t-test of the proportions (t(88)=6.73, p<.001). In the second step of our analysis, we tested each strategy individually to provide a more fine-grained view of the data. As reported elsewhere [Bhavnani et al. 2001] and as shown in Table II, the strategy group did significantly better than the command group in exploiting seven strategy opportunities (p<0.05 for each of the seven strategies based on chi-square tests on the frequencies in each group5 ). Furthermore, there was no statistically significant difference in command knowledge between the two groups as measured by mean scores on the in-class exams which tested only command knowledge as discussed previously (mean for command group: 96.07, mean for strategy group: 95.54 t(511)=0.63, p=.53). The results demonstrate that students could, in fact, be taught to recognize opportunities to use efficient strategies and to execute them. Closer inspection showed that five strategies opportunities (D, I, J, K, L in Table II) may be easily acquired just by learning commands. For example, even though the command group was given only command instruction, all of them recognized the opportunity to use formulas in the spreadsheet task (I). One explanation is that once commands such as formulas in MSExcel are learned, the alternate methods (e.g., doing manual calculations in the spreadsheet) are just too inefficient to be considered. This could also explain why two sets of strategy opportunities (F/K and C/J), each testing the same general strategy but with different commands, had very different usage profiles especially for the command group. These results also confirm laboratory studies which show that, under certain conditions, a strategy to reuse information through cut-and-paste can be discovered just by knowing commands [Charman and Howes 2003]. Future research needs to explore more closely what makes certain strategy opportunities more difficult to detect compared to others. Although the strategy group did significantly better for strategy opportunities B and H, the actual numbers of students exploiting those opportunities was small. This suggests that these strategies required more instruction than we provided. However, because zoom often leads users to become disoriented, the lack of zoom use could also be a conscious choice to avoid such problems. Overall, the results showed that most of the strategies can be taught in the same amount of time as the regular approach without harming the acquisition of command-knowledge. CMU-2. As shown in Table II, the results of CMU-2 were similar to those in CMU-1. The overall effect remained (command group mean proportion: 37.25% [SD=.19], strategy group mean proportion: 58.33% [SD=.20], t(34)=3.21, p<.01). Furthermore, six of the strategy opportunities showed a significant difference between the two groups. These results show the utility of the strategybased instruction for students with very little technical background. Finally, command knowledge was significantly higher in the strategy group than the command group (command group mean: 86.71; strategy group mean: 95.47, t(151)=3.97, p<.001). This suggests that teaching commands with strategies might have the added benefit of improving command knowledge. 2.5 Summary of the Strategy-Based Instructional Prototype In Phase-1 of our research we (1) identified a strategy framework that helped to organize nine effective and efficient strategies that were general across applications, (2) identified a framework to teach the knowledge components, (3) implemented the instructional framework, and (4) tested the instructional design in two controlled experiments. Furthermore, we learned that a few strategies did not require explicit instruction, while others required more instruction than we expected. The results of these experiments showed that the strategy-based instruction (1) enabled students to learn effective and efficient strategies, (2) benefited student populations with both technical and nontechnical majors, (3) did not require extra time compared to the traditional approach focused on command knowledge, and (4) did not harm the acquisition of command knowledge. In our next phase, we tested whether the instructional framework could be extended to new applications and in a different context compared to the prototype.

3. EXTENSION TO A NEW APPLICATION AND A NEW POPULATION 

Because the first author moved to the University of Michigan, we had the opportunity to test whether the strategy framework and the instructional framework were robust in the new university context with a different population of freshman art students. Such a test of robustness is critical because educational interventions can easily fail to produce positive results when used in a context where the original authors have less control [Brown 1992]. However, testing the robustness of an instructional approach in a new realworld context usually comes at the cost of trading off experimental control, a common problem in testing educational interventions in real world contexts [National Research Council 2000; Brown 1992]. As described in the following section, we had to make many modifications to the instructional design to fit the new context. This loss of control prevented us from making statistical comparisons with the experiments at CMU. Furthermore, unlike many user tests of systems that take around an hour per user, educational field experiments may take an entire semester. This long time span reduces the kind of manipulations that can be done practically. However, the experience of testing our approach in a new context led to insights of using strategy-based instruction in new contexts and how the approach could be disseminated to different institutions. 3.1 Extension of the Strategy-Based Instructional Design The new context had three major differences that tested the robustness of our strategy and instructional frameworks. (1) UNIX was not taught as it was not considered an application that was particularly useful for the Michigan Art students. (2) The Art Department faculty requested that the course be extended to teach Dreamweaver (a Web-authoring application). (3) At the request of the faculty, an extra day was added for teaching each application to enable the students to perform a summative task (e.g., creating a resume in Word). To test the changes, we asked the question: "Could the strategy-based instructional approach be applied to teach new applications?" 3.1.1 Extension to a New Application. To develop teaching guides for Dreamweaver, we followed a three-step process. (1) Identify in each application the commands that were appropriate to teach freshman art students. (This was done in consultation with the Arts faculty.) (2) Instantiate the 9 general strategies in Dreamweaver. (3) Construct for each application a 4-day teaching guide that closely followed our original instructional design framework but added an extra day for each application. We decided to teach in Dreamweaver 17 commands, which the Arts faculty agreed were useful for students, to build a basic Web site where the students could upload their graphics and music files. The commands ranged from OPEN NEW FILE to DEFINE WEBSITE and PUBLISH WEBSITE. Appendix A shows how each of the 9 strategies were instantiated in Dreamweaver using the chosen commands. For example, the strategy "make organizations known to the computer" was instantiated in Dreamweaver through commands to create and modify tables which are used by most Web site designers to organize content in a Web page. Appendix D shows the portions of the Dreamweaver teaching guide concerned with the commands to use tables. While the strategies themselves generalized with minimal effort, our main difficulty was in designing the demonstration, practice, and problems-solving tasks for Dreamweaver. This was because, while it is easy to construct Web sites that have minimal functionality, a credible looking Web site that was motivating and relevant to the Art students required a considerable effort in graphic design. As we did not have these graphic design skills, we employed a graphic designer to construct a Web site with many well-designed pages, each of which demonstrated the use of the commands and strategies that we had chosen to teach. Therefore, we were able to instantiate the nine strategies in a new application and to create a teaching guide for the new application. This demonstrated that we were successful in extending both the strategy framework and the instructional design framework to a new application. Furthermore, we learned that applications varied in the amount of setup costs to build demonstration and practice examples that were relevant and motivating. 3.2 Effect of Strategy-Based Instruction on a New Population To test the robustness of the instructional design in a new context, we conducted an experiment (henceforth referred to as UM-1) with the Art students at Michigan. Our goal led to the following research question: "How robust was the strategy-based instructional approach when implemented in a new context with a new population, and with less control?" Because retention of knowledge is critical in learning, we also wished to investigate how well the strategies were retained over time. This was not investigated in the CMU experiments as the posttests were given immediately after the instruction set was complete. The above goal led to the following research question: "Does the strategy-based instructional approach help the retention of strategic knowledge over time?" In addition to the primary research goals to test robustness and retention, we also used this opportunity to perform a small exploratory study to probe the role of the general form of a strategy in its transfer across applications. 3.2.1 Method for Experiment with Art Students (UM-1). As discussed in Section 2.2, our GOMS modeling helped to pinpoint three types of knowledge that were important for the strategic use of computer applications: (1) Command knowledge (e.g., the existence of the split window command, its location, and the method of how to use it); (2) Application-specific strategic knowledge (e.g., the existence of a strategy to modify distant parts of a MSWord document by using the split-window command); (3) Application-general strategic knowledge (e.g., the general form of the application-specific strategy so that it can be used across applications). The CMU-1 and CMU-2 experiments compared students who were taught only command knowledge (command group), to students who were taught all the three knowledge components (strategy group). In the current experiment, fifty Art students at the University of Michigan were randomly divided into two equal groups and then balanced by prior experience in Word and Excel. One group was given the same instruction as the strategy group in the CMU experiments (with the modifications discussed in Section 3.1) and is therefore still called the strategy group. Analysis of how the strategy group performed the posttest tasks will reveal robustness of the instructional framework when taught in a new context. In addition to this analysis, we explored the explicit role of the general form of the strategy by teaching the second half of the class only command knowledge and application-specific strategic knowledge. For example, while we taught how to use the SPLIT WINDOW command and when to use it strategically within MSWord, we did not teach the general form of the strategy "view parts of spread out information simultaneously on the screen". Because this group was taught command knowledge and only the application-specific strategic knowledge, it was called the command + ap-specific group. Furthermore, to test if the general form of the strategy was necessary for transfer across applications, we also did not teach the strategy "view parts of spread out information simultaneously on the screen" in Excel in the strategy group. Therefore, while the command + ap-specific group had no general strategy instruction at all, the strategy group had instruction for all general strategies in all applications except for the above strategy in Excel. This manipulation allowed us to explore if (1) the general form of the strategy was necessary for its use within an application, and (2) if the general form was necessary for transferring that knowledge across applications. The latter was tested in a single condition for exploratory purposes. Table III shows the different knowledge components taught in each group. The posttest data were collected as part of the final exam required by all students. This yielded 25 students in each condition. The posttest for each group was identical to the MSWord and MSExcel posttest tasks used in CMU-1. Because tasks for Dreamweaver require long set-up times exceeding the time constraints of the exam, we were unable to test strategies in that application. The retention test (see Appendix C) was conducted one month later to explore how well the students in both conditions retained the strategies. The students were requested to take part in the retention study for $406 . This yielded 18 and 20 students from the command + ap-specific group and the strategy group, respectively, where all these students constituted 76% of the original class. There was no statistically significant difference between the posttest scores of the students taking the retention test and the posttest scores for the students who did not take it. This result suggests the absence of a self-selection bias in the students who opted to take the retention test. The students in the retention test were given isomorphs of the posttest tasks described earlier, and the analysis criteria were identical to those used in the CMU experiments.

3.2.2 Results and Discussion. As discussed earlier, a statistical comparison between UM-1 and CMU-2 could not be done because the UM-1 students had one extra day of instruction per application. We therefore present the data in Figure 2, mainly to provide a direct comparison with the CMU-2 data. As shown7 , the strategy group had a high rate of strategy use, comparable to the strategy group in CMU-2 (also Art students). In four strategy opportunities (I, J, K, and L), there was a small drop (5%, 10%, 7%, and 11%) in strategy use. The rest had either equal or much more strategy use. The extra instruction might explain the higher scores that UM-1 students acquired in the posttest for strategy opportunities. But the results certainly suggest that the instructional framework was robust in a new context. Just as in the CMU experiments, we performed a two-step analysis procedure. In the first step, we tested for an overall effect by analyzing the proportion of strategy opportunities used by each student in the command+appspecific and in the strategy groups. This test showed that the mean proportions for both groups was very high and that no significant difference existed between the two groups (command+ap-specific group mean proportion: 72.89% [SD=.18], strategy group mean proportion: 72.44% [SD=.24], t(48)=.0034, p=.997). This result was expected because, with the exception of "view parts of spread out information simultaneously on the screen" in MSExcel, both groups were taught application-specific strategies in the applications in which they were assessed. As discussed earlier, neither the command + app-specific group nor the strategy group in UM-1 was taught the general form of the strategy "view parts of spread out information simultaneously on the screen" in MSExcel. However, the strategy group was taught the general form of the strategy in MSWord, while the command + ap-specific group was taught only the application-specific form of the above strategy in MSWord. To test if this had an effect on transfer, we performed the second step of our analysis, a Chi-square test on each individual strategy opportunity. As shown in Table IV, there were no significant differences between the groups on directly-instructed strategies (see Table VI in Appendix E for details of each statistical comparison). However, based on a Chi-square test on the frequencies in each group, there was a significant difference (df=1, n=50) = 3.99, p<.05) between the command + ap specific group and the strategy group for the untaught, transfer strategy (G) "view parts of spread out information simultaneously on the screen" in MSExcel. This result suggests that transfer is improved when a strategy is taught in its general form. This result is, however, not definitive because we tested for transfer in only one instance. However, despite the fact that the split window command (necessary to execute the strategy in Word and Excel) is identical in both applications, the results indicate that application-general strategic knowledge may be important to enable transfer. Transfer has been difficult to achieve in many studies (see Singley and Anderson [1989] for an extensive discussion). It is possible that transfer did occur in our exploratory study because we (1) explicitly taught the conditions of when to use the strategy in the context of tasks in a specific application, (2) made users aware of the nature of tasks that warrants the strategy, and (3) taught the strategy multiple times with different examples. Because this was true for all the strategies that we taught, we believe our instructional framework appears to be well-suited for the transfer of strategies and is in agreement with earlier research on transfer of skills [Bossock and Holyoak 1989; Fong et al. 1986; Gick and Holyoak 1983]. However, future research should test this result with more extensive transfer conditions. Finally, the results of the retention test showed high retention of strategic knowledge across all the strategy opportunities. None of the differences between the posttest and retention test were significant in MSExcel or MSWord for either group. Thus, retention of strategic knowledge after one month was high in both groups. 3.2.3 Dissemination of the Strategy-Based Instructional Approach. The success of our strategy-based instructional approach in two universities led to requests for the use of the course material by the School of Nursing at the University of Michigan. The Nursing faculty requested that the course be used to teach their freshman students the strategic use of computer applications. They also requested that we teach PowerPoint in addition to MSWord, MSExcel, and Dreamweaver. The course was taught in two iterations at the Nursing School with minimal involvement of the original authors. This was achieved by providing written instructions to graduate students who were hired to teach the course. These instructions included the teaching guides (discussed in Section 2.4.1) and a set of guidelines8 of how to design and execute the teaching guides. The graduate student instructors modified and executed the course with minimal involvement of the original authors. Additionally, we received requests from two other universities9 that wished to explore how to provide strategy-based instruction to freshman students. 3.3 Summary of Extending Strategy-Based Instruction to New Applications and Populations In Phase 2 of our research on strategy-based instruction, we tested whether the strategy and instructional frameworks could be extended to a context requiring a new computer application and using a new population of Arts students. Furthermore, we tested whether strategic knowledge could be retained over time. The results showed that the strategy framework and the instructional framework could be successfully extended to new applications and to new populations and that strategic knowledge was retained even after one month. Furthermore, an exploratory study suggested that transfer of strategic skill to a new application improves significantly when the strategy is taught in its general form. Finally, the course material was disseminated to another context within the University of Michigan and to the University of Western Australia. 

4. LESSONS LEARNED IN TEACHING THE STRATEGIC USE OF COMPLEX COMPUTER APPLICATIONS 

Over the five-year span of our research program, we have learned lessons related to (1) the need to teach strategies explicitly, (2) the organization of strategies, (3) approaches to teach commands and strategies, (4) guidelines for creating and teaching strategy-based instruction, and (5) the effects of strategy-based instruction on learning. 4.1 Strategies Need To Be Taught Explicitly Several studies have shown that many users do not acquire effective and efficient strategies to use computer applications. In fact, as discussed in Section 1, there are many reasons that conspire against users discovering and using helpful strategies, despite many years of experience and despite well-designed interfaces. While we have experimented with online intelligent help systems [Bhavnani et al. 1996] and are open to other approaches to deliver instruction, we have come to believe that whatever the medium and style of instruction, most users need to be taught strategic knowledge explicitly before they acquire a wide range of effective and efficient strategies. 4.2 Strategies Exploit General Powers of Computers To teach strategies explicitly, we first need to know what they are. While several studies had identified the need to teach effective and efficient strategies, none of the studies had identified a systematic approach to organize them. Because we wished to identify strategies that generalized across authoring applications, we focused on the general functions or powers that these applications provided. This led us to organize strategies based on four powers of computer applications: iteration, propagation, organization, and visualization. In addition to helping us to organize the general strategies and instantiate them systematically across applications, the framework can be extended to include new strategies as new powers of computer applications are discovered and provided to users. 4.3 Strategies Should Be Taught in Combination with Commands A critical component of strategy-based instruction is that commands should be tightly integrated with the teaching of strategies. Through our early prototypes, we learned that an effective way to teach strategies was to first teach how to use a small set of commands, then immediately teach when to use those commands, followed by teaching the general form of the strategy. Other approaches, such as providing a general framework of all the strategies before teaching commands, did not motivate students in the classroom. We believe this is because learning strategies, in the absence of knowing how to implement them with commands, is too abstract. 4.4 General Strategies Can Enable Users to Acquire Strategic Knowledge While there has been research in teaching strategies to perform tasks in a wide range of domains such as math and reading, we found no systematic studies that explored how to teach general strategies to use computer applications. Our research has shown that, for the most part, merely learning commands does not easily lead users to acquire many strategies. Our experiments have shown that to learn how to use efficient and effective strategies to use computer applications in a short amount of time, users should be explicitly taught (1) the commands needed for a strategy, (2) the conditions under which a strategy is useful, and (3) the strategy itself in its application-specific form. If users are expected to transfer strategies across applications, there is some indication that the general form can significantly improve transfer even when the commands to use the strategies are virtually identical. 5. REFLECTIONS This article has focused on our five years of research related to strategy-based instruction. However, our research path began much earlier when we first conducted an ethnographic study to observe how architects were using CAD systems to perform real-world tasks [Bhavnani et al. 1996]. This study revealed that the architects were not using effective and efficient strategies despite knowing how to use the commands on the interface and despite many years of experience in using the CAD system. Cognitive analysis of these real-world tasks suggested the existence of strategies that could improve the performance of the users. Because the observed users had few problems with the interface, we decided to focus on strategy-based instruction to address the ineffective and inefficient use of computer applications. Our decision to pursue strategy-based instruction has often been criticized for attempting to change users to fit poorly designed systems. This argument takes the position that the need for instruction represents a failure in the design of the interface and therefore, that instead of attempting to change the user through instruction, we should attempt to change the interface so that it enables users to spontaneously discover and use effective strategies. We believe this argument ignores important characteristics of the problem. Consider the SPLIT WINDOW command available in MSWord and MSExcel. This command is explicitly designed to perform the simple goal of dividing a window into two panes and is useful in a wide range of higher-level editing tasks. However, while a user might learn from the interface that the split window command can divide the window into two panes, it is much more difficult to learn from the same interface when best to use that command. To know when to divide the window into two panes, a user must recognize specific characteristics in the higher-level task. For example, a user must learn to recognize that when two information objects that need to be compared are far apart in a document, then they need to be brought together on the screen before performing the comparison10 . Acquiring knowledge to recognize such task-related cues is difficult, and as demonstrated by our experiments, such recognition often does not happen spontaneously just from knowing how to use commands. As described in Section 2, knowledge to detect characteristics of a higher-level task and connect them to specific commands, has been a critical part of the strategic knowledge that we have abstracted and taught. One can argue that interfaces could automatically detect characteristics of a task from user actions and then suggest to the user when to use more efficient and effective methods. This approach can be successful when the detection is unambiguous (like inferring that a user is manually creating a numbered list and automatically converting the text into a numbered list). However, in most cases, automatic detection of task characteristics is not unambiguous. For instance, even intelligent interfaces would have difficulty in unambiguously inferring that a user was attempting to compare two distant pieces of information and correctly suggest the use of the split window command. Such ambiguity can lead to complex [Carroll and Aaronson 1988] and often annoying dialogs with the user to resolve the ambiguity. We have explored approaches to automatically detect opportunities for using more effective strategies [Bhavnani et al. 1996], and believe such research should continue and be informed by the results reported here. For example, such projects should target those strategies, like the ones identified in our experiments, which are difficult to acquire just by knowing commands. However, we believe that such attempts should, where possible, be complemented with strategy-based instruction that can assist users in acquiring a comprehensive understanding of the power of computers and how best to exploit them. Such knowledge, as we have explored, could have the added advantage of being transferable across applications. Looking back, our research has attempted to reduce the cost of learning by using strategies. This was achieved by teaching users how to use commands, how to recognize opportunities to use them strategically within an application, and then how such opportunities generalize across applications. While our overall approach has been fairly successful, it may be insufficiently motivating for students who already have a substantial knowledge of commands. Accordingly, we believe that it might be useful to develop a course that focuses only on strategic knowledge without also teaching commands. We are also exploring the development of a minimalist strategy manual that would provide brief online instruction of strategies for use with different computer applications. Furthermore, while we have focused on delivering strategy instruction in a classroom context, we would also like to explore how the same material could be delivered through computer-based tutors. Future research also needs to explore how well the strategies transfer across applications and are retained over longer periods of time. Furthermore, as discussed by others [Payne et al. 2001], we need to better understand the attributes of and conditions under which some strategies are automatically acquired just by learning commands. More research is needed to investigate how best to teach instructors how to design and execute strategy-based instruction effectively. Finally, the ineffective use of computer applications is not unique to authoring applications. Many users have difficulty in acquiring strategies to perform effective searches on the Web [Bhavnani 2001], and we believe that our instructional framework could be adapted to teach strategic knowledge to improve information-seeking behavior [Bhavnani 2005; Bhavnani et al. 2006]. Our hope is that such research will help achieve our ultimate goal of making users more effective and efficient in the use of a wide range of computer applications.

PRISM Interaction for Enhancing Control in Immersive Virtual Environments
SCOTT FREES Ramapo College of New Jersey G. DREW KESSLER Sarnoff Corporation and EDWIN KAY Lehigh University

When directly manipulating 3D objects in an immersive environment we cannot normally achieve the accuracy and control that we have in the real world. This reduced accuracy stems from hand instability. We present PRISM, which dynamically adjusts the C/D ratio between the hand and the controlled object to provide increased control when moving slowly and direct, unconstrained interaction when moving rapidly. We describe PRISM object translation and rotation and present user studies demonstrating their effectiveness. In addition, we describe a PRISMenhanced version of ray casting which is shown to increase the speed and accuracy of object selection. 

1. INTRODUCTION 

Designing models within immersive virtual environments is unique in that we directly manipulate objects in a manner similar to how we work in the physical world. Direct 3D interaction offers a highly intuitive and transparent user interface, in contrast to using a mouse and keyboard to indirectly control a 3D model through two-dimensional abstractions. A significant drawback of direct interaction however is the limited accuracy and fidelity of interactions. In the physical world, we use an object's weight and inertia, along with friction and supporting objects, to control our hand movements and counteract hand instability. In the virtual world, these aids are normally not at our disposal and hand instability dramatically reduces the accuracy with which we interact with the world. Hand instability affects many of the design choices made when developing an immersive application. Objects must be large enough to be easily selected, limiting the complexity of the world in which a user can work. Once the object is selected, the user must be able to precisely specify its final position and orientation. One might provide anchor [Mapes and Moshell 1995] and grid points [Beir 1990], which snap the object to predefined (and thus limited) positions and orientations in the world. Another technique is to explicitly zoom in or scale up the workspace in order to work in small areas and make fine adjustments to objects [Mine et al. 1997; Bederson et al. 2000]. This works well in some instances but in an immersive environment occlusion and the loss of context can be problematic. The virtual inertia technique [Ruddle et al. 2002] addresses hand instability directly by making objects resistant to movement, limiting the rate in which they can be moved. Virtual inertia does not support unconstrained motion when precision is not desired, however. Yet another approach is to use indirect object control, perhaps a 3D menu, slider, or button. Once again, hand instability limits the design of these widgets, requiring them to be larger than their 2D desktop counterparts. The increased size of widgets limits the number of widgets that can simultaneously be placed within a comfortable working space. We present a 3D interaction technique called PRISM: Precise and Rapid Interaction through Scaled Manipulation. The goal of PRISM is to increase the accuracy and control of the user in a virtual world to a level closer to what they are accustomed to in the physical world. PRISM increases the control/display (CD) ratio, which causes the cursor or object to move and rotate more slowly than the user's hand, reducing the effect of hand instability. One challenge of improving precision is deciding how to constrain interaction without sacrificing speed when precision is not desired. Scaling the movement and rotation of the hand slows the speed of interaction and is only suitable when the user is interested in accuracy. This suggests the need for two distinct modes: one that scales hand movement when accuracy and precision is needed and one that provides direct, unconstrained interaction when moving the object from one general location or orientation to another. One method to provide these two modes requires the user to explicitly indicate what mode is desired, perhaps through a menu, button, or other control metaphor. Recalling Fitts' Law [Fitts 1954], however, the user is continuously providing the interface a clue as to whether they have a precise goal in mind. Fitts' Law states that, as the target gets smaller, we must slow our hand movement. Conversely, if the target is large we can, and will, move more quickly. This implies that when the user is moving their hand slowly they are likely to have a precise position or orientation in mind; PRISM uses this premise to dynamically adjust the CD ratio according to the user's current hand speed. As the speed of the hand decreases, PRISM increases the CD ratio to provide scaled manipulation and filter out hand instability. As the speed of the hand increases, PRISM reduces the CD ratio back towards 1, providing direct, unconstrained interaction. By taking advantage of the principals of Fitts' Law, PRISM provides a very natural control metaphor to facilitate modal switching; the user's natural behavior indicates how much accuracy and precision to provide. This type of dynamic adjustment of the CD ratio has its roots in 2D mouse/cursor control [Foley et al. 1984]; the mouse is commonly configured to cover more pixel space when moved at high speeds and less when moved across the user's desk slowly. The next section provides an overview of other techniques that relate to precision interaction in both 2D and 3D. Section 3 describes the implementation of PRISM translation and rotation in detail. Section 4 discusses how PRISM can be applied to several common tasks in an immersive environment including object manipulation, ray casting, and widget control. Section 5 presents a series of user studies showing the effects of PRISM followed by future directions and conclusions. 

2. RELATED WORK 

Techniques adjust the control display ratio for various purposes such as overview vs. detailed modes while navigating documents [Igarshi and Hinckley 2000] and desktop-based 3D worlds [Tan et al. 2001] as well as increasing precision on desktop [Blanch et al. 2004] and touch screen displays [Albinsson and Zhai 2003]. Although similar to PRISM in concept, these systems (along with mouse cursor control) involve indirect manipulation, where reconciling offset/divergence between the physical device and the hand is a more straightforward task. For 3D direct manipulation, Dominjon et al. [2005] altered the control/display ratio (in the opposite direction from PRISM) in mixed reality environments to alter the user's perception of object weight. For their purposes, users could not see the divergence between the hand and object. The Voodoo Dolls technique [Pierce et al. 1999] enables users to scale their workspace (and thus scale their movements) by selecting a voodoo doll of an appropriate size. PRISM also scales down the user's movement; however, it does so without requiring an appropriately sized reference object, which might not always be available. Other techniques use relationships between objects to constrain interaction [Bukowski and S´ quin 1995; Stuerzlinger and Smith e 2002]. These techniques still require some form of direct interaction on an object, which is where PRISM can be useful, possibly working in conjunction with these techniques. Although PRISM is aimed at virtual environments without force feedback, many applications benefit from the use of physical props to aid and constrain interaction. The use of props in neurosurgery applications [Hinckley et al. 1994b], menu interaction, and object manipulation [Lindeman et al. 1999; Ishii and Ullmer 1997] have been shown to be effective. Although there is little doubt that manipulating physical props helps in cognition and performance, props are specific to the application and are not general solutions to the precision problem. Much of the earlier work in 3D rotation tasks focused on 2D mouse-based techniques for the desktop, including the Virtual Sphere [Chen et al. 1988] and the Arcball [Shoemake 1992]. Unlike object translation, task completion time is generally quite high in 3D rotation tasks, as reported by Ware [1990] and Zhai et al. [1996]. The difficulties of these indirect rotation techniques can be mitigated by direct manipulation, where the controlled object and the physical hand are co-located [Hinckley et al. 1997; Ware and Rose, 1999]. Poupyrev et al. [2000] have amplified 3D rotation (control/display ratio less than 1) and concluded that amplifying rotation decreases task completion time without significantly effecting accuracy. Another area of research concerning object manipulation focuses on selecting and interacting with objects at a distance. Some of the more common techniques include ray-casting, Scaled-world grab [Mine et al. 1997], Image Plane interaction [Pierce et al. 1997], Go-Go [Poupyrev et al. 1996], HOMER [Bowman and Hodges 1997], and World in Miniature (WIM) [Stoakley et al. 1995]. As discussed below, PRISM can be used in conjunction with some of the above techniques to increase precision. 

3. PRISM INTERACTION 

Direct manipulation of virtual objects normally consists of "grabbing" an object, moving it to a new location and/or orientation, and "releasing" it. In our implementation, an object is grabbed when the user places the tip of a hand-held stylus within a virtual object and holds down the stylus button. While the user holds the button, the position and orientation of the virtual object directly follows the movement of the stylus. The virtual object is released when the stylus button is released. A specific design challenge for a direct manipulation interface in 3D is to provide the user with the ability to perform deliberate, precise, fine-grained adjustments to the position and orientation of objects without removing the ability to move objects quickly from one general position to another [Hinckley et al. 1994a]. This design goal implies two distinct modes of interaction; one where constraints are supplied to aid in precision manipulation and another where the user directly controls the movement of the object, free of any artificial constraints. In response to this, PRISM uses the hand speed of the user to gradually switch between modes by altering the control/display ratio. As shown in Figure 1, PRISM uses speed thresholds--defined by thee constants--to determine the CD ratio that controls the held object. The first constant is a minimum speed (MinS), below which a user is unlikely to be moving purposefully. Any motion below this speed is most likely tracking error or inadvertent drift and the controlled object is not moved. The second, and most significant constant, is the Scaling Constant (SC). If the user is moving their hand slower than this relatively low speed, they are likely to have a precise goal in mind. While in scaled manipulation, PRISM sets the CD ratio inversely proportional to the hand speed. For example, if the user is moving or rotating their hand at a slow speed, close to the MinS, a very high CD ratio is used and the object would move very little. If the hand speed is closer to SC, the CD ratio approaches 1 and the controlled object might move or rotate 90% of the distance the hand has moved during the most recent sampling interval. Any time the user is moving their hand at a speed above SC they are transitioned into imprecise, direct manipulation mode and the object mimics each movement and rotation of the hand. Implicit in this method is the accumulation of an offset value representing either the positional or angular displacement between the hand and the object being manipulated. Each time movement is scaled, the virtual object moves or rotates a fraction of the distance the hand does. Although there are implementation differences between the translation and rotation techniques, in both cases a third constant, MaxS, triggers automatic offset recovery. Once the speed of the hand reaches this threshold, the offset is automatically reduced, causing the object to catch up to the hand. As shown in Figure 1, MaxS is typically larger than SC in order to provide the user with a "buffer" between speeds that lead to direct interaction and speeds that will trigger automatic offset recovery. There exists an important relationship between the scaling constant (SC) and the sensitivity and precision (via scaling) the user experiences. The lower the SC value the less scaling will take place, making the controlled object more sensitive to hand movement. In contrast, a high SC value provides more scaling and the object will be resistant to hand motion, giving the user the ability to be more precise. Some users naturally move their hands more slowly and steadily than others and thus feel more comfortable using a lower scaling constant. On the other hand, some users have a lower degree of dexterity and have particular difficulty keeping their hands steady. Users in this category require the object to be less sensitive to their movements, thus they are likely to benefit from a higher scaling constant. The ability of the interface designer to easily change the behavior and "feel" of PRISM by adjusting SC not only allows it to be helpful for different people with different skills, it also allows it to be applied in many different situations or interaction tasks where users might require more or less sensitivity or accuracy. 3.1 PRISM Translation Implementation Details PRISM uses hand speed to govern the CD ratio for object translation. The hand speed is determined by taking a sample of the hand position before each frame and determining the speed using the current location and the location of the hand 500 ms in the past. Note that this is a somewhat large interval and that small movements back and forth within the last 500 ms will not necessarily count towards the speed value. By using a "smoothed out" speed, a small movement in the positive direction quickly followed by a movement in the negative direction will result in a total hand speed below MinS and the CD ratio will be set to , nullifying hand movement. This is crucial since small frequent movements back and forth are often indicative of tracking jitter or hand instability that ideally should not cause the object to move. Intentional hand movement in a particular direction will almost always last more than 500 ms--resulting in a non-zero speed. This interval was chosen through observation and proved to be a reasonable value for most users. Slightly different intervals may be more desirable for specific individuals. This is a simplistic form of input noise reduction compared to other techniques designed to combat tracking jitter [Lian et al. 1991; Welch and Bishop, 1997], however, this technique has proved suitable for our purposes. No subjects in our experiments have stated that the latency introduced by the use of this interval was perceptible or undesirable in any way. To move the object itself, PRISM determines the distance and direction moved between the last two frames and moves the object some proportion of that distance according to the hand speed. By using the "smoothed out" speed and applying it to the instantaneous movement of the hand, PRISM accurately reflects purposeful and sustained movement when determining the CD ratio while still being responsive to hand movement. As implied in (1), once the hand speed reaches the Scaling Constant (SC), the CD ratio is set to one, providing direct translation. Dobject is the distance the controlled object will move Dhand is the distance the hand itself moved since the last frame Shand is the speed of the hand over the last 500 milliseconds SC is Scaling Constant (meters per second). In our trials the framerate was quite constant, making a frame based offset recovery feasible. A more general approach would be to reduce offset strictly based on time.

3.1.1 Offset Recovery. Whenever the user is in scaled mode an offset accumulates between the hand and the controlled object. This offset is graphically represented by a white line between the virtual hand and the object and is shown in Figure 2. The PRISM translation technique uses two mechanisms to recover this offset without interrupting the user's interaction. The first mechanism works when the user is in any mode, has accumulated an offset in a particular direction, and then changes direction moving their hand back towards the object. Under this circumstance, the object is not moved until the hand crosses back through the object, reducing the offset to zero. This mechanism was effectively used as a clutching/ratcheting technique by users and also guarded against unintended hand movements. The second automatic offset recovery technique activates once the user's hand speed exceeds the MaxS threshold. At this speed, the user is likely trying to move the object to a general location and is not concerned with accuracy. Starting at the moment the speed exceeds MaxS, PRISM speeds up the movement of the object so it catches up to the hand. Once the object and hand are at the same position, the CD ratio returns to one. If at any time during the recovery period (around one second) the hand slows below MaxS, the recovery is stopped and the object's position will follow the hand according to (1). Table I summarizes this offset recovery technique. 3.1.2 Axis Independent Scaling. PRISM translation operates on each axis (x, y, and z in world coordinates) independently. For instance, the hand speed in the X direction only affects the scaling mode (and movement of the controlled object) in the X direction. This allows the user to move their hand rapidly in the X direction and retain direct control while simultaneously being more precise (in scaled mode) in the Y and Z direction, eliminating inadvertent drift. PRISM could have been implemented using the Euclidean speed of the hand to calculate the mode and degree of scaling as well, however this method would not eliminate drift to the same extent. The biggest advantage of independent scaling is that it helps users move the object in a straight line along a principal axis. The disadvantage is that PRISM scales diagonal movement more than movement along a principal axis. Scaling against the principal world axis was a fairly arbitrary design choice; other options could be the principal axis of the user, view, or perhaps another object's coordinate system. Further investigation is required to determine which scaling method is most suitable for different situations. 3.1.3 Interaction Examples. Figure 3 presents several typical interaction examples. For simplicity, the examples are illustrated in 2D. In Figure 3(a), the user moves a virtual sphere to the right at a speed just under SC. The hand motion in the horizontal direction is scaled by a small amount. At the same time, the user also moved their hand down in vertical direction at a very slow speed, indicative of an inadvertent drift in their hand position. Since the scaling values are calculated independently in each axis, this vertical movement is completely filtered out, without affecting the movement in the horizontal direction. In Figure 3(b), the user moves their hand in the same manner as in Figure 3(a), however, this time the hand is moved quickly in both the horizontal and vertical direction. This quick motion (faster than SC) results in direct manipulation in both directions and the sphere maintains its relationship with the hand. In Figure 3(c), the user begins with an offset in the vertical direction (from some previous interaction) and then moves slowly up towards the object and to the right. In this situation, the vertical offset is completely recovered and none of the upward hand movement translates into vertical movement of the sphere. Since the hand also moved slowly to the right, the user is left with an offset in the horizontal direction. In Figure 3(d), the user again begins with an offset in both the horizontal and vertical directions. Since the hand motion is faster than MaxS, the offset is recovered in both directions as the controlled object "catches up" to the user's hand. 3.2 PRISM Rotation Implementation Details The PRISM rotation technique uses the angular speed of the hand and three constants to determine the CD ratio of the interface. When the hand is rotating slowly, below a MinS value, the CD ratio will be  and the object will not rotate. As the rotational speed of the hand increases, the CD decreases and the object is rotated by an angle proportional to the distance the hand has rotated in the most recent sampling interval. As the angular speed approaches an SC value, the object rotates directly with the hand. The third constant, MaxS, triggers automatic offset recovery and is discussed in Section 3.2.1. Working with 3D rotation is more complex than the Euclidean geometry involved in translation. With translation, PRISM separates the movement and speed of the hand into three values corresponding to the x, y, and z directions. The distance covered in each of these directions can be easily scaled up or down while preserving the general direction of movement in three dimensions. Although a similar implementation may be possible using Euler angles (pitch, yaw, and roll), there are multiple combinations of these angles that describe a single change in orientation, and it is difficult to choose one that is consistent with the user's model of the change. Along with Gimbal lock, these problems make describing 3D rotation using Euler angles a poor approach. Instead, PRISM uses a quaternion representation for the orientation of the hand and the controlled object. The mathematics of quaternions is more difficult to understand, however concatenation, interpolation and scaling of 3D rotations is more straightforward. A simplified view of a quaternion is a four-dimensional vector consisting of a 3D vector v{x, y, z} and a real number w. The real component w relates to the angle rotated around the axis defined by the vector v (w is cos(/2) where  is the angle rotated around v). Before each frame a quaternion representation of the hand orientation is recorded. PRISM rotation calculates the rotational speed of the hand before each frame by comparing the current orientation and the orientation of the hand 200 ms in the past, in the same manner as in PRISM translation. This sampling interval was determined through observation and is less than its counterpart in the translation technique because a continuous rotation normally has a shorter duration than a translation. As with translation, users did not report negative consequences stemming from the latency introduced by using this interval. Equation (3) calculates the rotational difference between the current (Qt ) and last orientation (Qt-1 ) of the hand, in the form of Qdiff . Note, to find the quaternion needed to rotate from q1 to q2 , q2 is divided by q1 . Equation (4) converts the angle represented by Qdiff from radians to degrees and Eq. (5) simply divides the angle by 200 ms (the time between Qt and Qt-1 ) to obtain the rotational speed of the hand. Equation (6) is used to determine the control display ratio to be used. The inverse of the control display ratio, k, is used to scale rotation. In Eq. (7), the quaternion representation of the angle the hand has rotated (Qdiff ) is scaled by raising it to the power k, where k is a real number between 0 and 1. The reader is referred to Poupyrev et al. [2000] and Shoemake [1985] for details on how the quaternion power function amplifies and scales rotations and how it is calculated). The scaled rotation is then added to the current orientation of the object (Qobject ), which gives the new orientation of the object Qdiff is the quaternion representing the angle the hand has rotated in the last 200 ms.* Qt is the quaternion representing the current hand orientation Qt-1 is the quaternion representing the hand orientation 200 ms before the current time Qnew is the quaternion representing the new orientation of the object Qobject is the quaternion representing the current orientation of the controlled object A is the angle (in degrees) the hand has rotated in the last 200 ms.* RS is rotational speed of the hand in degrees/second. SC is Scaling Constant (degrees per second).

3.2.1 Offset Recovery. When scaling the rotations of the hand during slower movement, an angular offset will be accumulated between the hand and the controlled object. PRISM helps the user visualize this offset by drawing two sets of 3D axes, as shown in Figure 4. A lower axis follows the rotation of the hand while a upper axis follows the orientation of the controlled object. When the axes are aligned, the hand and the object are at the same base orientation. As they diverge, the user can see that scaling is taking place through the separation of these axes. As an additional aid for the user, two arcs are drawn on the top right of the viewing plane. The lower arc is always the same length (half circle) but the upper arc is scaled in real time to show the user the current CD ratio. When the upper arc is very small, the object will resist movement (high CD) and, when it is a half circle, it will follow the hand more directly. As with translation, PRISM rotation automatically reduces the accumulated angular offset once the user rotates their hand beyond the MaxS speed threshold. While PRISM translation reduces this offset gradually over one second, PRISM rotation eliminates the offset immediately; which causes the object to align with the hand. We use immediate offset reduction because the amount of rotation that can be performed quickly is quite limited. Unlike translation, rotation is bounded in that one can only rotate an object 360 degrees before returning to the same position. Further limiting rotation is the physical limitations of the wrist--normally a user can rotate their wrist no more than about 180 degrees in one motion without putting an undesirable amount of strain on their elbow. Thus, using a gradual offset reduction (accelerating the object) would force the user to rotate quickly for a longer period of time than necessary, and would be undesirable. For translation, PRISM also provides offset recovery whenever an offset is accumulated in a particular direction and the hand is then moved back towards the object. Translation can be easily broken down into three orthogonal directions, simplifying the implementation of this form of recovery. For rotation, the quaternion representation of the hand and object rotation cannot be broken down as easily; it is difficult to determine how much motion is actually aimed at reducing the offset. Although a similar automatic clutching, or ratcheting in this case, may be possible for 3D rotation, we have not implemented this form of offset recovery. 3.2.2 Interaction Examples. As shown in Figure 5(a), whenever a user gains control of an object the CD ratio is initialized to one, which is direct manipulation. The upper arc is at its full size, indicating to the user that they are in direct control of the object and the object and hand axes are aligned indicating that there is no angular offset present. In Figure 5(b), the user has slowly rotated their hand clockwise at a speed of around SC/2 (SC was 30 degrees/second in our implementation). Here the upper arc is at half size indicating that the object will only rotate about half the distance the hand rotates. Since the object rotation has been scaled, there is an angular offset between the hand and the object, indicated by the gap between the upper and lower axes. Figure 5(c) shows that the user has now quickly rotated their hand counter clockwise. All offset is reduced since the speed of rotation was above SC. The upper arc is once again at full size, indicating that if the user continues to rotate at this speed the object will directly follow. 

4. PRISM APPLICATIONS 

The primary goal of PRISM is to enhance the precision of translational and rotational input coming directly from the user's hand. Although PRISM was designed to aid in direct manipulation of virtual objects, there are many other situations where the position and orientation of the hand is used to interact with the virtual world or the interface. In this section, we describe several situations aside from direct object manipulation where PRISM interaction can be used. 4.1 Selection By increasing the precision in which the user positions and orients the cursor, the density of the selectable objects within a confined place can be increased without introducing selection errors. One of the most fundamental ways of selecting objects is to simply move the cursor (controlled by the user's hand) such that it intersects the object. Using the PRISM translation technique, the position of the cursor can be specified more accurately, allowing for easier selection of small, densely packed objects. This increase in density and precision could improve interaction in scientific visualizations and surgical training applications. In addition, PRISM can allow designers to increase the density or decrease the size of input widgets such as the Command & Control Cube [Grosjean and Coquillart 2001] without decreasing usability. The Command & Control Cube is a 3D cube divided into 27 equally sized cubes (3 × 3 × 3) in which each cube represents a specific command. In order to invoke a command the user must select one of the cubes by placing the cursor inside it. Using PRISM, each individual cube could be smaller allowing for either more commands (4 × 4 × 4, 5 × 5 × 5 etc.) within the same dimensions or the entire interface could be shrunk uniformly, saving screen space. Other menu interfaces could benefit from the same increase in density as well. There are many situations where users desire to select objects beyond arms reach. A common technique for this task is ray casting [Mine 1995], were the user points his or her hand at the object of interest and a ray is drawn which extends outwards from the user's hand/cursor. One problem with this approach is that small rotations of the wrist sweep out relatively large arcs at the end of the selection ray, which is shown in Figure 6. The increased sensitivity to slight hand rotations makes it difficult to select distant objects that do not occupy a large area in the viewing plane, a problem noted by Poupyrev et al. [1997]. One solution to this is to "snap" the ray to the nearest intersecting object. This method breaks down when working in a cluttered environment where slight movements of the wrist cause the ray to "snap" between several proximate objects. Our solution to improve the accuracy of ray casting employs PRISM rotation to control the angles swept out by the ray. When the user slows the rotation of their wrist the orientation changes are scaled down which offsets the amplification of these rotations at the other end of the ray. Section 5 presents a user study that provides more detail on the implementation of PRISM enhanced ray casting, along quantitative performance results. 4.2 Widget Control Most 3D widgets incorporate some form of direct 3D translational or rotational hand input. The World in Miniature (WIM) widget [Stoakley et al. 1995] provides users with a hand held, miniaturized model of the virtual world, complete with all (or most) of the objects within it. The user directly manipulates the miniaturized version of the objects with the cursor and their actions are mimicked in the full-sized world. One problem with the WIM is that in order to interact with the miniaturized objects one must scale down the movements of the cursor as well. Moving an object within the WIM one centimeter might result in a translation of one meter in the full-sized world. If the user wishes to move an object by a centimeter in the full-sized world it would be nearly impossible to do so using the WIM, the user would need to navigate to the location in the fullsized world and manipulate the object directly. A WIM containing a cluttered world with many objects poses another problem in that many small, miniaturized objects might occupy a very small space, making selection increasingly difficult. With PRISM, users would be able to move the cursor quite accurately at very small scales, allowing them to select small closely packed objects and translate them at scales, which would be difficult with direct manipulation due to hand instability.

5. EVALUATION 

We have conducted four separate user studies examining PRISM. Before each experiment users were provided with a training session requiring them to complete practice trials using both direct manipulation and PRISM. Participants were not permitted to participate in more than one of the four experiments. The equipment used included a four-port Polhemus 3SPACE FASTRAK electromagnetic tracking system with a hand-held stylus (Polhemus ST8, approx. 18 cm long, 1.3 cm diameter, 1 button) and a Virtual Research Systems V8 Head Mounted Display. Most users gripped the stylus as they would a pencil or pen (using the index finger to press the stylus button), however several used their thumb to depress the button. We used the Simple Virtual Environment toolkit [Kessler et al. 2000] to implement the environment. The system rendered the test environments in stereo and the frame rate was held between 20 and 30 frames/sec throughout the experiments. Tracking jitter and latency were within expectable limits; users did not report any difficulties. In each experiment, participants were standing and free to walk within tracking range (approximately 1m radius), although completion of the experiments did not require a large degree of mobility. 5.1 PRISM Translation The object translation task required the user to pick up a virtual sphere and place it completely inside a translucent virtual cube such that no part of the sphere protruded any side of the cube. The cube changed color (and turned opaque) each time the user positioned the sphere completely inside the target (and released the stylus button to "drop" the sphere) to indicate a completion. A short time later the sphere re-appeared in its original starting position outside the cube. The participants were asked to repeatedly place the sphere inside the cube as many times as possible during the 3-minute trial. In each trial the target cube became smaller after each completion, beginning with an "Easy" difficulty level and progressing to the fourth target in which all subsequent cubes were "Very Difficult". This variation in difficulty level was included to increase the chance that all participants could complete at least one target in a trial without becoming overly frustrated. The difficulty levels the user experienced are summarized in Table II. Since all trials and users were given the same sequence of difficulty levels, this variation does not affect the overall performance measure (number of completions per trial). The starting position of the target cube and mobile sphere were the same for each participant and trial. The target cube was placed at shoulder height (25 cm below the top of the HMD) near the center of the virtual world. Each sphere was placed one meter away from the target in order to eliminate the possibility of the distance from the target affecting the difficulty level [Accot and Zhai, 1997; Fitts 1954]. The spheres were also placed at shoulder height. The distance between the target and initial position of the sphere ensured that movements consisted initially of arm/shoulder movements to bring the sphere close to the target and then fine-grained hand motion to precisely align the objects. The target and sphere positions relative to the user varied, since the user was free to move around as they wished. Most users stood at a position where the target was directly in front of them and the spheres appeared on the same side as their dominant hand. The training session prior to the experiment allowed them to become familiar with the layout of the world. We conducted a within-subject experiment consisting of 18 (14 male, 4 female) undergraduate and graduate students. The experiment consisted of six trials and included two factors; the first was the interaction technique, at three levels: Direct, PRISM with Generic Scaling, and PRISM with Custom Scaling. The Custom Scaling trials used a Scaling Constant (SC) chosen by the participant during their training session. The difference between Generic and Custom PRISM was found to be insignificant and will not be discussed in detail. For more detail on the Custom Scaling aspect of this experiment see Frees and Kessler [2005]. A scaling constant of 0.15 m/sec was used in the Generic trials; MinS and MaxS were 0.01 m/sec 0.25 m/sec respectively for all trials. Target orientation was the second factor; target cubes were either axis-aligned or rotated 45 degrees in each principal axis (shown in Figure 7). Although target orientation has no theoretical effect on the difficulty of the task (the sphere still needs to be placed at the centerpoint of the target), users have a tendency to move the sphere diagonally towards the rotated targets instead of along a principal axis. We compared the use of rotated and axis aligned targets to draw conclusions about the axis-independent scaling strategy. Trial orders were balanced independently for interaction type and target orientation.

5.1.1 Results. The number of completions (number of spheres placed inside the cube) in each trial was used as the dependent variable in the experiment. The fixed-effect independent factors were Interaction Type (Direct, Generic PRISM, and Custom PRISM) and Target Orientation (Axis-Aligned and Rotated). Participants were treated as a random factor. Figure 8 shows the mean performance under each of the conditions. Table III reports the results of the ANOVA on these data. There was a significant effect due to Interaction Type (F(2,34) = 33.17, p < 0.001), the effect of target orientation was not significant (F(1,17) = 3.57, p > 0.05), and there was a significant Interaction-Type × Target-Orientation interaction (F(2,34) = 3.50, p < 0.05). A visual inspection of Figure 8 indicates that there were more completions for the Generic PRISM and Custom PRISM conditions, that this effect was greater for the Axis-Aligned conditions, and that there was no difference between the two PRISM conditions. This last observation was confirmed by an insignificant post-hoc Tukey test of pair-wise difference between the two PRISM conditions.

5.1.2 User Feedback. We administered an exit survey in which users responded to our questions by giving a score ranging from 1 (Strongly Disagree) to 5 (Strongly Agree). When asked whether they were able to adequately learn PRISM during the training period, 14 participants agreed, 2 were neutral, and 2 felt they did not. Nearly all participants agreed that either Generic or Custom PRISM was better than direct manipulation (13 preferred Generic over direct, 16 preferred Custom over direct). There was no consensus as to which set of targets (axis aligned or rotated) were more challenging (7 preferred axis aligned while 8 preferred rotated targets). 5.1.3 Discussion. PRISM allowed users to place the sphere more accurately inside targets then when using direct manipulation. As noted above, difficulty level increased within each trial until the participant reached the most difficult level; afterwards each target remained at that level until the completion of the 3-minute trial. This meant that the number of completed targets at the higher difficulty levels varied based on how quickly the participant completed the easier ones. This aspect of the design prevents us from making any statistical claims as to how PRISM or direct manipulation was affected by the difficulty level; however, we observed a clear trend. On the first two targets in each trial (the largest cubes), participants did not have trouble using PRISM or direct manipulation. When using direct manipulation, performance dropped off dramatically around the third or fourth cube. When using PRISM, performance did not noticeably drop off until the targets reached their smallest size and even then performance remained acceptable. The common strategy when using direct manipulation on the smaller targets was to rapidly and repeatedly pick up and drop the sphere--with the hope that it would eventually be dropped in the correct position. This strategy suggests that users did not have confidence in their ability to purposefully move the sphere to its proper position. This lack of confidence or control was not observed during the PRISM trials. Although performance was better with PRISM for both types of targets, improvement over direct was reduced when using rotated targets. We were unable to determine the true cause of this and none of the participants expressed concerns about how PRISM was responding to their movements; most participants cited difficulties with getting a good view of the target when asked why they favored one set of targets over another. As discussed in the future work section below, the most obvious reason performance suffered is PRISM's axis-independent scaling, which slows diagonal motion more than movement along a principal axis. 5.2 PRISM Rotation The rotation task required the user to rotate a virtual object (seen in Figure 9) such that it was completely inside a translucent target object that was the same shape but slightly larger. The centers of the object were always placed at the same location in the world (at shoulder height) and translation of the objects was disabled, leading to a purely rotational task. Participants were free to walk about the world as they wished. Once the user rotated the object (and released the stylus button) such that it was completely inside the target, the target changed color (and became opaque) to indicate a completion. A few seconds later the object re-appeared with an orientation offset from the target (the target orientation was held constant). The starting orientation of the mobile object was randomly selected at runtime from a set of four orientations to prevent the participant from being able to perform the same exact rotation repeatedly (which could lead to significant learning effects). Each of the four orientations required between 50 and 80 degrees of rotation about an arbitrary axis in order to align with the target. Both objects were made up of nine individual cubes of equal size, numbered 1 to 9 in Figure 10. In order to record an alignment, the user needed to rotate the mobile object such that the displacement between the centers of each set of corresponding cubes (cube 1 in both the mobile and target object) were within a maximum allowed tolerance. Note, because of the 3D nature of the objects, it is impossible to align the objects without aligning each corresponding cube. Alignments could have been measured using the pure orientation of the objects; however, determining alignments based on the displacement between each set of corresponding cubes is more straightforward for situations when translation and rotation are involved, as is the case in Section 5.3. This experiment included two factors: the first factor was the interaction technique (Direct and PRISM), the second factor was target size (or error tolerance), which was at three levels described in Table IV. A Scaling Constant of 30 degrees/sec was used, which was determined from pilot studies. For each PRISM trial, MinS was set to 1 degree/sec and MaxS was at 35 degrees/sec. There were six trials, each lasting 2 minutes. Users were asked to get as many completions as possible within the trial. Each user performed the task with each interaction type/error tolerance combination. We conducted this experiment with 15 (14 male, 1 female) undergraduate and graduate students. For each subject, the trial ordering was randomly determined. Note that the error tolerances used in this experiment are less than what was used in the translation task (more accuracy is needed). Based on pilot studies we felt users would be able to achieve a higher level of precision in a "rotation only" task than they were able to achieve with translation. In addition, rotation error propagates when larger objects are being used--which is not the case with translation. For instance, when rotating an object 2 m long, an error of just 1 degree will result in a 1.75 cm displacement of the object at its end points. The size of the target was not adjusted to represent the error tolerance. During pilot studies, several users complained that occasionally the system did not register an alignment even when the mobile object appeared to be completely within the target. This was extremely confusing to the user since it gave no indication as to the nature of the misalignment. This was most likely a visual artifact caused by the graphics system. The target object remained slightly larger than the mobile object only so the geometries of the objects could never completely overlap (which caused an awkward blending of the objects). Participants were told to align the two objects as closely as possible and that depending on the error tolerance, the alignment would be accepted when they released the object. Each error tolerance allowed a larger displacement than the size difference between the target and mobile object. This ensured any alignment that appeared to be completely within the target would always be accepted--thus ensuring that the user always had a visual indication of the misalignment. A wire frame visualization of the error tolerance could have been used. It is questionable whether users would have been able to effectively use that type of feedback however, as it would be challenging to identify where the solid object was protruding out of a wire frame object by a small amount. 5.2.1 Results. In this analysis, Interaction Type and Difficulty were the fixed independent variables. We also included participant number as a random independent variable. The dependent variable was the number of completions per trial. The ANOVA table for this experiment is shown in Table V, and the mean number of completions per trial are displayed in Figure 11. There were more completions per trial in the PRISM condition than in the Direct condition (F(1,14) = 5.28, p < 0.05). As the difficulty of the target increased, the number of completions per trial decreased (F(2,28) = 20.67, p < 0.0001). The InteractionType × Difficulty interaction was not significant (F(2,28) = 2.27, p > 0.05). 5.2.2 User Feedback. Feedback from a post-experiment survey was similar to the results we received for PRISM translation. Eleven out of 15 participants preferred PRISM over direct rotation, three preferred direct rotation and one participant had no preference. As for PRISM being easy to learn, PRISM received a score of 3.8 on a 5-point scale where 5 was Very Easy and 1 was Very Difficult. 5.2.3 Discussion. The results show a clear increase in performance when using PRISM; however, throughout the experiment, we noticed that many participants had difficulty determining which way to rotate the object in order to fit it inside the target. Often the user would see the object protruding out of a section of the target but could not determine which axis to rotate the object around in order to get the desired effect. These types of problems with understanding 3D rotation along with poor performance in 3D rotation tasks have been recognized in the literature [Parsons 1995]. When using direct rotation, this was not as problematic since accidental rotations of the hand resulted in object rotation. This accidental rotation occasionally caused the object to go inside the target on the easier difficulty levels, or at least cued the user as to which direction they needed to rotate the object on the more difficult targets. This confusion was a more serious problem when using PRISM. Since PRISM filters out much of the accidental and unintentional movement, the user needs to thoroughly understand which direction and around which axis they need to rotate the object. From observation, it was clear that, once the user knew which rotation would result in success, they were able to execute that rotation far more effectively with PRISM. In short, PRISM enhanced their ability to rotate objects purposefully, but it was a detriment if the user did not know which rotation was needed in the first place. This seems to be a general problem with 3D rotation rather than a problem with PRISM; one solution might be to administer spatial ability tests and lower the Scaling Constant (which would allow for more unintentional movement) for those who score poorly. We also suspect that providing a wire-frame representation showing the user how the object would rotate using direct manipulation would alleviate some of this confusion when using PRISM. This technique has been successfully employed by Ruddle et al. [2002] in cluttered environments. 5.3 Six DOF with PRISM This experiment was quite similar to the rotation-only experiment; however, the object being manipulated was not constrained to a fixed position. The mobile object started approximately 0.5 meters away from the target and needed to be translated and rotated to fit inside the target. The layout of the world mimicked that of the translation experiment--the target object and the starting position of the mobile object were always at the same location in the world. Just as in the translation experiment, the distance between the target and initial position of the mobile object ensured that the task required both arm/shoulder movement and fine-grained hand motion. This task required more effort and skill than the first two tasks, the user needed to accurately control the position and orientation of the object. This is more difficult because hand rotation often changes the hand position and vice-versa [Bowman et al. 2001]. Due to the added difficulty, we increased the time for each trial to 3 minutes and only examined two difficulty levels, which are shown in Table VI. The experiment consisted of four trials; each participant performed the task with direct manipulation and PRISM with both tolerance levels. In each trial, once the user rotated and translated the object (and released the stylus button) such that it was completely inside the target, the target changed color and became opaque. A few seconds later the object reappeared at the original starting position and with a random orientation chosen from the same set used in the rotation-only experiment. Sixteen undergraduate and graduate students (12 males, 4 females) participated in this experiment. A scaling constant of 0.15 m/sec was used for translation and 30 degrees/sec was used for rotation. For each subject, the trial ordering was randomly determined. We recorded the number of completions made in each trial. 5.3.1 Results. The results of this experiment are displayed in Figure 12 and Table VII below. As expected, there were fewer completions for the hard target than for the easy target (F(1,15) = 54.00, p < 0.0001). The Interaction-Type × Difficulty interaction was significant (F(1,15) = 6.24, p < 0.05), there were fewer completions for the PRISM condition with easy targets and more with the hard targets. We observed that when using direct manipulation with the smaller error tolerances the participants often resorted to "guessing", where they quickly picked up and dropped the object repeatedly in hope that it would fall into place. To look at this more closely, we also used the number of button clicks per completion as a dependent variable. Participants did not seem to have much trouble on the easier targets however for the more difficult targets participants clearly had more trouble using direct, as evidenced by a nearly 100% increase in the number of times the user picked up and dropped the object. This is further evidenced by a significant Interaction-Type × Difficulty interaction (F(1,15) = 14.09, p < 0.01). 5.3.2 User Feedback. Overall, participants found the trials with the higher error tolerance to be quite easy and some told us that PRISM slowed them down on these trials. On the other hand, most participants felt PRISM was a large improvement over direct manipulation for the more difficult trials. Overall, 75% of the participants thought PRISM was more preferable and more effective than direct manipulation. Most users felt PRISM was relatively easy to learn, with an average score of 3.44 out of 5 (5 being "Very Easy to Learn"). 5.3.3 Discussion. We saw a number of outside factors contributing to performance when using PRISM. First, it was quite obvious that the participants were having difficulty determining which direction they needed to turn the object to move it inside the target, just as we saw in the rotation-only experiment. For some participants there was even more confusion due to the translation component of the task. In situations where a small rotation would have moved the object inside the target, many participants instead tried to translate it repeatedly. Once again, this type of confusion favored direct manipulation; accidental movement of the object at least gave the user the chance of moving it in the correct direction; when using PRISM this accidental movement was filtered out. Despite this issue, PRISM outperformed direct manipulation for the more difficult (smaller) targets, and we believe that the gap between PRISM and direct manipulation's effectiveness is larger than the results suggest. The increase in button clicks (guessing) indicates to us that the user did not feel confident in their control over the object when using direct manipulation. This is precisely the type of behavior PRISM is trying to remedy. 5.4 Ray Casting with PRISM Rotation We implemented ray casting by drawing a white line extending out from a small red cone attached to the user's stylus. To aid in targeting, a red sphere was placed at the far end of the ray (far beyond the targets). When the ray intersected the target, the far end of the ray was attached to the object and the sphere in the distance turned green to indicate that the target could be selected with a button press. When using direct manipulation, the red cone attached to the stylus always pointed in the same direction as the stylus; when using PRISM, the rotation of the red cone was scaled proportionally to the rotational speed of the stylus, which can be seen in Figure 13. In pilot studies, it was determined that the scaling constant did not need to be as high as it was for direct object rotation--the PRISM implementation of ray casting used a scaling constant of 20 degrees/second. This experiment required users to select small, far away objects as quickly and as accurately as possible. Each trial presented the user with a series of 15 targets that "exploded" when selected. Users were given a starting value of 3000 points; their points decreased at a rate of 10 points per second. In addition, each time the user pressed the stylus button but missed the target they lost 200 points. The point system was explained to the user prior to the experiment and they were instructed that in order to maximize their points for each trial (a trial was complete once all 15 targets were selected) they needed to value accuracy over speed. Each participant completed four trials during the experiment, two trials using PRISM ray casting and two with directly controlled ray casting. The participants alternated between direct and PRISM. There were a total of 12 participants (1 female). Half of the participants began with direct, half with PRISM. Each target was a 30 cm cube and was positioned within a 10 × 10 × 25 meter (x, y, z) region centered at (0, 0, -37.5) in world coordinates, as shown in Figure 14. The participants began the experiment at world coordinate (0, 0, 0) looking down the negative z-axis. They were told in advance where the target region was (relative to their position) and the training session allowed them to become accustomed to the layout of the environment. The target sizes and the set of positions within the target region remained constant for each trial and for each participant. To avoid search time becoming a factor, the user was presented with one target at a time. Each time a target was shown, it first became visible directly in front of the user and then was slowly moved to its final position. Once the target reached its position a sound played indicating the user was permitted to select it. Upon selection, the next target would become visible and the procedure was repeated. The backdrop of the virtual environment was a star field (the white spots in Figure 13 are the stars), which helped increase the users' situational awareness and depth perception. 5.4.1 Results. We performed an ANOVA (Table VIII) on the data with completion time and error rate (misses/total button clicks) as the dependent variable. The independent variables included Interaction Type (direct or PRISM). In addition, we also looked at the trial number corresponding to the technique used (1 if it was the first trial using direct or PRISM, 2 if it was the second trial using the respective technique). Overall, our results show that PRISM was extremely effective in increasing the accuracy and decreasing completion time when using ray casting with distant targets. Interaction type was highly significant for both Completion Time (F(1,11) = 28.89, p < 0.001) and Error Rate (F(1,11) = 187.23, p < 0.001) and there was no evidence of learning effects between trials. These results are summarized in Figure 15.

5.4.2 User Feedback. Most participants thought PRISM was more effective than the direct form of ray casting, with 11 of 12 participants favoring PRISM. PRISM ray casting also received 3.75 out of 5 when asked how easy it was to learn. A common problem with ray casting and precision is the slight movement of the wrist when the user attempts to press the stylus button--an example of the "Heisenberg Effect" described in Bowman et al. [2001]. This problem has also been recognized by Gerber and Bechmann [2005] as a factor limiting the number of menu items that can be placed in their spin menu and was echoed by our participants. Many participants told us that whenever they pressed the button their hand would move slightly and cause them to miss when using direct ray casting. These participants told us that this problem was almost completely eliminated during the PRISM trials and that they felt like the ray "locked into place". 5.4.3 Discussion. The results of this experiment show that using PRISM with ray casting is extremely effective. PRISM solves a fundamental problem with ray casting, the inability to select small, distant objects. Carrying this further, it is likely to solve the equally important problem of not being able to accurately move objects from a distance. The problems with ray casting are caused by the fact that small rotations of the wrist result in large translations at the end of the ray, where the cursor or an object is being moved. This worsens as the length of the ray increases. PRISM works to counteract this by scaling down the rotation of the hand or source object (the red cone in our implementation). We also noticed that using PRISM for ray casting encouraged more comfortable hand positions and movements. We observed that when users had difficulty targeting objects with the directly controlled ray casting they tended to point outwards with their arms, sometimes using two hands much like they would when aiming a pistol. A more experienced user would normally "shoot from the hip" instead, which is much less fatiguing. PRISM encouraged the user to hold their hand in a comfortable position and move the ray only by rotating their wrist. This was partly due to the increased ability to target objects, however PRISM was also easier to use when the wrist was the primary joint involved. Sweeping out arcs with the shoulder or elbow is in general a much slower movement than a wrist rotation and this slower rotation forces PRISM into scaling mode. Once users realized they could take advantage of PRISM's modal switching more effectively when rotating their wrist quickly to cover larger angular distance they became much more effective. Of course, when designing a technique that requires the use of the shoulder or elbow to rotate something the Scaling Constant could be decreased to provide direct control when moving more slowly; however, for traditional ray casting, the Scaling Constant used in this experiment was a good choice. 

6. FUTURE DIRECTIONS 

A short-term issue we would like to further investigate is PRISM translation, specifically the axis-independent scaling technique. Axis-independent scaling offers a distinct advantage over uniform scaling; it helps the user move an object in a straight line or along a plane by eliminating drift along the axis of movement in which movement is slowest. Unfortunately, performance is slowed for diagonal movement, seen in the tasks with rotated target cubes in our user study. In the future, we plan to implement a version of PRISM that scales translation uniformly in all directions by using the absolute speed of the hand to determine one true CD ratio. In the long term, we would like to further investigate PRISM's applicability to other tasks and with a number of user interface widgets. As described in our user study, PRISM rotation can enhance selection with ray casting significantly. To follow this up, we would like to work with user interface controls such as menus and buttons. We believe that by using a PRISM controlled cursor we can increase "touch selection" accuracy and thus decrease the size of the typical widgets used in VR. In addition, the usability of a World in Miniature (WIM) may be significantly enhanced through PRISM manipulation and we plan on running experiments focused on this. 

7. CONCLUSIONS 

In the physical world, we move and rotate objects quite accurately by using friction and the object's inertia to help steady our movements. In the virtual world, none of these physical properties normally exist, and object manipulation is negatively affected by hand instability. We have shown in a series of user studies that PRISM increases precision and allow users to make purposeful and fine-grained adjustments to the position and orientation of an object in a timely manner. When examining translation and rotation separately, there was a statistically significant increase in performance when using PRISM over direct manipulation. With PRISM rotation and translation combined (6DOF task) results were less clear, however PRISM continued to out perform direct manipulation for trials requiring very high precision. Accuracy was increased in these tasks because PRISM successfully filters out a significant amount of hand instability; however, in situations where there is absolutely no tolerance for error, other techniques might be more appropriate. Unlike many other techniques aimed at increasing precision, PRISM does not place any limits on the granularity in which the user can work with objects and does not overconstrain interaction when precision is not required. By utilizing the principle of Fitts' Law, PRISM dynamically adjusts the control/display ratio without explicit user intervention to provide extra precision when moving slowly and direct, unconstrained manipulation when the hand is moving quickly. We believe that the natural control metaphor used by PRISM is also what makes it so easy to learn. We have shown that PRISM can also apply to the control of a 3D cursor. In particular, we have shown that a PRISM-enhanced version of ray casting can significantly increase the speed in which objects (or buttons and controls) can be selected and dramatically decreases miss rates. PRISM is applicable across a wide variety of tasks in immersive virtual environments wherever the hand directly controls the position and orientation of the cursor. It is our hope that this increase in precision will allow designers to increase the complexity of their virtual worlds without suffering from a manipulation and interaction "bottleneck".
Frequency-Based Identification of Correct Translation Equivalents (FITE) Obtained Through Transformation Rules
ARI PIRKOLA, JARMO TOIVONEN, HEIKKI KESKUSTALO and KALERVO JARVELIN University of Tampere

We devised a novel statistical technique for the identification of the translation equivalents of source words obtained by transformation rule based translation (TRT). The effectiveness of the technique called frequency-based identification of translation equivalents (FITE) was tested using biological and medical cross-lingual spelling variants and out-of-vocabulary (OOV) words in Spanish-English and Finnish-English TRT. The results showed that, depending on the source language and frequency corpus, FITE-TRT (the identification of translation equivalents from TRT's translation set by means of the FITE technique) may achieve high translation recall. In the case of the Web as the frequency corpus, translation recall was 89.2%­ 91.0% for Spanish-English FITE-TRT. For both language pairs FITE-TRT achieved high translation precision: 95.0%­ 98.8%. The technique also reliably identified native source language words: source words that cannot be correctly translated by TRT. Dictionary-based CLIR augmented with FITE-TRT performed substantially better than basic dictionary-based CLIR where OOV keys were kept intact. FITE-TRT with Web document frequencies was the best technique among several fuzzy translation/matching approaches tested in cross-language retrieval experiments. We also discuss the application of FITE-TRT in the automatic construction of multilingual dictionaries. 

1. INTRODUCTION 

Out-of-vocabulary (OOV) words constitute a major problem in cross-language information retrieval (CLIR) and machine translation (MT). In those cases where equivalent terms in different languages are etymologically related technical terms (cross-lingual spelling variants--as German konstruktion and English construction) it is possible to use a transliteration type of translation to recognize the target language equivalents of the source language words. In Pirkola et al. [2003] we generated automatically large collections of character correspondences in several language pairs for the translation of cross-lingual spelling variants. Equivalent term pairs in two languages were first extracted automatically from translation dictionaries, and then regular character correspondences between the words in the two languages were identified using an edit distance measure. Large sets of transformation rules augmented with statistical information were generated for automatic translation of spelling variants. We call the translation technique based on the generated rules transformation rule based translation (TRT). TRT is similar to transliteration except that no phonetic elements are involved in it. The term fuzzy translation is used in connection with TRT. It refers to the fact that TRT often gives many possible equivalents for a source word, not one equivalent or several alternatives like regular translation. In Toivonen et al. [2005] we showed that high translation recall--the proportion of source words for which TRT yields equivalents among all source words--may be achieved when most of the rules available for a source word are used in TRT. However, high translation recall is associated with low translation precision (the proportion of equivalents among all word forms yielded by TRT). In other words, the translation set containing the target language word forms often includes the correct translation equivalent of a source word and a large number of other word forms. It is obvious that a technique where words not found in a dictionary are translated by transformation rules would be useful in many information systems where automatic translation is part of the system. However, in many cases the TRT technique may be useless if it just indicates a set of possible translations for a source word but is not able to indicate the one correct equivalent, which was the case in Pirkola et al. [2003] as well as in Toivonen et al. [2005]. In the present research we attack this problem, moving TRT from fuzzy translation towards dictionary-like translation, where for each source word either one translation equivalent is indicated or the source word is indicated not to be translatable by means of TRT. For this we developed a novel statistical equivalent identification technique called frequency-based identification of translation equivalents (FITE). The identification of equivalents is based on regular frequency patterns associated with the target word forms obtained by TRT. In this paper we also present a novel feature of TRT: translation through indirect translation routes. If a direct translation from a source language into a target language fails to find an equivalent, the source word is retranslated into a target language through intermediate (pivot) languages. As in the case of direct translation the equivalents are identified from TRT's translation set by means of the novel FITE technique. Transitive translation through a pivot language is a well-known technique in CLIR, used to address the problem of limited availability of translation resources [Ballesteros 2000; Gollins and Sanderson 2001; Lehtokangas et al. 2004]. Indirect translation could be used in TRT in cases where direct translation is not possible due to the lack of translation resources (transformation rules). In this study, however, we investigate whether indirect translation improves FITE-TRT effectiveness. It may compensate the failures of direct translation and thereby increase translation recall. We explore the effectiveness of FITE in Spanish-English and FinnishEnglish TRT. German and French serve as intermediate languages for both language pairs. As test words we use terms in the domains of biology and medicine. The terms were selected from texts and real information requests of biomedical researchers. FITE-TRT is also applied as part of an actual CLIR system. The effectiveness of dictionary-based CLIR augmented with FITE-TRT is compared to the effectiveness of dictionary-based CLIR augmented with plain TRT and skipgram [Keskustalo et al. 2003] OOV word methods. We also run dictionary-translationonly (no OOV word technique is applied) and monolingual English queries as baselines. In Pirkola et al. [2006] we presented the main features of FITE-TRT and the first results on FITE-TRT effectiveness and the effectiveness of CLIR augmented with FITE-TRT. In this article we describe the FITE-TRT technique in more detail, present the find-equivalent algorithm, and extend the first study by using large word frequency lists mined from the Web as FITE-TRT's frequency source, and by comparing FITE-TRT to other OOV word methods in cross-language retrieval experiments. The novel FITE-TRT technique is fundamentally different from other OOV word methods/systems presented in the literature. For instance, Cheng et al. [2004] and Zhang and Vines [2004] both developed a Web-based translation method for Chinese-English OOV words, where the OOV words were extracted from bilingual Chinese-English texts found in Chinese Web pages using word co-occurrence statistics and syntactic structures. Meng et al. [2000] employed a TRT type rule-based approach to the OOV word problem. Phonetic mappings were derived from English and Chinese (Mandarin) pronunciation rules for English-Chinese spoken document retrieval. The researchers also considered Chinese name variation. An English proper name may have several character sequence variants and pronunciations in Chinese. To combat this problem the transliteration approach should involve approximate matches between the English and Chinese pronunciations. Fujii and Ishikawa [2001] used characterbased rules to establish mapping between English characters and romanized Japanese katakana characters. They also utilized probabilistic character-based language models, which can be seen as a variation of fuzzy matching. The technique is different from FITE-TRT but bears some resemblance to the fuzzy translation reported in Pirkola et al. [2003], however focusing on languages with different orthographies, thus having a different focus. The skipgram fuzzy matching approach to OOV words by Keskustalo et al. [2003] is discussed in Section 5.2.1. The rest of this article is organized as follows. Section 2 presents the TRT technique, its background research, the transformation rule collections, and the dictionary data that was used in the rule generation. In Section 3 we define the terms cross-lingual spelling variant and native word, and present the research problems and evaluation measures used in the experiments. The novel FITE technique is described in Section 4. Section 5 presents the methods and data used in the experiments and the findings. Section 6 contains discussion and conclusions. 

2. TRT, TRANSFORMATION RULES AND BACKGROUND RESEARCH 

The idea of TRT and the automatic method to generate transformation rules is described in Pirkola et al. [2003]. A transformation rule contains source and target language characters that are transformed, and their context characters. In addition, there are two important numerical factors associated with a rule: frequency and confidence factor, which may be used as thresholds to select the most common and reliable rules for TRT. Frequency refers to the number of the occurrences of the rule in the dictionary data that was used for the rule generation. Confidence factor (CF) is defined as the frequency of a rule divided by the number of source words where the source substring of the rule occurs. Below we present an example of a German-English rule: ekt ect middle 191 214 89.25 The rule is read as follows: the letter k, prior to t and after e, is transformed into the letter c in the middle of words, with the confidence factor being 89.25% (100% * 191/214). Examples of target word forms obtained in TRT are shown in Sections 4.2 and 4.3. In Pirkola et al. [2003] we studied TRT in combination with fuzzy matching-- digram and trigram matching. We investigated five source languages, with English being a target language for all the source languages. The results showed that for Finnish, German and Spanish the combined technique performed better than digrams and trigrams alone. For French and Swedish performance changes were slight. In Toivonen et al. [2005] we studied how effective TRT is without fuzzy matching. We found that translation recall was high when low frequency and confidence factor were used as thresholds to select the rules for TRT. However, at low confidence factor and frequency levels, translation precision was low. The FITE-TRT technique addresses this problem and, as we will show in this article, it achieves both high recall and precision. The transformation rules used in TRT in this study were generated using the rule generation method based on the use of dictionary data described in Pirkola et al. [2003]. The dictionary data consisted of a multilingual medical dictionary by Andre Fairchild (http://members.interfold.com/translator/) for the language pairs of Spanish-English, Spanish-German, Spanish-French, German-English, and French-English. For Finnish-English the data for rule generation was obtained by translating (1) a list of Finnish medical terms into English using a medical dictionary by Kielikone Inc., and (2) a list of Finnish terms in various domains into English using Kielikone's general-purpose dictionary. Thus, for Finnish-English we constructed two collections. The second collection was constructed because the first collection missed many important rules. Table I shows the number of entries and the average number of translations per an entry for each dictionary used in the rule generation (columns 2 and 3). Table I also shows the total number of rules in the generated rule collections as well the number of rules at or above the thresholds of CF = 4.0% and frequency = 2 applied in this study (columns 4 and 5). We applied the confidence factor and frequency thresholds because TRT may give very large translation sets, and at the present stage of development the TRT program is not efficient enough to process very large word sets. Due to the efficiency issues, we also applied a limit of 40 word forms: if there were more than 40 word forms in a translation set of an intermediate language the source word was retranslated by TRT with a confidence factor of 10.0% and frequency of 10, to yield a smaller translation set. As can be seen in Table I, each rule collection contains a high number of rules, which suggests that the rule generation method effectively captured spelling variation between the language pairs. 

3. RESEARCH PROBLEMS AND EVALUATION MEASURES 

We distinguish between two kinds of words in a language with respect to the words in another language: cross-lingual spelling variants and native words. Cross-lingual spelling variants are etymologically related words and therefore similar in the two languages, differing only slightly in spelling. A native word and its target language equivalent are not related to each other morphologically even if they share the same meaning. The words have different origins and etymologies in the history of the respective languages. The words do not have morphological or phonetic resemblance--or if there is some, it is purely accidental. As examples, consider the English words computer and chemotherapy and their Finnish equivalents tietokone and kemoterapia. The first pair computertietokone do not have morphological or phonetic resemblance, computer originating from Latin (computare, to calculate) and tietokone being a compound of knowledge and machine. The Finnish words tieto and kone are old words in the language. In the second pair chemotherapy-kemoterapia both words originate from Greek (chemeia + therapeia) and, albeit having been modified to fit the style of their present languages, still have not lost their morphological or phonetic resemblance. TRT is intended to translate spelling variants and FITE is intended to identify the translation equivalents of spelling variants and to indicate the native source language words--the source words that cannot be correctly translated by TRT. Using test word sets containing both types of words, we examine the following research questions: r In the case of spelling variants, how to effectively identify the correct equivalent of a source word among the many word forms produced by TRT when most of the transformation rules available for a language pair are used in TRT? r How to reliably identify native source language words? r Are word frequency lists mined from the Web competitive with the Web as a collection of documents as FITE-TRT's frequency source? r What are the translation recall and precision and indication precision (see the definitions below) of the proposed FITE-TRT method? r What is the contribution of each step in the FITE-TRT process to its overall effectiveness? r What is the effectiveness of a standard CLIR system boosted by the use of FITE-TRT in comparison to a CLIR system augmented with TRT and fuzzy matching OOV word methods, and in comparison to dictionary-translationonly CLIR and monolingual baselines? The effectiveness of FITE-TRT was evaluated by using the measures of translation recall, translation precision, and indication precision. For spelling variants translation recall is defined as the proportion of source words for which FITE identifies correct equivalents among all source words. For example, if there are 10 source words and TRT gives for these, 100 target language word forms, among which there are correct equivalents for 8 source words, then translation recall is 8/10 = 80%. Translation precision is defined as the proportion of correct equivalents among all words that are indicated as equivalents. For example, if FITE identifies 10 translation equivalents of which 9 are correct equivalents translation precision is 9/10 = 90%. For native words the question of what share of them is translated by TRT is an irrelevant question, and naturally recall is not measured for them. For native source words indication precision is defined as the proportion of words correctly indicated to be untranslatable by TRT. For example, if there are 5 native source words and FITE indicates that none of these translation equivalents are contained in the translation sets indication precision is 5/5 = 100%. Retrieval effectiveness was evaluated using the measures of mean average precision (MAP) and precision at 20 documents. MAP is a standard evaluation measure used in TREC (http://trec.nist.gov), and it refers to the average of the precision values obtained after each relevant document is retrieved. MAP is a system-oriented measure while precision at 20 documents is important from the practical IR standpoint. The probability of a searcher scanning further down a ranked result list decreases as (s)he scans down and we use a document cutoff value of 20 as a rule of thumb for the stopping point of scan. 

4. THE FITE TECHNIQUE 

4.1 Frequency Data FITE identifies the correct translation equivalents among the TRT generated word forms by their frequency distribution in some corpus. Frequencies for FITE were taken from the Web and word frequency lists. In the case of the Web we consider document frequency (DF) and in the case of frequency lists word frequency (WF). DF statistics were collected using the Altavista search engine and its language selection feature. A research assistant fed the word forms into the search engine, which reported for each word form the number of documents containing the word form. The word frequency lists were mined from the Web using a Web mining technique, which is described next. In the first step of the Web mining process a query script based on the use of text-based Web browser Lynx was run to fetch medical and biological documents in a desired language from the Google search engine. The query script described in Zhang and Vines [2004] was modified for this purpose. We used the following parameters and parameter values in the script: language [=English/Finnish/German/Spanish]; the number of documents to fetch [=700]; query keys [the words medicine, biology, and disease (conjuncted by the AND-operator) and the corresponding words in Finnish, German, and Spanish]. The use of these keys directed the actual Web mining towards medical and biological sub-Webs. In the second step, URLs were extracted from the fetched documents and were saved in a file. In the third step, the URL file was cleaned by removing duplicates so that only URLs with unique domain names were kept in the file. The URL file served as an input for the fourth step, the actual Web mining stage where documents were downloaded from each Web site represented in the URL file using a wget program (http://www.gnu.org/software/wget/). Wget's parameter "directory depth" was set at 3--on each Web site documents at directory depths 1­3 were downloaded. In the fifth step of the process all downloaded documents were combined into one large file. In the sixth step, word frequency lists were constructed from the combined document file. The number of documents downloaded from the Web varied depending on the language. For example, for German 35 000 documents were downloaded. The total size of these documents was 2.26 GB. The numbers of unique words contained in the frequency lists are as follows: English: 762,000 words Finnish: 886,000 words German: 470,000 words Spanish: 386,000 words intermediate language (French) was small it was not considered in the frequency list experiments, and we did not construct a word frequency list for French. Its minor contribution was probably due to the fact that it was used as the second intermediate language, rather than due to its linguistic features. In statistical MT the choice between translation alternatives depends on the translation probabilities of the alternatives and their context [Al-Onaizan et al. 1999; Brown et al. 1990]. Translation probabilities are computed on a basis of aligned corpora. In contrast to this, the source and target language corpora used by FITE are independent of each other. In statistical MT bigram and trigram language models are typically used to capture the context. FITE is based on a unigram language model: no context dependence of translations is assumed. 4.2 Frequency Pattern In order to avoid several long function definitions not precisely in the focus of our article, we introduce some notational conventions used in the definition of the FITE method. Notational Convention 1. Let SL be a source language and TL a target language, and sw be some source language word in the source language collection S. We denote this by sw  SL. We denote the word set produced by our TRT translation by TRT S L->T L (sw) using the transformation rules for SL- > TL translation. The result is a set: its elements tw hold the relationship tw  TRT S L->T L (sw). If the TRT translation is performed using a strict confidence factor (=10%) and a strict rule frequency (=10) (see Section 2), we denote this by TRT S L->T L|strict (sw). Notational Convention 2. Let sw be some word in source language SL: sw  SL. We denote its document frequency in the source language collection S by df S (sw). Note that if sw does not appear in any documents of S then df S (sw) = 0. If S is a source language wordlist containing word frequencies, we denote the frequency of sw in S by wf S (sw). Notational Convention 3. Let tw be some word of the target language TL in the target language document collection T : tw  TL. We denote its document frequency in T by dfT (tw). It refers to the frequency of target language documents that contain the word tw. Note that if tw does not appear in any documents of T then dfT (tw) = 0. If T is a target language wordlist containing word frequencies, we denote the frequency of tw in T by wfT (tw). Notational Convention 4. Let sw be some source language SL word: sw  SL, TL a target language, TWS = TRT S L->T L (sw) the word set produced by our TRT translation, and T a target language document collection or word list. The frequency-ranked list of words of TWS in T is denoted by R = trt-frank(TWS, T). Table II is an example of such a list with the frequency data added. For a given source language word sw we obtain this list by trt-frank(TRT S L->T L (sw), T ). The elements of this list are denoted by the usual notation, for example, trt-frank(TWS, T)[3] gives its third component. As an example, in the case of Table II, trt-frank(TRTSPA->ENG (biosintesis), EngWeb)[3] = biosyntesis. Its frequency is dfEngWeb (trt-frank(TRTSPA->ENG (biosintesis), EngWeb)[3]) = dfEngWeb (biosyntesis) = 634. The core of FITE is that except for the translation equivalents the word forms yielded by TRT are malformed rather than real words, or they are rare words, for example, foreign language words in the target language text. The equivalents belong to a language's basic lexicon and are much more common in the language than the other word forms. This regular frequency pattern allows the identification of the equivalents. The example in Table II shows the document frequency pattern associated with the word forms obtained by TRT for the Spanish word biosintesis in Spanish-English TRT in the English sub-Web. The word forms are sorted by document frequency, by trt-frank(TRTSPA->ENG (biosintesis), EngWeb). We can see that the DF of biosynthesis, the equivalent of biosintesis, is remarkably higher than the DFs of the other the word forms. This type of frequency distribution is very common for word forms within a translation set of TRT. Given a target word form ranking R = trt-frank(TWS, T), the magnitude of difference between the document frequency of the first word form (dfT (R[1])) and the document frequency of the second word form (dfT (R[2])), or the frequencies between R[2] and R[3] (see Section 4.5) forms the basis of the equivalent identification. We used the coefficient value (the magnitude of difference) of 10 for the identification of equivalents (both for Web and word frequency lists). The same pattern holds for the Web and word frequency lists, with the main differences being in that in the case of frequency lists word frequencies instead of document frequencies are considered and in that Web gives more malformed words than the frequency lists. The following definition of the function freqpattern-ok checks whether the frequencies of two target language words twi and tw j have the required pattern. Definition 1. Let twi and tw j be two candidate word forms in the target language (sub-) Web document collection, or word frequency list, T as given by TRT. Let dfT (twi ) and dfT (tw j ) be their frequencies in T . Let  be a corpus dependent normalizing factor,  > 1. The Boolean function freq-pattern-ok gives the value true if the frequency of twi in T is at least  times the frequency of tw j in T . freq-pattern-ok(twi , tw j , , T ) = true, if dfT (twi )  ( × dfT (tw j ) false, otherwise. Typically, the function freq-pattern-ok is applied on two consecutive words in a frequency-ranked order. For example, freq-pattern-ok(biosynthesis, biosintesis, 10, EngWeb) yields the value true (Table II). In the tests, the coefficient  was experimentally set at  = 10 using the training data (Section 5.1.1). 4.3 Relative Frequency There are situations where the highest DF (WF) is possessed by a word that is not the correct equivalent. For example, the source word may occur frequently in a target language collection; if TRT fails to translate the source word it may appear at the first position in a translation set. (A source word is always included in TRT's translation set because source and target language words may be identical.) Also, in the case of Web as a document collection there are mixed language pages some of which a search engine may consider target language pages, which wrongly increases the target DF of a source word found in the mixed language pages. TRT may also accidentally give high DF words that are not correct equivalents. As a solution for this problem, we compute relative document frequency (rel-df) and relative word frequency (rel-wf), defined as follows. Definition 2. Let sw be a source language word in the source language collection S, and tw a target language word form in the target language (sub-) Web document collection T as given by TRT. Let df S (sw) be the frequency of sw in S and dfT (tw) the frequency of the target language word tw in T . Let  be a corpus dependent normalizing factor,  > 0. The function rel-df gives the relative document frequency for tw in T. Definition 3. Let sw be a source language word in the source language word list S, and tw a target language word form in the target language word list T as given by TRT. Let wf S (sw) be the frequency of sw in S and wfT (tw) the frequency of the target language word tw in T . Let  be a corpus dependent normalizing factor,  > 0. The function rel-wf gives the relative word frequency for tw in T. rel-wf(tw, sw,, T, S) = wfT (tw)/(xwf S (sw)). The coefficient  is a corpus dependent normalizing factor. It is assigned such a value that rel-df and rel-wf > 1 indicate that the target word form is an equivalent, and rel-df and rel-wf < 1 indicate the equivalent is not found in the translation set. The coefficient values reflect the relative sizes of the sub-Webs/word frequency lists in relation to each other. In our case  = 2 was used in all test conditions. The value  = 2 was determined experimentally. The values of  from 1 to 2 are appropriate for the conditions where the target corpus is much larger than the source corpus, which was the case in our experiments. The Finnish word frequency list contains more words than the English list (Section 4.1). However, the sum frequency over all words is substantially higher in the English than Finnish list. This allows the use of  = 2 in the rel-wf formula also for Finnish-English. The example in Table III illustrates the case where the word with the highest DF is not the correct equivalent. The translation set contains the word forms and the associated frequencies of English Web pages for a Spanish source word fraccionamiento. A typical frequency pattern is found. However, fraccionamiento, the word with the highest DF, is the Spanish source word not translated into English. Its DF in the Spanish portion of Web is 416 000. It is not accepted as an equivalent since rel-df(fraccionamiento, fraccionamiento, 2, EngWeb, SpaWeb) < 1. We considered the two highest ranked word forms, and naturally also for the second form, fraccionamento, rel-df < 1. 4.4 Length Factor Cross-lingual spelling variants are close to each other in word length. A great difference between the length of a target word form and the source word is an indication of a wrong equivalent. The length factor is taken into account, as FITE identifies equivalents. The length criteria for accepting an equivalent candidate as an equivalent are shown in Table IV. It can be seen, for example, that when a source word contains 7 characters the target word form has to have from 5 to 9 characters in order to be accepted as an equivalent. Definition 4. Let sw be a source language word and len(sw) its length in characters. Likewise, let tw be a target language word and len(tw) its length in characters. The Boolean function tw-len-ok gives the value true if the length of the target word tw is within the range defined in Table IV. tw-len-ok(tw, sw) = true, if 4  len(tw)  7 and len(sw) = 5 true, if 5  len(tw)  8 and len(sw) = 6 true, if |len(tw) - len(sw)|  2 and 7  len(sw)  10 true, if |len(tw) - len(sw)|  3 and len(sw) > 10 false, otherwise 4.5 The Application of FITE In the empirical experiments, the source test words (Section 5.1.1) were translated into English by the TRT translation program. The applied thresholds were described in Section 2. The equivalents were searched for from the translation sets using the FITE technique. As described in Sections 4.2­4.4, the main criteria of equivalent identification of FITE are the following: (1) the frequency patterns of the top word forms tested by the function freq-pattern-ok, (2) the relative frequency criterion tested by the rel-df/rel-wf functions, and (3) length criterion tested by the function tw-len-ok. The basic idea is to apply the criteria 1­3 in three steps A­C: First, in Step A, direct translation is tried and then in Steps B and C the pivot language translations one after the other. The criteria are first applied to the highest ranking target word candidate as given by the function trt-frank. If these steps do not yield a solution, then basically the same steps are repeated with the second highest ranking target word candidate. This process is specified as Algorithm find-equivalent, which is presented in the Appendix. The algorithm is for the case of word frequency lists S and T for the source and target languages. In the case of Web document collections, the word frequency lists are replaced by a function that gives the Web document frequency for a given source word. The TRT rule bases for the source, pivot and target languages, as described above, need to be available but are not precisely defined (see notational conventions in Section 4.2). We use the notations and functions of preceding sections in the definition of the algorithm. The algorithm find-equivalent first tries the first candidates produced by the TRT direct or pivoted processes. It calls the procedure direct-trans to produce the frequency-based ranking of the direct TRT candidates. The procedure generates them as the list R, and then the first component of R is tested for the criteria 1­3 by the procedure test-cand. If the first component passes the test it is given as the equivalent. If not, the algorithm find-equivalent then uses the first, and finally the second, pivot language translation, given by the procedure pivot-trans, which first checks the number of pivot language word forms obtained. The TRT rules are used liberally (the thresholds of CF = 4.0% and frequency = 2 are used; see Section 2), if there are at most 40 candidates and otherwise strictly (the thresholds of CF = 10.0% and frequency = 10 are used). Either way produces a target language word candidate list TWS, which then is ranked by frequency and the first component tested for the criteria 1­3. If the first pass, focusing on the first-ranked components, is not successful, then the algorithm find-equivalent tries the second equivalent candidate produced by the TRT processes. The first component is still selected as the equivalent if the three criteria are fulfilled as follows: the second component passes the frequency pattern and relative frequency criteria and the first one, the length criterion. The second word form is selected as the equivalent only if the first word form does not meet the length criterion and the second form meets all the three criteria. Otherwise the source word is indicated to be untranslatable by means of TRT--the string "nil" is returned. We found empirically the need to compare the second candidate word form to the third form to find out if there are more than one correct target language words (high frequency word forms) in the translation set. If there are exactly two acceptable words, the first word rather than the second, is selected as the equivalent based on our observations that in these cases the equivalent tends to be at the first position. The second word form is accepted as the equivalent only if the first form does not meet the length criterion. In the actual experiments described in Section 5, the algorithm findequivalent was applied/modified as follows. In the case of frequency lists the second pivot language (French) was not considered. Finnish-English experiments differed from the Spanish-English experiments in that there were two direct translation routes thanks to two Finnish-English rule collections. The order of the use of the translation routes for Finnish-English was as follows: Finnish-English/collection 1, Finnish-English/collection 2, Finnish-GermanEnglish, and Finnish-French-English. All the source words were in base form, and only base form equivalents were accepted as correct equivalents. Thus, equivalents in plural form and the derivatives of the actual equivalents were not accepted as correct equivalents. This is because our aim is to develop a dictionary-like rule-based translation method, which indicates the precise equivalents of source words. We conclude this section by summarizing, in Figure 1, the FITE-TRT process. The left side of the figure describes the production of transformation rules and the translation of OOV words by means of TRT. The FITE technique--the focus of the present article--is the grey shaded area. FITE-TRT effectiveness was evaluated using the measures of translation recall and precision and indication precision. 

5. EXPERIMENTS AND FINDINGS 

In this section we present the methods and data used in the FITE-TRT and CLIR effectiveness experiments and the experimental results. Subsection 5.1 presents the training and test words, describes how the words were translated by means of TRT, and presents the findings of the FITE-TRT effectiveness experiments. The CLIR experiments are dealt with in Subsection 5.2. 5.1 FITE-TRT Effectiveness 5.1.1 Training and Test Word Sets and Translation by TRT. FITE-TRT is intended to handle both spelling variants and native source language words. The training and test word sets contained both types of words. Next we describe the selection of training and test words. Then we characterize quantitatively the difference between cross-lingual spelling variants and native words. We used a training word set for the development of the FITE technique. The set contained the title words (n = 75) of the Spanish CLEF topics numbered 91 to 109. In addition to native Spanish words, the titles contain Spanish-English spelling variants, native English words, and English acronyms. The effectiveness of FITE-TRT was evaluated using four sets of test words. For each source language word set there was a corresponding English word set that contained the equivalents of the source words. For the first two test word sets a list of English biological and medical terms was gathered manually from the index of CLEF's [Peters 2005] LA Times collection. The English terms were translated into Spanish and Finnish by means of translation dictionaries and monolingual (Spanish and Finnish) medical dictionaries. From these words we selected Spanish-English and Finnish-English spelling variants for our tests. The identification of spelling variants was done based on the similarity of the Spanish-English and Finnish-English word pairs judged by a researcher. The similarity feature used as a selection criterion is discussed in Section 3 and later in this section. The Spanish terms formed the first, and the Finnish terms the second, test word set. Both contained the same terms (n = 89) albeit in different languages. These terms are called bio-terms. For the third and fourth test word sets, TREC Genomics Track 2004 topics in Spanish and Finnish (Section 5.2.1) were translated into English using the UTACLIR system, an automatic dictionary-based query translation and construction system developed in the Information Retrieval Laboratory at the University of Tampere [Hedlund et al. 2004]. The OOV keys of the UTACLIR runs were used as test words. Among the OOV keys there were, in addition to spelling variants, native Spanish/Finnish words as well as English words and English acronyms. The Spanish word set contained 98 and the Finnish set 53 OOV keys (after the removal of short words). The difference in the number of the OOV keys reflects the different sizes of UTACLIR's Spanish-English and Finnish-English dictionaries. These test key sets are here called OOV-UTACLIR-SPA-ENG and OOV-UTACLIR-FIN-ENG. Words containing four or less letters were not translated by TRT. This restriction was set because the short words were English acronyms and they need not be translated. (Generally, acronyms cannot be translated by means of TRT which only handles spelling variants.) On the other hand, cross-lingual spelling variants are not very short words. Within the four test word sets there were two short (4-letter) spelling variants, which were removed from the sets according to the short word restriction. The total number of unique source words translated by TRT was 89 + 98 (Spanish) + 89 + 53 (Finnish) = 329 words. To characterize quantitatively the difference between cross-lingual spelling variants and native words, we computed for both types of source language test words their degree of similarity with respect to their English equivalents using a simple measure of longest common subsequence divided by the mean length of source and target words (LCS/MWL). LCS is defined as the length of the longest subsequence of characters shared by two words. The closer to 1.0 LCS/ MWL is, the more similar the words are. As an example, for the Spanish bioterm omnivoro LCS/MWL = 7/((8 + 8)/2) = 0.875 with respect to the English equivalent omnivore. For the native Spanish OOV word vinculante LCS/MWL = 0.353 with respect to the equivalent binding. Table V shows the results of LCS/MWL calculations. From the viewpoint of TRT, the target language (English) words as source words are similar cases as spelling variants and they were thus regarded as spelling variants in the sets OOV-UTACLIR-SPA-ENG and OOV-UTACLIR-FIN-ENG. (As mentioned in Section 4.3, a source word is included in TRT's translation set because source and target language words may be identical.) In cases where native Spanish and Finnish words had multiple meanings in English the meaning that appeared in the Genomics Track topic was selected for LCS/MWL calculation. Both the Spanish-English and Finnish-English OOV word sets contained five native words. It can be seen in Table V that for bio-terms and spelling variant OOV words, the average LCS/MWLs are 0.839 and 0.911 (Spanish-English) and 0.784 and 0.853 (Finnish-English). For native Spanish and Finnish words average LCS/MWLs are much lower: 0.339 for Spanish and 0.361 for Finnish. Low standard deviation figures show that the LCS/MWL values are clustered around the average values. In the experiments the source words were translated by the TRT program through direct and indirect translation routes into English using the confidence factor and rule frequency thresholds (Section 2). The equivalents of source words were identified from TRT's English translation sets by means of the FITE technique, as described in Section 4.5. Like target language translation sets, the intermediate translation sets are often large, and only five top German and French forms in a frequency-ranked translation set were further translated into English. Different English translation sets corresponding to the same Finnish/Spanish source word were combined. 5.1.2 Findings. Table VI reports the translation recall and precision results for bio-terms, and Table VII the contribution of different translation routes to the recall for bio-terms. Table VIII presents the translation recall and precision, and indication precision results for OOV-UTACLIR-SPA-ENG and OOVUTACLIR-FIN-ENG words. Table VI shows that Spanish-English FITE-TRT reaches 91.0% recall in the case of Web document frequencies and 82.0% recall in the case of word frequency lists. Finnish-English FITE-TRT reaches 71.9% (Web) and 67.4% (frequency lists) recall. While Spanish-English FITE-TRT achieves higher recall, precision is approximately the same and it is remarkably high for both language pairs: 97.0%­98.8%. The same trends hold for the OOV words (Table VIII): for Spanish-English recall is higher than for Finnish-English, and for both language pairs precision is very high (95.0%­97.6%). Table VII shows that the contribution of direct translation to the recall is substantial for both language pairs. For Finnish-English FITE-TRT, the contribution of the second direct route (collection 2) to the recall is high. Indirect translation adds recall only for Spanish-English. In the case of Web as a frequency corpus the first pivot language adds recall by 6.7% while the second one adds recall only by 2.2%. For the native words indication precision is 100% in all test situations (Table VIII). There were only 10 native Spanish and Finnish words in all, however the results are reasonable since the cases where TRT accidentally gives correct words are not common. 5.2 CLIR Effectiveness 5.2.1 Methods and Data. FITE-TRT was applied as part of an actual CLIR system. As test data we used TREC Genomics Track 2004 data [Hersh et al. 2005]. The data consisted of 50 test topics, a subset of the Medline collection containing around 4.5 million documents, and relevance judgments. Queries were formulated on the basis of the Title and Need fields of the topics. The data are well suited for investigating FITE-TRT since the topics are rich in technical (mainly biological and medical) terms. The topics were translated manually into Spanish and Finnish by a researcher. The final Spanish topics were formulated by a knowledgeable Spanish speaker (a university teacher of Spanish). The researcher is a native Finnish speaker and has expertise in medical informatics. The test system was the InQuery retrieval system [Allan et al. 2000; Larkey and Connell 2005]. InQuery is a probabilistic retrieval system based on the Bayesian inference network model. Queries can be presented as a bag of word queries, or they can be structured using a variety of query operators. In this study the translated queries were structured using InQuery's #syn-operator as described in Pirkola [1998] and Sperer and Oard [2000]. The keys within the #syn-operator are treated as instances of one key. The Spanish and Finnish topics were translated back into English using the UTACLIR system and the queries were then run on the Genomics Track test collection. UTACLIR's output without any OOV word technique provides crosslingual baseline for the FITE-TRT queries for which UTACLIR's OOV words were translated by means of TRT and equivalents were identified using FITE. The original English queries were also run to show the performance level of the translated queries. We also compared the effectiveness of FITE-TRT queries to the effectiveness of plain TRT and skipgram queries. Plain TRT is the TRT part of FITE-TRT, and for plain TRT queries UTACLIR's OOV words were translated by TRT with CF = 4% and frequency = 2. All translations of a source word were included in a query and were wrapped in the #syn-operator. In skipgram queries the OOV words were translated using a skipgram fuzzy matching technique [Keskustalo et al. 2003]. This string matching technique is a generalization of the n-gram technique where words are split into digrams on the basis of both consecutive as well as nonconsecutive characters (see below). In these comparison experiments only direct translation/matching was examined since it is not sensible to study indirect fuzzy matching. Also, indirect TRT without frequency-based selection of intermediate word forms for further translation, would give very long queries that would be hard to manage. The skipgram fuzzy matching technique constructs digrams of both consecutive and nonconsecutive characters of words [Keskustalo et al. 2003]. The generated digrams are put into comparable categories based on the number of skipped characters as digrams are constructed. The character combination index (CCI) indicates the number of skipped characters as well as the comparable categories. Here we used CCI = ([0], [1, 2]). This means that digrams formed of consecutive characters form one comparable category, and digrams with one and two skipped characters, the other. CCI = ([0], [1, 2]) was very effective in the study conducted by Keskustalo et al. [2003] who explored the same general problem as we do in this article: the identification of translation equivalents of cross-lingual spelling variants. Skipgrams with CCI = ([0], [1, 2]) outperformed conventional digrams formed of consecutive characters of words. In the skipgram experiments each OOV word was matched against each string in the index of the TREC collection. Two types of queries were constructed: for each OOV word (1) two best matches and (2) five best matches were selected for a query. In the query the best matches were linked to each other with InQuery's #syn-operator. All inflected query words were rendered into base form for a dictionary lookup. For Finnish, UTACLIR's morphological analyzer gave base forms for most inflected words, and those that the analyzer was not able to handle were lemmatized manually. All Spanish inflected words were lemmatized manually. Manual lemmatization of the inflected keys was necessary because at this stage of development TRT only translates lemmas. Thus, the results show CLIR performance when a searcher gives query keys in base form. The results were tested for significance by the Wilcoxon signed rank test [Conover 1980]. The Wilcoxon test takes into account both the direction and magnitude of change between each comparable result of a query. In summary, we run the following queries in Spanish-English and FinnishEnglish CLIR experiments: r Original English queries r UTACLIR baseline (with OOV words passed through unchanged) r UTACLIR + FITE-TRT (Web) r UTACLIR + FITE-TRT (frequency lists) r UTACLIR + TRT r UTACLIR + skipgrams with two best matches r UTACLIR + skipgrams with five best matches 5.2.2 Findings. The results of the retrieval experiments are presented in Tables IX­XI. The statistical significance of the test queries was tested against the UTACLIR baseline (Tables IX, X) and against UTACLIR + FITE-TRT/Web (Table XI). In the tables the statistical significance is indicated by asterisks. As expected, the queries where OOV keys are translated by FITE-TRT perform better than the baseline queries where OOV keys are retained untranslatable (Tables IX and X). In Spanish-English CLIR, MAP improvement percentages are 40.3% (Web) and 35.2% (frequency lists). Precision at 20 documents is improved by 50.5% (Web) and 49.2% (frequency lists). Also FinnishEnglish FITE-TRT queries remarkably outperform the CLIR baseline although performance improvements are smaller than in the case of Spanish-English (Table X). All Spanish-English and Finnish-English results are statistically significant at p = 0.001. These findings are in agreement with the high number of OOV keys in the UTACLIR runs and FITE-TRT's high translation recall and precision. It should be noted that some OOV words may only be marginally topical, in which case correct FITE-TRT identification may result in a performance drop. Therefore FITE-TRT performance does not always correlate linearly with CLIR+ FITE-TRT performance. Spanish-English FITE-TRT queries perform better than Finnish-English FITE-TRT queries, which were much longer and more ambiguous than SpanishEnglish queries due to the larger coverage of UTACLIR's Finnish-English dictionary. The higher degree of translation ambiguity and FITE-TRT's lower performance resulted in lower CLIR performance. The higher performance of English queries with respect to the performance of Spanish-English and in particular Finnish-English queries is mostly caused by translation ambiguity. Table XI shows the performance of FITE-TRT, TRT, and skipgram queries. The results are ranked based on MAP values. It can be seen that for both language pairs the best OOV word method is FITE-TRT with Web document frequencies. However, it shows significantly better results only against the cases of Spanish-English/skipgram/2 and Finnish-English/TRT, and the results are significant only at p = 0.05. In the latter case, the difference in MAP is small (0.2447­0.2393) but systematic, and hence significant. In comparison to UTACLIR baselines (Tables IX and X) all queries perform well. It was expected that plain TRT queries yield good results since TRT with CF = 4% and frequency = 2 very often gives a source word's correct equivalent while the other translations typically are malformed word forms not occurring in the database index and having no effects whatsoever on retrieval results. In many applications (outside cross-language document retrieval) it is, however, important to avoid the ambiguity of TRT and obtain one reliable translation, as given by FITE-TRT. 

6. DISCUSSION AND CONCLUSIONS 

In this study we first examined the following two basic questions. Regarding spelling variants, how to effectively identify the correct equivalent of a source word among the many word forms produced by TRT when most of the transformation rules available for a language pair are used in TRT. How to reliably identify native source language words: source words that cannot be correctly translated by TRT. We devised the FITE-TRT technique--a novel OOV word translation technique. Its effectiveness was tested for Spanish-English and Finnish-English spelling variants and actual OOV words in the domains of biology and medicine. Here the research questions were as follows. What are the translation recall and precision and indication precision of the proposed FITE-TRT method? Are word frequency lists mined from the Web competitive with the Web as a collection of documents, as FITE-TRT's frequency source? What is the contribution of each step (direct and indirect translation routes) in the FITE-TRT process to its overall effectiveness? We found that depending on the source language and frequency source, FITETRT may achieve high translation recall. When equivalents were identified on the basis of Web document frequencies Spanish-English FITE-TRT achieved 89.2%­91.0% recall. For Finnish-English and for frequency lists mined from the Web, recall was lower but still substantial. The lowest recall (67.4%) was obtained for Finnish-English/frequency lists. The results indicated that FITETRT achieves high precision. FITE indicates precisely the equivalents of source words as well as the native words. For Spanish-English and Finnish-English test words translation precision was 95.0%­98.8%. All native OOV words were correctly indicated to be untranslatable by TRT. The test requests only contained 10 native OOV words: the results reported here need to be corroborated using a larger set of native words. The contribution of direct translation to the overall recall was substantial for Spanish-English bio-terms. Direct translation achieved 82.0% recall while the overall recall was 91.0%. We expected that Finnish-English FITE-TRT through pivot languages would have compensated failures of direct translation but that did not happen. Indirect translation did not help at all. These findings suggest that that the costs of indirect translation against its benefits are high. Because a pivot language increases FITE-TRT complexity it does not seem sensible to use two pivot languages as part of a FITE-TRT system. Last, we examined the effectiveness of a standard CLIR system boosted by the use of FITE-TRT in comparison to a CLIR system augmented with TRT and fuzzy matching OOV word methods, and in comparison to dictionarytranslation-only CLIR and monolingual baselines. It was shown that FITETRT with Web document frequencies was the best technique among several approaches to handling OOV words tested in the experiments. Dictionarybased CLIR augmented with FITE-TRT performed substantially better than the baseline, where OOV keys were kept intact. In Spanish-English CLIR MAP improvement percentages were 40.3% (Web) and 35.2% (frequency lists). Also Finnish-English FITE-TRT queries remarkably outperformed the CLIR baseline although performance improvements were smaller than in the case of Spanish-English (about 26% for both Web and frequency lists). The TRT program we used in this study was not able to process a high number of word forms in a reasonable time frame, and we had to apply CF and frequency thresholds. In the preliminary tests we translated without using any thresholds, and for long words we had to end the program because it was not able to complete the translation process within a day. We observed that for many source words, equivalents were not found in translation sets because the CFs and/or frequencies of the relevant rules were below the thresholds. We therefore expect that recall values can still be improved by using a more sophisticated TRT program that allows efficient TRT even without the use of thresholds. For example, in the case of Spanish/bio-terms/Web there were 81 correct identifications, one wrong identification, and seven words for which TRT did not identify equivalents. For five of the seven words, equivalents were not contained in the translation sets because of low CF or frequency rules. The five words and the rules are shown in Table XII. Also deficiencies in the Finnish-English rule collections and word frequency lists caused recall errors. FITE-TRT effectiveness was better for SpanishEnglish than Finnish-English. The better effectiveness can be attributed to the higher quality of Spanish-English rule collection. Deficiencies in the FinnishEnglish transformation rules can be overcome by using more data in rule generation. The frequency lists we constructed using Web mining turned out to be good frequency data for FITE. However, for some source word/equivalent pairs, frequencies were too low for rel-wf formula, which resulted in a decrease in recall performance. This deficiency can be overcome by adding more data to the existing lists. The main advantage of using frequency lists is that after the lists have been constructed FITE-TRT is independent of the Web. This is important when a practical FITE-TRT implementation is developed. The frequency lists we used contain biological and medical terms in accordance with the test terminology used in the study. The lists are large and contain terms in various domains. The application of FITE-TRT in the other domains may require lists with different types of terms. However, for each domain, the lists are concise enough to be held in main memory for efficient implementation. Overall the percentage of wrong equivalents indicated by FITE was small. The identification of derivatives and plural forms of the correct equivalents was the primary cause of precision errors. As an example, for the Finnish word leukosyytti and the Spanish word bacteria the correct equivalents are leucocyte and bacterium while FITE gave the words leucocytic and bacteria. Many of these types of cases could be solved by augmenting the transformation rules with word class information, for example, that a rule is likely to refer an adjective rather than a noun. Information on OOV word's word class is achieved when the sentential context of the OOV word is known. At present TRT is only intended to translate singular base forms. Word class information is needed if FITE-TRT will be applied to running texts containing inflectional word forms. This would imply the supplementation of rule collections with word class information. The next main challenge in the FITE-TRT research is to improve the rules such that FITE-TRT can handle words in a running text. The CLIR experiments showed that the best fuzzy translation/matchingbased query was FITE-TRT with Web document frequencies. The FITE component of FITE-TRT was the focus of this article, and here an important issue is its contribution in CLIR. The results showed that in the case of Finnish-English, FITE-TRT was significantly better than plain TRT but only at p = 0.05. In the case of Spanish-English, FITE-TRT did not show significantly better results. Overall, the results are inconclusive, and the issue needs to be investigated more thoroughly in future research. Such factors as query structure and other than OOV keys affect the effectiveness of FITE-TRT and TRT queries. Also, efficiency needs to be taken into account in future research. TRT queries are much longer than FITE-TRT queries and thus require more processing power. On the other hand, the FITE component of FITE-TRT increases computational expense. In other information systems than retrieval systems, in particular in MT fuzzy translation, TRT does not come into question. The good quality of translations achieved through FITE-TRT suggests that it can contribute to better MT performance. There is still one CLIR-related application of FITE-TRT that is worth mentioning: an automatic construction of multilingual dictionaries of technical terms and proper names by means of FITE-TRT. The dictionary construction process could be designed to be largely automatic thanks to FITE-TRT's high degree of effectiveness. Given a list of words and a set of transformation rule collections the process would automatically yield the translation equivalents of the words in different languages--the result would essentially be a multilingual dictionary. The construction of dictionaries is a non-time-critical task; given enough time it would be possible to construct large multilingual dictionaries. The cost benefits of an automatic method are obvious. It can easily be seen that there is a difference in the cost of automatic as opposed to manual construction of a dictionary of, say, 10 languages and 50 000 dictionary entries.


A Formal Model of Annotations of Digital Content
MARISTELLA AGOSTI and NICOLA FERRO University of Padua

This article is a study of the themes and issues concerning the annotation of digital contents, such as textual documents, images, and multimedia documents in general. These digital contents are automatically managed by different kinds of digital library management systems and more generally by different kinds of information management systems. Even though this topic has already been partially studied by other researchers, the previous research work on annotations has left many open issues. These issues concern the lack of clarity about what an annotation is, what its features are, and how it is used. These issues are mainly due to the fact that models and systems for annotations have only been developed for specific purposes. As a result, there is only a fragmentary picture of the annotation and its management, and this is tied to specific contexts of use and lacks-general validity. The aim of the article is to provide a unified and integrated picture of the annotation, ranging from defining what an annotation is to providing a formal model. The key ideas of the model are: the distinction between the meaning and the sign of the annotation, which represent the semantics and the materialization of an annotation, respectively; the clear formalization of the temporal dimension involved with annotations; and the introduction of a distributed hypertext between digital contents and annotations. Therefore, the proposed formal model captures both syntactic and semantic aspects of the annotations. Furthermore, it is built on previously existing models and may be seen as an extension of them.

1. INTRODUCTION 

Digital Library Management Systems (DLMS) are currently in a state of evolution: today they are simply places where information resources can be stored and made available to end users, whereas tomorrow they will increasingly become an integrated part of the way the user works. For example, instead of simply downloading a paper and then working on a printed version, a user will be able to work directly with the paper by means of the tools provided by the DLMS and share their work with colleagues. By doing this, the user's intellectual work and the information resources provided by the DLMS can be merged to form a single working context. The DLMS, therefore, is no longer perceived as something external to the intellectual production process, nor is it seen as a mere consulting tool; instead it becomes an intrinsic and active part of the intellectual production process, as pointed out in Agosti and Ferro [2005a], and Candela et al. [2006]. This turning point in DLMS also clearly emerges from the outcomes of the third brainstorming meeting, organized by DELOS1 , the European Network of Excellence on Digital Libraries funded by the EU's 6th Framework Programme. The main conclusions were the following: first, digital libraries need to become more user-centered; second, digital libraries should not simply be passive repositories, rather they should provide users with tools for more active cooperation and communication; and third, there is an increasing need for generalized digital library management systems [DELOS 2004]. Annotations are an effective means to enable the interaction between users and the DLMS we envision, since their use is a diffuse and very well-established practice. Annotations are not only a way of explaining and enriching an information resource with personal observations, but also a means of transmitting and sharing ideas to improve collaborative work practices. Furthermore, annotations allow users to naturally merge and link personal contents with the information resources provided by the DLMS so that a common context unifying all of these contents can be created. Furthermore, annotations cover a very broad spectrum, because they range from explaining and enriching an information resource with personal observations to transmitting and sharing ideas and knowledge on a subject. Therefore, annotations can be geared not only to the individual's way of working and to a given method of study, but also to a way of doing research. Moreover, they may cover different scopes and have different kinds of annotative context: they can be private, shared, or public, according to the type of intellectual work that is being carried out. In addition, the boundaries among these scopes are not fixed, rather they may vary and evolve with time. Finally, annotations call for active involvement, the degree of which varies according to the aim of the annotation: private annotations require the involvement of the authors, whereas shared or public annotations involve the participation of a whole community. Therefore, annotations are suitable for improving collaboration and cooperation among users.

As pointed out by Ioannidis et al. [2005], this turning point of Digital Library (DL) requires that "DL development must move from an art to a science [and it needs] unifying and comprehensive theories and frameworks across the lifecycle of DL information" [Ioannidis et al. 2005, p. 266]. The Streams, Structures, Spaces, Scenarios, Societies (5s) model, proposed by Goncalves et al. [2004a], is ¸ an example of such a framework and was one of the first efforts in this direction. More recently, the reference model for DLMS [Agosti et al. 2006a] aims at laying the foundations and identifying the cornerstone concepts within the universe of digital libraries, thus facilitating the integration of research and proposing better ways of developing appropriate systems; the notion of annotation has been explicitly introduced in this model as a first class concept for the universe of DLs. The aim of this article is to contribute to the development of such unifying frameworks by proposing a formal model for the annotation of digital contents. The motivations of this proposal lie in the previous presentation: DLs are moving towards more mature DLMS, which are supported by well defined formal frameworks, and annotations are also headed in that broad direction. Furthermore, to date there has been little agreement about what an annotation is, nor has a comprehensive and formal model of the annotation been proposed. With respect to this last point, Buneman et al. [2002, p. 150] state that: view annotation2 is becoming an increasingly useful method of communicating meta-data among users of shared scientific data sets, and to our knowledge, there has been no formal study of this problem. and Bottoni et al. [2003, p. 216] point out that: strangely enough, there is not an agreement yet on the definition of digital annotation, or on how to distinguish it from other digital entities (e.g. hyperlinks, metadata, newsgroup messages). Furthermore, an analysis of the basic operations, to be enabled by a digital annotation system, seems to be lacking. The aim of the formal model we propose is to formalize the main concepts concerning annotations and to define the relationships between annotations and annotated information resources. Therefore, the proposed formal model captures both syntactic and semantic aspects of the annotations, as well as building on previously existing models, such as the Streams, Structures, Spaces, Scenarios, Societies model. This new model thus becomes as compatible as possible with the previous ones and may be seen as an extension of them. The rest of this article is organized as follows: Section 2 provides an overview of annotations and their use in different contexts, thus presenting the reader with some background knowledge and the main issues concerning annotations; Section 3 highlights the key points about annotations that have to be taken into consideration when modeling them, and introduces the different areas covered by the proposed formal model; Sections 4 to 8 explain and formalize the various concepts of the formal model, according to what has been anticipated in Section 3; Section 9 capitalizes on the definitions introduced in the previous sections and proposes a comprehensive definition of annotation; Section 10 shows how the notion of hypertext between annotated objects and annotations follows from the proposed definition of annotation; finally, Section 11 draws some conclusions and discusses possible directions for future work. 

2. BACKGROUND ON ANNOTATIONS 

Over the years, a lot of research has been done on annotations. The main focus of this work has been on the employment of ad hoc devices, or handheld devices that enable reading appliances with annotation capabilities [Marshall et al. 1999, 2001a; Marshall and Ruotolo 2002; Schilit et al. 1998], and the design and development of document models and systems that support annotations [Agosti et al. 2007a; Phelps and Wilensky 1996, 1997, 2000a, 2000b, 2001; Bottoni et al. 2003] in specific management systems, in particular: -- in the Web [Handschuh and Staab 2003; Bottoni et al. 2004, 2005a, 2005b; Brush et al. 2001, 2002; Davis and Huttenlocher 1995; Nagao 2003; W3C 2005a, 2005b, 2007], -- in digital libraries [Agosti et al. 2003, 2005a; Agosti and Ferro 2005b; Agosti et al. 2005b; Agosti and Ferro 2003a; Agosti et al. 2004; Constantopoulos et al. 2004 Rigaux and Spyratos 2004; Gueye et al. 2004; Frommholz et al. 2003, 2004; Neuhold et al. 2004; Thiel et al. 2004], and -- in databases [Stein et al. 2002; Bhagwat et al. 2004; Buneman et al. 2001, 2002, 2004; Tan 2004]. The aim of this section is to introduce the two main approaches that have been adopted for dealing with annotations: we can consider them either metadata, which is discussed in Section 2.1, or content, which is presented in Section 2.2. This broad distinction in the viewpoints about annotations is also pointed out by Marshall [1998] when she distinguishes between formal versus informal annotations, where the former are metadata and the latter are "marginalia of the sort that we write to ourselves as we read a journal article," [Marshall 1998, p. 41], which correspond to the content. This section does not aim at providing a full and exhaustive survey of all the systems offering annotation capabilities that have been developed so far; for this please refer to Agosti et al. [2007a]; Handschuh and Staab [2003]; and Nagao [2003]. Instead, the goal here is to describe some relevant uses and features of annotations, so that in Section 3, these features can be used to gain some insights which will prove useful when developing a formal model for annotations. 2.1 Annotations as Metadata Annotations can be considered metadata, that is, additional data which relate to an existing content and clarify the properties annotated content. With this aim in mind, annotations have to conform to some specifications that define the structure, the semantics, the syntax, and even the values that annotations can assume. The recipients of this kind of annotation are both people and computing devices. On the one hand, metadata can benefit people because, if they are expressed in a human-readable form and their format and fields are known, they can be read by people and used to obtain useful and well-structured information about an existing content. On the other hand, metadata offer computing devices the means for automatically processing the annotated contents. Consider, for example, the case of the Machine Readable Cataloging (MARC)3 records: not only they are a useful information source for the users of Online Public Access Catalogs (OPAC), which often make MARC records accessible, but they also act as a standard for the representation and communication of bibliographic and related information in machinereadable form, and hence for the automatic processing of this information by computing devices. Note that the examples presented below have the dual use just described: they can be considered useful for both people and computing devices. A relevant example of this use of annotations is the MPEG-7 standard, formally named "Multimedia Content Description Interface," which is a standard for annotating and describing multimedia content data [ISO 2004]. To a certain degree MPEG-7 supports the interpretation of the information meaning, which can be passed onto, or accessed by, a device or a computer code. MPEG-7 is not aimed at any application in particular; rather, the elements that MPEG-7 standardizes can support many broad ranges of applications. In this case, annotating a multimedia document means filling in the various fields provided by the MPEG-7 standard in order to describe the features of the object at hand. Similar uses of annotations can be found in the natural language processing field; for example, part of speech tagging consists of annotating each word in a sentence with a tag that describes its appropriate part of speech so as to decide whether a word is a noun, a verb, an adjective, and so on [Jurafsky ¨ and Martin 2000; Manning and Schutze 2001]. In both cases, annotations are usually embedded in the annotated digital objects. A broader example of the use of annotations as metadata is provided by the Semantic Web [W3C 2007] initiative promoted by the World Wide Web Consortium (W3C), which aims at enhancing human-understandable data, namely Web pages, with computer-understandable data, namely metadata, so that "information is given well-defined meaning, better enabling computing devices and people to work in cooperation" [Berners-Lee et al. 2001]. The process of adding metadata to Web pages is called semantic annotation, because it involves the decoration of existing data, such as a piece of text whose content is understandable only to people, with semantic metadata that describe that piece of text, so that computers can process it and thus in turn, offer automation, integration, and reuse of data across various applications [Handschuh and Staab 2003]. The Semantic Web makes use of the Resource Description Framework (RDF)4 as a syntax for describing and exchanging metadata. In this context, the Annotea project developed by the W3C [Kahan and Koivunen 2001; W3C 2005a] sees annotations as metadata and interprets them as the first step in creating an infrastructure for handling and associating metadata with content, thus leading to the Semantic Web. Annotea predefines annotation types from a list that contains some of the following: comments, notes, explanations, or other types of external remarks that can be attached to any Web document or a selected part of the document, without modifying the document. Annotea uses RDF and eXtensible Markup Language (XML)5 for describing annotations as metadata, and XPointer6 for locating the annotations in the annotated document. Annotea employs a client-server architecture, where annotations reside in dedicated servers, and a specialized browser is capable of retrieving them upon request when visiting a Web page. Koivunen and Swick [2001] and Koivunen et al. [2003] go one step further and employ annotations as an extension of bookmarks to improve cooperation among users: the additional data provided by annotations are exploited to describe, organize, categorize, share, and search for the bookmarks. In the SCHOLNET7 DLMS, Constantopoulos et al. [2004] use annotations as metadata to support communication and interaction within scholarly communities and employ the SIS-Telos8 knowledge representation language to express them. They introduce a semantic annotation model, where annotations are treated as documents themselves, the semantics of which is captured by a controlled vocabulary of annotation types. Furthermore, annotations can be translated into records compliant with a subset of Dublin Core Metadata Initiative (DCMI) 9 specification to improve interoperability. Constantopoulos et al. [2004] developed a service for the SCHOLNET system, which offers storage, retrieval and deletion of annotations. In a similar context, Imaginum Patavinae Scientiae Archivum (IPSA) [Agosti et al. 2003, 2005a, 2006c] supports the annotation and personalization of image digital archives. The final goal is to provide end users with tools for performing scientific research on images taken from illuminated manuscripts [Canova 1988]. One of the most important aims of the research on illuminated manuscripts is the unveiling of hidden connections among illustrations belonging to different manuscripts. The use of annotations has been proposed as a useful way of accessing a digital archive and sharing knowledge in a cooperative environment. In IPSA, annotations are links that connect one image to another image related to it because illustrations were copied from images in other manuscripts, or they were merely inspired by previous works, or they were directly inspired by nature. IPSA utilizes annotations as metadata because they are drawn from a link taxonomy, which comprises two broad classes. The first deals with hierarchical relationships between two images, where one image somehow depends on an earlier one. The second concerns relatedness relationships between two images, where they both share similar properties even though they were created independently. Both classes contain more specialized annotation/link types. IPSA manages and stores annotations in the same archive where the illuminated manuscripts are stored, even if annotations are separated from the annotated digital objects and they do not modify them. Finally, Rigaux and Spyratos [2004] and Gueye et al. [2004] propose a data model for the composition and metadata management of documents in a distributed setting, such as a DLMS. The model enables the creation of composite documents, which are made up of either composite documents, or atomic documents, which can be any piece of uniquely identifiable material. A set of annotations is associated to each composite document, where they interpret annotations as terms taken from a controlled vocabulary or taxonomy to which all authors adhere. The model provides algorithms to automatically compute the annotations of composite documents, starting from the annotations of its composing atomic documents, by means of a subsumption relation defined within the taxonomy mentioned previously. Annotations are also used in the context of DataBase Management Systems (DBMS) and, in particular, in the case of curated databases and scientific databases. SWISS-PROT10 is a curated protein sequence database, which strives to provide a high level of annotation, such as the description of the function of a protein, its domain structure, and so on. In this case, the annotations are embedded in the database and merged with the annotated content. BIODAS11 provides a distributed annotation system, which is a system based on Web servers for sharing lists of annotations across a certain segment of the genome. In this case, annotations are not mixed together with the content they annotate, they are instead separated from it. Annotations have types, methods, and categories. The annotation type is selected from a list of types that have biological significance; the annotation method is intended to describe how the annotated feature was discovered and may include a reference to a program; the annotation category is a broad functional category that can be used to filter, group, and sort annotations [Stein et al. 2002]. Buneman et al. [2001, 2002] investigate the use of annotations with respect to the data provenance problem, sometimes also referred to as data lineage or data pedigree. Data provenance, which is the description of the origins of a piece of data and the process by which it arrived in a database, is undoubtedly an open and challenging research issue in the field of DBMS, As Abiteboul et al. [2005] point out. Buneman et al. [2001] distinguish between why-provenance, which explains why a given piece of data is in the database, and where-provenance, which explains where a given piece of data comes from. The distinguishing feature of scientific databases is that data needs to be tracked, for example, to know which instruments were used to gather the data, and what their settings were. Moreover, scientists and experts continuously correct and annotate the original source data and this, too, is a way of carrying out their research work. Bhagwat et al. [2004] carry on the research into provenance and propose an extension to a relational DBMS and to Structured Query Language (SQL) called propagate SQL, which provides a clause for propagating annotations to tuples through queries. They see annotations as information about data such as provenance, comments, or other types of metadata; they envisage the following applications of annotations in DBMS: tracing the provenance and flow of data, reporting errors or remarks about a piece of data, and describing the quality or the security level of a piece of data. As a final example of the use of annotations as metadata, once more in the context of scientific databases, Buneman et al. [2004] propose an archiving technique for managing and archiving different versions of scientific databases over time. They exploit the hierarchical structure of scientific data to represent the content and the different versions of the database with a tree structure. They attach annotations to the nodes of the tree, annotations that contain time-stamp and key information about the underlying data structure. Therefore, these annotations are metadata about the annotations contained in the database itself. In a sense, we could say that these annotations are meta-metadata; as we can see, they differ from the annotations contained in the database, in that they are metadata about the modifications to the contents of the database over time, while the latter are metadata about genome sequences. On the whole, this annotated tree structure provides an additional data layer that enables the development of efficient algorithms for archiving and searching for the different versions of the database. 2.2 Annotations as Content Annotations are regarded as additional content that relates to an existing content, meaning that they increase the existing content by providing an additional layer of elucidation and explanation. However, this elucidation does not happen, as in the case of annotations as metadata, by means of some kind of constrained or formal description of the semantics of the annotated object. On the contrary, the explanation itself takes the shape of an additional content that can help people understand the annotated content. However, the semantics of the additional content may be no more explicit for a computing device than the semantics of the annotated content. This view of annotations is comparable to the activity of reading a document and adding notes to it: explanation and clarification of words or passages of the document by expounding on it, providing a commentary on it, and finally completing it with personal observations and ideas. Therefore, the final recipients of this kind of annotation are people; because a content annotation does not make the annotated object more readily processable by a computer than the same object without annotations. In fact, from the point of view of a computer, the semantics of content annotations needs to be in some way processed, for example, indexed, before it can be used to deal with the semantics of the annotated object; this is quite different from the case of metadata annotations, which are pieces of information ready to be used for interpreting the semantics of the annotated object. In contrast, the additional semantics provided by content annotations can offer people useful interpretations and comments for the annotated object, making it easier to understand its hidden facets. This view of annotations entails an intrinsic dualism between annotation as content enrichment and annotation as stand-alone document: the former considers annotations as mere additional content regarding an existing document and, as a result, they are not autonomous entities, but in fact rely on previously existing information resources to justify their existence; the latter regards annotations as real documents and autonomous entities that maintain some sort of connection with an existing document. This twofold nature of the annotation can be made clearer by considering how we study a document. First of all, we can annotate some passages that require a further looking into; we can consider this as a sort of "annotation as content enrichment." Second, we can reconsider and collect our annotations and we can use them as a starting point for a new document; this is an example of "annotation as a stand-alone document." In this case, the annotation process can be seen as an informal, unstructured elaboration that could lead to a rethinking of the annotated document and to the creation of a new one. Note that both kinds of content annotation are valuable and the boundaries between them may fade into one another with the passing of time; as a consequence, both kinds of content annotations need to be considered as first-class digital objects. NoteCards [Halasz et al. 1987; Halasz 1988] influenced the successive research in the field of hypermedia/hypertext systems. NoteCards is a hypermedia system designed for helping people to work with ideas: authors, researchers, and intellectual work practitioners can analyze information, construct models, formulate topics, and elaborate ideas by using a network of electronic notecards interconnected by typed links. One of the famous "seven issues" mentioned by Halasz [1988] concerns support for collaborative work: he highlighted how annotations are part of the "activities that form the basis of any collaboration effort" [Halasz 1988, p. 848]. Moving forward in the context of the Web, the CoNote [Davis and Huttenlocher 1995] is a cooperative system for supporting communications within groups of users by using shared annotations on a set of documents. CoNote offers plain text or HyperText Markup Language (HTML) [W3C 1999] annotations on Web pages and pays particular attention in structuring annotations on the same part of a document as a tree, in order to ease the discussion among the users by supporting replies to previously inserted annotations. CoNote stores annotations separately from the original Web page and displays them on request by using a standard browser. A similar approach is also adopted by the WebAnn system [Brush et al. 2002], which is aimed at supporting interaction between students and instructors by using annotations; with respect to CoNote it offers users the possibility of more finely tuned annotations. A recent example of this kind of annotation system in the Web is Multimedia Annotation of Digital Content Over the Web (MADCOW) [Bottoni et al. 2004, 2005b]. This system enables multimedia annotation on Web pages and is based on a client-server architecture. Servers are repositories of annotations to which different clients can connect, while the client is a plug-in for a standard Web browser. MADCOW uses HyperText Transfer Protocol (HTTP) [Fielding et al. 1999] as the communication protocol between the annotation servers and the browser plugin; moreover, it assumes that pages are written in HTML in order to annotate Web pages. It supports both private and public annotations and allows different pre-established types of annotations, such as explanation, comment, question, solution, summary, and so on, which are defined according to the Rhetorical Structure Theory (RST) [Mann and Thompson 1987]. Note that once annotations have been created, they are assigned a Uniform Resource Locator (URL) [Berners-Lee 1994a] and are treated as any other HTML document; therefore, annotations can be made on other annotations. Annotations as content also find a natural application in the context of a DL. A DL is not only the digital version of traditional libraries and archives, but also offers instruments and services that can go beyond the mere presentation of content stored in digital repositories [Lesk 2005; Witten and Bainbridge 2003; Candela et al. 2006]: annotations can be considered one such instrument. Moreover, as introduced in Section 1, both DLs and their management systems, the DLMS, can greatly benefit from annotations for actively involving users and promoting interaction among them. Ioannidis et al. [2005] also agree with this point when they state that DLMS should provide the means for creating annotations, and should support the storage, selective sharing, and configurable presentation of annotations. Different layers of annotations can coexist for the same document: a private layer of annotations accessible only by authors of the annotations, a collective layer of annotations, shared by a team of people, and finally a public layer of annotations, accessible to all the users of the digital library. In this way, user communities can benefit from different views of the information resources managed by the DL [Marshall 1997; Marshall and Brush 2002, 2004]. A DL can encourage cooperative work practices, enabling the sharing of documents and annotations, also with the aid of special devices, such as XLibris [Schilit et al. 1998]. Finally, as suggested in [Marshall et al. 2001b; Marshall and Ruotolo 2002], searching, reading and annotating the information resources of a DL can be done together with other activities: for example, working with colleagues. This may also occur in a mobile context, where merging content and wireless communications can foster ubiquitous access to DLMS, improving well established cooperative practices of work, and exploiting physical and digital resources. The wireless context and the small form factor of handheld devices challenge our technical horizons for information management and access. Specialized solutions are required to overcome the constraints imposed by such kinds of devices, as reported in Agosti and Ferro [2003b]. An example of this use of annotations in DLMS, is Collaboratory for Annotation Indexing and Retrieval of Digitized Historical Archive Material (COLLATE) [Frommholz et al. 2003; Thiel et al. 2004], which supports the collaboration among film scientists and archivists who are annotating historical film documentation dealing with digitized versions of documents about European films from the 1920s and 1930s. Such documents are censorship documents, newspaper articles, posters, advertisement material, registration cards, and photos. Annotations support user communities in accessing the information resources provided by the DL in a personalized and customized way: they are dialog acts, part of a discourse about film documentation, and constitute the document context, intended as the context of the collaborative discourse in which the document is placed. This collaborative discourse is carried out by allowing annotations to annotate other annotations. Note that COLLATE offers different predefined types of dialog acts, such as elaboration, comparison, argumentation, counterargument, and so on; in this respect, COLLATE opts for a solution similar to the one adopted by MADCOW. Flexible Annotation Service Tool (FAST) FAST [Agosti and Ferro 2003a, 2004, 2005b, 2005a, 2006; Ferro 2004, 2005] is a flexible system designed to support two different things: various architectural paradigms, such as Peer-ToPeer (P2P) or Web Services (WS) architectures; a wide range of different DLMS. The flexibility of FAST and its independence from any particular DLMS is a key feature for providing users with a uniform means of interaction with annotation functionalities, without the need for changing their annotative practices only because a user works with different DLMS. FAST supports both users and groups of users with different access permission on annotations and offers three different scopes for the annotations: private, shared, and public. Furthermore, annotations in FAST allow users to merge their personal content with the information resources managed by diverse DLMS: annotations can span and cross the boundaries of a single DLMS, annotating digital objects that are part of different DLs, if users so desire. Finally, this use of annotations gives the users the possibility of linking digital objects that otherwise would have remained separated because they were managed by different DLMS. Ioannidis et al. [2005] recently noted this as an advantage for users and a challenge for the next generation DLMS. FAST also constitutes the underlying infrastructure of the Digital Library Annotation Service (DiLAS) project [Agosti et al. 2005c, 2006d, 2006b], which is an ongoing project in the framework of DELOS, the European Network of Excellence on Digital Libraries. The goal of DiLAS is to design and develop a generic annotation service that can be easily used in different DLMS; DiLAS aims at defining a set of application program interfaces to enable both access to this service from different DLMS, and the creation of different annotation clients and user interfaces embedded in various DLMS. The annotation service will be evaluated as a new way of interacting with a DL and cooperating among DL users and stakeholders. With respect to this last issue, the DiLAS project defines the overall aim of a formative evaluation about how the present design of decentralized annotation services complies with the needs of the prospective users. The overall goal of the evaluation is to investigate the extent to which the annotation system complies with the characteristics, activities, tasks, and environments of users, in order to inspire future developments of annotation services and tools for DLs.

3. MODELING ANNOTATIONS 

3.1 Key points Table I summarizes the discussion introduced in Section 2 and presents systems along two dimensions. One is the degree of structure of the content and the other is the degree of structure of the annotation. The structure of the content can range from loosely structured documents, as in the case of the Web, to highly structured data, as in the case of a database. Similarly, the structure of the annotation can vary from unstructured or loosely structured annotations, as in the case of content annotations, to very structured annotations, as in the case of metadata annotations. Note that we have put the NoteCards system under the Web box, even though it was developed before the Web, because a hypermedia system is much more closer to the Web than to DBMS or DLMS. We can point out that across the different systems there is a very wide range of uses of annotations as a powerful tool for uncovering and clarifying the semantics of the annotated objects. The final recipients of annotations can be computing devices or people. The former is mainly the case of metadata annotations which allow annotated objects to be automatically processed, integrated and reused in different applications, even though these metadata annotations can be understandable and useful for people too. The latter is mainly the case of content annotations which elucidate and expound on an annotated object. Note that, also in this latter case, a computing device can become the recipient of such annotations, provided that some further step of processing is performed, for example, indexing. However, in both cases, the semantics of the annotation itself needs to be taken into consideration and modeled. This can happen formally and precisely by agreeing on metadata standards that describe how annotations should to be interpreted and used; alternatively, support can be provided for identifying different predefined annotation types, perhaps with varying levels of detail. The medium of the annotation can vary a lot: it can range from textual annotations, to image, audio, and video annotations; in a general setting, we may need to deal with multimedia rich annotations, composed of different parts, each with its own medium. All of these different kinds of media have to be considered and properly modeled, in a uniform way where possible. Both annotations and annotated objects need to be uniquely identified. Moreover, annotations comprise a temporal dimension that is often not explicit, but which limits the creation of the annotation to the existence of another object. This temporal relationship between the annotation and the annotated object does not mean that the annotation cannot be considered a stand-alone intellectual work, but it does impose a temporal ordering between the existence of an annotated object and the annotation annotating it, which cannot be overlooked. In addition, once we have identified both the annotation and the annotated object, we need to link and anchor the annotation to the part of the annotated object in question. This can happen in a way that mainly depends on the medium of the annotated object. On the whole, we need to model how annotations and annotated objects are uniquely identified and linked together, maybe with a varying degree of granularity in the anchoring, paying particular attention to the temporal dimension that regulates the relationships between annotations and annotated objects. As far as co operation is concerned, almost all of the analyzed systems show that annotations have great potential for supporting and improving interaction among users, and even among computing devices. Therefore, there is a need for modeling and offering different scopes of annotations, for example, private, shared, or public, and managing the access rights of various groups of users. Finally, a relevant aspect of annotations is that they can take the part of a hypertext [Agosti et al. 2004; Halasz 1988; Marshall 1998] since they enable the creation of new relationships among existing objects by means of links that connect annotations together with existing objects, as we will see later in more detail. The hypertext viewpoint about annotations is common to different systems, such as Annotea, MADCOW, and NoteCards in the hypermedia/Web context, or DiLAS, FAST, and IPSA in the DLMS context. Halasz [1988] points out that annotations are one of the activities that form the basis of any collaborative effort, and for which hypermedia systems are ideally suited, while Marshall [1998] considers annotations a natural way of creating and growing hypertexts that connect information resources by actively engaging users. In addition, the hypertext between annotations and annotated objects can be exploited not only for providing alternative navigation and browsing capabilities, but also for offering advanced search functionalities, able to retrieve more and better ranked objects in response to a user query and also by exploiting the annotations linked to them [Agosti and Ferro 2005b]. Moreover, DLMS usually offer some basic hypertext and browsing capabilities based on the available structured data, such as authors or references. On the other hand, DLMS do not normally provide users with advanced hypertext functionalities, where the information resources are linked on the basis of the semantics of their content and Hypertext Information Retrieval (HIR) functionalities are available, as in Agosti et al. [1991]. Therefore, annotations can turn out to be an effective way of associating this kind of hypertext to a DLMS to enable the active and dynamic use of information resources. In addition, this hypertext can span and cross the boundaries of the single DLMS, if users need to interact with the information resources managed by diverse DLMS [Agosti and Ferro 2004, 2005a]. This latter possibility is quite innovative, because it offers the means for interconnecting various DLMS in a personalized and meaningful way for the end-user, and, as Ioannidis et al. [2005] point out, this is a major challenge for the next generation DLMS. In conclusion, this hypertext has to be explicitly modeled and taken into consideration when dealing with annotations. 3.2 Modeling Approach The previous discussion clearly demonstrates that annotation is quite a complex concept comprising a number of different aspects. Therefore, when we attempt to model annotations, we have to work through this complexity, first to identify the main macro-areas of this concept, and second to provide clear definitions of the concepts within each macro-area and of the relationships among these concepts. Figure 1 provides both an overview of the areas covered and the detail of the definitions introduced within each area. The figure clearly shows how these areas correspond to the very basic issues that emerge when we think about annotations: we need to identify annotations and annotated objects in order to link them together, perhaps providing facilities for supporting cooperation, and we have to deal with both the actual contents of an annotation and the semantics expressed by those contents. Therefore, the objective of the formal model is to delimit the boundaries of each area and to clearly define the concepts contained in each area together with the relationships among them. We can then exploit these definitions to provide a comprehensive definition of annotation and to derive from this definition the notion of hypertext between annotated objects and annotations. To this end, we adopt a modeling approach based on set theory [Halmos 1974] and graph ´ theory [Bollobas 1998; Diestel 2000]. In the remainder of this section, we will briefly introduce the areas shown in Figure 1, which can be used as a map of the concepts dealt with in the formal model. Note that Figure 1 illustrates the main areas from bottom to top in the same order in which they were discussed in Section 3.1, which represents the natural way in which they occur when we reason about annotations. In contrast, the areas are presented below in the order in which they will be discussed in detail in Sections 4 to 8. This order derives from the need to introduce concepts and definitions in a linear fashion, thus avoiding back and forth references between them. Finally, Section 9 will define the annotation itself and Section 10 will discuss the hypertext between annotated objects and annotations, which can also be thought of as a kind of a concise view of the main relationships between annotations and annotated objects. Identification is the problem of uniquely identifying both the annotation and the annotated objects, highlighting the temporal constraints between them. This area is built around the concept of handle, which is defined as a unique identifier of both digital objects and annotations, and the proposed notation for dealing with the time dimension involved by annotations.

Cooperation is about annotations as a cooperation tool among users. It introduces the definitions of user and group of users, together with the associated concept of scope of annotation and access permission, which regulate the access policies for a given annotation. Linking deals with the allowed linking patterns between digital objects and annotations, and the problem of correctly anchoring annotations to digital objects. It defines the concepts of link type, which is defined as the allowed methods of linking annotations to annotated objects, stream, which abstracts the notion of content of a digital object, and segment, which represents a given portion of a stream, useful for anchoring an annotation to a digital object. Semantics concerns the meaning of the annotation and what it stands for, trying to make explicit the semantics of the different parts of the content of an annotation. It introduces the notions of meaning of annotation, which is part of the semantics of the whole annotation, and meanings graph, that is a graph allows for interoperability between the different meanings. Materialization deals with the way in which the semantics carried by an annotation can take shape, that is, the actual content of the annotation perceived by the user. It describes the sign of annotation, which is a particular type of stream representing part of the content of an annotation. The proposed formal model was first introduced in Ferro [2004] and a reduced version of it was adapted to the context of illuminated manuscripts in Agosti et al. [2006c]. 

4. IDENTIFICATION 

In order to uniquely identify both the annotation and the annotated objects, we need to proceed as follows: first, we need to define the objects we deal with, as described in Section 4.1; then, we also have to be able to deal with objects whose relationships are constrained by a temporal dimension, as explained in Section 4.2; finally, a suitable identification mechanism has to be provided, as introduced in Section 4.3. 4.1 Document, Annotation, and Digital Object Sets According to widely accepted terminology, we adopt the term digital object to refer to information resources managed by an Information Management System (IMS). Indeed, Paskin [2006, p. 6] defines the digital object as "a data structure whose principal components are digital material, or data, plus a unique identifier for this material." Goncalves et al. [2004a, p. 292] say that "information in ¸ digital libraries is manifest in terms of digital objects, which can contain textual or multimedia content (e.g., images, audio, video), and metadata," and they define a digital object as a tuple constituted by a unique handle, structured contents, and structural metadata [Goncalves et al. 2004a, p. 294]. Finally, Bottoni ¸ et al. [2003, p. 217] define the digital object as a typed tuple of attribute-value pairs with at least two mandatory attributes: a unique identifier, and the actual content of the digital object; furthermore, they consider annotations to be digital objects with specific attributes, that is, annotations are specialized digital objects. In the following, we need terminology to distinguish between two kinds of digital objects: the generic ones managed by the IMS, which we call documents, and the ones that are annotations. Therefore, when we use the generic term digital object, we mean a digital object that can be either a document or an annotation. Note that the term "document" is used here in a broad sense, since it indicates a generic multimedia, possibly compound, digital object; examples of such a broad use of the term document can be found in Castelli and Pagano [2002a], who define the document as a structured multilingual and multimedia information object, or in the Document Object Model (DOM [W3C 1998] where the term document indicates "many different kinds of information that may be stored in diverse systems, and much of this would traditionally be seen as data rather than as documents." Finally, note that when we talk about annotations we mean both metadata annotations, introduced in Section 2.1, and content annotations, explained in Section 2.2; therefore, we consider both of these types of annotations as kinds of digital objects. The following definition introduces the different sets of digital objects we will need to deal with. Definition 4.1. Let us define the following sets: -- D is a set of documents and d  D is a generic document. U D is a universe set of documents, which is the set of all the possible documents, so that D  U D . -- A is a set of annotations and a  A is a generic annotation. U A is a universe set of annotations, which is the set of all the possible annotations, so that A  U A . --DO = D  A is a set of digital objects and do  DO is either a document or an annotation. UDO = U A  U D is a universe set of digital objects, so that DO  UDO . 4.2 Expressing the Temporal Dimension Involved by Annotations The universe sets U D , U A , and UDO are abstract sets, since they contain all the possible needed objects, whether they actually exist or not in any given moment; on the other hand, the sets D, A, and DO are tangible sets that contain the objects that already exist in a given moment: if we pick out an element from D, A, or DO we are dealing with a digital object that has been created even before we start working on it; in other words, the element already exists. The D, A, and DO, sets are a sort of time-variant sets, since we can add, delete or modify elements of these sets over time. On the other hand, the U D , U A , and UDO sets are a sort of time-invariant sets, since they already contain every possibile object we may need to deal with. The annotation is the result of an intellectual task performed on an existing digital object and it follows an already existing digital object. Therefore, the annotation comprises a temporal dimension, which is often not explicit, but that limits the creation of the annotation to the existence of another digital object. This temporal relationship between the annotation and the annotated digital object does not mean that the annotation cannot be considered a stand-alone intellectual task, but it does impose a temporal ordering between the existence of an annotated digital object and its annotation that cannot be overlooked. In conclusion, we need some mechanism for rendering the time dimension explicit, if necessary. We will illustrate this mechanism by means of some examples, which show some interesting cases; please note that they do not aim to be exhaustive. Although, these examples do make use of the set DO, they have a more general validity. Creation of a new digital object consists of the following events. (1) We start with the set of digital objects at time k: DO (k). (2) We create a new digital object: we pick out an element from the universe set of digital objects that does not belong to DO (k): d o  DO (k)  UDO . (3) We end up with a new set of digital objects at time k + 1, which contains the newly created digital object: DO (k + 1) = (DO (k)  {do})  2UDO . Therefore, we have the following temporal ordering: both events 1 and 2 happen at time k, but at that time the newly created digital object does not yet belong to the set DO (k) of digital objects at time k; event 3 happens at time k + 1 and represents the new set of digital objects that now also contains the newly created and existing digital object. Deletion of an existing digital object consists of the following events: (1) We start with the set of digital objects at time k: DO (k). (2) We choose an existing digital object in the set of digital objects at time k: do  DO (k). (3) We end up with a new set of digital objects at time k + 1, which no longer contains the previously chosen digital object: DO (k + 1) = (DO (k) \ {do})  2UDO . Therefore, we have the following temporal ordering: both events 1 and 2 happen at time k; event 3 happens at time k + 1 and represents the new set of digital objects, which does not contain the previously existing digital object.

Modification of an existing digital object consists of the following events: -- We start with the set of digital objects at time k: DO (k). -- We choose an existing digital object in the set of digital objects at time k: do  DO (k). -- We choose a new digital object, which is the modified version of the previously chosen digital object and does not belong to DO (k): do  DO (k)  UDO . -- We end up with a new set of digital objects at time k + 1, which contains the modified version of the digital object. Therefore, we have the following temporal ordering: events 1, 2, and 3 happen at time k, but at that time the modified digital object does not yet belong to the set DO(k) of digital objects at time k; event 4 happens at time k + 1 and represents the new set of digital objects, which now contains the modified digital object. These three basic examples reveal our strategy for addressing the time dimension: -- Time k: identify an initial set DO (k) to work with. -- Time k: identify the digital objects to work with, which may belong to DO (k) or not. If the identified digital objects belong to DO (k), then they already exist; on the other hand, if the identified digital objects do not belong to DO (k), then they do not yet exist and therefore this step represents their creation. -- Time k +1: identify the new set DO (k +1), which results from performing the appropriate operations on the set and the digital objects previously identified. In all of the cases, both DO (k) and DO (k + 1) contain only digital objects that already exist: this mechanism allows us to unambiguously state which objects we are dealing with in any given moment and the moment when they are able to be utilized. DO (k) and DO (k + 1) unambiguously identify the digital objects we are dealing with, which are given by DO (k) DO(k + 1). In particular, the deleted digital objects are given by DO (k) \ DO(k + 1) and the newly created digital objects are given by DO (k + 1) \ DO(k). Therefore, we can talk about the digital objects identified by the transition from DO (k) to DO (k + 1). We assume that the operations previously shown are atomic: no operation can occur during the execution of another operation, so as to avoid concurrency issues. In conclusion, this mechanism provides us with a means to clearly identify which objects are involved in a given operation, when they can be utilized, and the ordering among the different events involved by an operation.

The definitions of collection C and repository provided by [Goncalves et al. ¸ 2004a, p. 295] to a certain extent resemble the proposed mechanism. Indeed, they model the storing of a digital object in the repository as the transition from the original repository to a repository which manages a collection augmented by the newly inserted digital object; similarly, the deletion of a digital object is modeled as the transition from the original repository to a repository which manages a smaller collection without the deleted digital object. On the other hand, the definitions introduced by Goncalves et al. [2004a] do not explicitly ¸ take into account and model the temporal dimension involved with these operations. In this sense, we can consider our mechanism an extension of the proposal made by Goncalves et al. [2004a]. ¸ Furthermore, our mechanism extends, formalizes, and makes more explicit what Rigaux and Spyratos [2004, p. 421] left implicit when they said: "in order to define a document formally, we assume the existence of a countably infinite set D whose elements are used by all authors for identifying the created documents . . . in fact, we assume that the creation of a document is tantamount to choosing a new element from D." Indeed, the set D used by Rigaux and Spyratos [2004] corresponds to the set UDO of Definition 4.1 and the creation of a document corresponds to the transition from DO (k) to DO (k + 1), where an object from UDO is chosen and is added to DO (k), thus creating DO (k + 1), as explained above. Furthermore, we provide a formal mechanism for describing the deletion and the modification of a digital object, as well. In the following sections, we will use the notation DO (k), which explicitly points out the time dimension, only when needed; otherwise we will use the simpler notation DO, without explicitly pointing out the time dimension. We will also use a similar notation for the other sets we will define in the following. 4.3 Handle According to the previous discussion, we can assume that each digital object is identified by a unique handle, which is a name assigned to a digital object to identify and to facilitate the referencing process to the digital object. Over the past years, various syntaxes, mechanisms, and systems have been developed to provide handles or identifiers for digital objects: -- Uniform Resource Identifier (URI) is a compact string of characters for identifying an abstract or physical resource [Berners-Lee 1994b; Kunze 1995; Berners-Lee et al. 1998; Mealling and Denenberg 2002]. The term URL refers to the subset of URIs that identify resources via a representation of their primary access mechanism (e.g., their network "location"), rather than identifying the resource by name or by some other attribute(s) of that resource. The term Uniform Resource Name (URN) refers to the subset of URIs that are required to remain globally unique and persistent even when the resource ceases to exist or becomes unavailable [Berners-Lee et al. 1998]. --Digital Object Identifier (DOI) is a system that provides a mechanism to interoperably identify and exchange intellectual property in the digital environment. DOI conforms to a URI and provides an extensible framework for managing intellectual content based on proven standards of digital object architecture and intellectual property management. Furthermore, it is an open system based on non-proprietary standards [Paskin 2006]. -- OpenURL aims at standardizing the construction of "packages of information" and the methods by which they may be transported over networks [NISO 2005]. Therefore, OpenURL is a standard syntax for transporting information (metadata and identifiers) about one or multiple resources within URLs, i.e. it provides a syntax for encoding metadata and identifiers, limited to the world of URLs [Paskin 2006]. -- Persistent URL (PURL)12 : instead of pointing directly to the location of an Internet resource, a PURL points to an intermediate resolution service that associates the PURL with the actual URL and returns that URL to the client as a standard HTTP redirect. The client can then complete the URL transaction in the normal fashion. -- PURL-based Object Identifier (POI)13 is a simple specification for resource identifiers based on the PURL system and closely related to the use of the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) defined by the Open Archives Initiative (OAI)14 [OAI 2004]. The POI is a relatively persistent identifier for resources that are described by metadata "items" in OAI-compliant repositories. The following definition introduces the notion of handle, compatible with the mechanisms described above, and its relationship with digital objects. Definition 4.2. H is a set of handles such that |H| = |DO| and h  H is a generic handle. UH is a universe set of handles, which is the set of all the possible handles, such that |U H | = |UDO |; it follows that H  U H . We define a bijective function h : U H  UDO , which maps a handle to the digital object identified by it.

The relationship between the sets H and U H is the same as the one between the sets DO and UDO , described in section 4.2. 

5. COOPERATION 

In order to provide users with annotations as an effective cooperation tool, we need to proceed as follows: first, we need to define the notion of user, group of users, and author, as described in Section 5.1; then, we have to deal with both scopes of annotation, as explained in Section 5.3, and various access permissions, as introduced in Section 5.2. The following definitions do not aim at building a new authentication and authorization scheme from scratch, since many such schemes already exist.

On the contrary, since a formal model needs to be self-contained and coherent, they aim at providing users of this model with a basic infrastructure for plugging into their preferred authentication and authorization scheme, perhaps extending this model with the additional concepts peculiar to the chosen scheme. 5.1 User, Group of Users and Author Definition 5.1. Let USR be a set of users and usr  USR is a generic user; UU S R is a universe set of users, which is the set of all the possible users, so that U S R  UU S R . G R  2U S R is a set of groups of users and G  G R is a generic group of users; UG R = 2UU S R is a universe set of groups of users, which is the set of all the possible groups of users, so that G R  UG R . We define a function gr : U S R  2G R , which maps a user to the groups of users he belongs to. The following constraint must be adhered to:  usr  USR, gr(usr) = ; each user in USR must belong to at least one group of users. The relationship between the sets USR and G R and the sets UU S R and UG R is the same as the one between the sets DO and UDO , described in Section 4.2. Note that the constraint on the gr function can be also expressed as  usr  U S R,  G  G R | usr  G. Obviously, a user may belong to more than one group of users, since G R is a subset of the power set of USR or, equivalently, since gr(usr) is an element of the power set of G R. Digital objects--both documents and annotations--always have at least one author who authored them. Therefore, the author is a specialization of the more general concept of user, introduced in the definition above: an author is a user who authored one or more digital objects. Definition 5.2. Let us define a function, au : USR  2 H , which maps a user to the handles of the digital objects authored by him. Let the set of authors AU be the following set: AU = {usr  USR | au(usr) = }; we denote with au  AU  USR a generic author. The following constraint must be adhered to:  h  H  au  AU | h  au(au); each digital object must be authored by at least one author. The function au characterizes the authors, distinguishing them from generic users; indeed, if a generic user usr  U S R has not authored any digital object, it follows that au(usr) =  and thus usr  AU . In general, a digital object may have more than one author; in other words, there may exist au1 , au2  AU | au(au1 )  au(au2 ) = . On the other hand, an author may author more than one digital object, since au(au) is an element of the power set of H. These definitions allow us to organize the users according to their roles and qualifications. Consider, for example, the case of a genomic annotation that relies on specially trained biologists whose job it is to create highly valuable and domain-specific annotations: these biologists may be users who belong to a group of curators, which grants them the necessary access rights for carrying out their curatorial role. Finally, note that these proposed definitions share similarities with the notion of society proposed by Goncalves et al. [2004a, p. 275], when they ¸ say "a society is a set of entities and the relationships between them." The above definitions simply introduce the "set of entities" that come into play plus the distinction between users and authors, which represents a first kind of "relationship" between those entities. Furthermore, the definitions in the following sections will enable the introduction of more "relationships" between users and groups, especially with respect to the access management issue, which is part of what Goncalves et al. [2004a, p. 276] call "societal ¸ governance". 5.2 Permission As discussed in Section 3, an annotation can have different access permissions: -- Denied, if no access is allowed. -- Read only, if it can be accessed only for reading. -- Read and write, if it can be read, modified, and deleted. Definition 5.3. Let P = { Denied, ReadOnly, ReadWrite} be a set of access permissions and p  P is an access permission. Let us define the following relations: -- equality relation = {( p, p)  P × P | p  P } = {( Denied, Denied), ( ReadOnly, ReadOnly), ( ReadWrite, ReadWrite)}. --strict ordering relation  {( Denied, ReadOnly), ( Denied, ReadWrite), ( ReadOnly, ReadWrite).} -- ordering relation {( p1 , p2 )  P × P | p1 = p2  p1  p2 }. In contrast to the set of the previous definitions, the set of access permissions P is a time-invariant set, which does not need the notation for taking into account the temporal dimension. Indeed, we assume that an annotation can only have the access permissions previously listed. Note that (P, ) is a totally ordered set.

5.3 Scope As discussed in Section 3, an annotation can have one of the following scopes, which are mutually exclusive: -- Private, when it can be accessed only by its own author. -- Shared, when it can be accessed only by a desired set of groups of users. --Public, when it can be accessed by all users. The different scopes of an annotation can be partnered with the various access permissions introduced here, in order to obtain the necessary access policies. For example, in the case of a public annotation we might decide that every user holds read only access to it, while its author has read- and write-access to it. In addition to the policy just described, we might also add a list of groups of users, each one with its specific access permission, in order to pinpoint finegrained access permission even in the context of a public annotation. Therefore, the proposed formal model does not aim at describing a specific access policy, but at providing the means for describing the different access policies one might need to deal with. Definition 5.4. Let SP = { Private, Shared, Public} be a set of scopes and sp  S P is a scope. Let us define the following relations: --Equality relation = {(sp, sp)  S P × S P | sp  S P } = {( Private, Private), ( Shared, Shared), ( Public, Public)}. -- Strict ordering relation  {( Private, Shared), ( Private, Public), ( Shared, Public).} -- Ordering relation {(sp1 , sp2 )  S P × S P | sp1 = sp2  sp1  sp2 }. As in the case of the set of access permissions, the set of scopes S P is also a time-invariant set, because we assume that an annotation can have only one of the three scopes listed above. Note that (S P, ) is a totally ordered set. 

6. LINKING 

In order to link annotations to digital objects and to correctly anchor annotations to digital objects, we need to proceed as follows: first, we need to choose a linking mechanism and define the link types that can exist between annotations and digital objects, as described in Section 6.1; then, since annotations are usually linked to specific parts of a digital object, we need to model the content of digital objects, as explained in Section 6.2; finally, a suitable anchoring mechanism for annotations has to be provided, as introduced in Section 6.3.

6.1 Linking Annotations to Digital Objects Handles can be used not only for the purpose of uniquely identifying a digital object, but they can also provide us with a means for linking an annotation to a digital object. This use of handles is particularly clear if we think about URLs, but it is also still valid in the case of the other types of handles presented in Section 4.3. Once we have decided to use handles as a basic mechanism for linking annotations to digital objects, we still have to consider the kind of links an annotation can have with a digital object. Annotations can be linked to digital objects with two main types of links: -- Annotate link: an annotation annotates a digital object, which can be a document or another annotation. The "annotate link" is intended to allow an annotation to only annotate one or more parts of a given digital object. Therefore, this kind of link lets the annotation express intra-digital object relationships, meaning that the annotation creates a relationship among the different parts of the annotated digital object; -- Relate-to link: an annotation relates to a digital object, which can be a document or another annotation. The "relate-to link" is intended to allow an annotation to only relate to one or more parts of other digital objects, but not the annotated one. Therefore, this kind of link lets the annotation express inter-digital object relationships, meaning that the annotation creates a relationship between the annotated digital object and the other digital objects related to it. With respect to these two main types of link, we introduce the following constraint: an annotation must annotate one and only one digital object, which can be either a document or another annotation--an annotation must have one and only one "annotate link." This constraint means that an annotation can be created only for the purpose of annotating a digital object and not exclusively for relating to a digital object. An annotation then, can annotate one and only one digital object, because the "annotate link" expresses intra-digital object relationships and thus it cannot be mutual to multiple digital objects different from the annotated one. Finally, this constraint does not prevent the annotation from relating to more than one digital object: from having more than one "relate-to link." This situation is very similar to what happens in the real world. When we deal with paper documents, we can annotate one or more parts of the document that we have at hand; this document also provides us with the physical medium for writing the content of the annotation. On the other hand, the content of the annotation can contain references to other documents; in other words, it can relate the document at hand to other documents that are currently being viewed. Therefore the act of annotating concerns one and only one document, to which the annotation is anchored, although there may be one or more references that relate the annotation to other documents. One could argue that in the digital world these limitations could be overcome and that an annotation could annotate multiple documents at the same time. Apart from being possible, what could we gain from this option? If we allow multiple annotate links, we are going to add some uncertainty because the annotation would lose its strong relationships with only one object. In fact, this object represents its main purpose, while linking the annotation to multiple objects would give us unclear semantics. Therefore, we opt for constraining the link types that an annotation can have, and the following definition introduces the set of allowed link types. Definition 6.1. Let LT be a set of link types; an element lt  LT corresponds to one of the allowed link types. The set LT contains the following link types: LT = { Annotate, RelateTo}. As in the case of the set of access permissions and the set of scopes, the set of link types LT is also a time-invariant set, because we assume that an annotation can be linked to digital objects only with the link types listed here. 6.2 Stream Digital objects can be very different--texts, images, audio, videos, hypertexts, multimedia objects, and so on--and the way in which their structure and content is modeled and expressed can also widely vary across different conceptual and logical models of DL and digital object. Nevertheless, many of these types of models share the idea that beyond representing the structure of the digital object, the model also has to take into account a mechanism for representing the actual content of the digital object. For example, Navarro and Baeza-Yates [1997] and Goncalves et al. [2004a] both use the notion of stream, which is an ¸ ordered sequence of symbols representing the actual content of a digital object or part of it. Bottoni et al. [2003] define the content of a digital object as a function from a set of indices to a set representing the vocabulary of the symbols. Finally, Castelli and Pagano [2002a, 2002b] associate each digital object with many different manifestation entities, which represent sequences of bytes and files containing different parts of the digital object itself. The following definition introduces the concept of stream in order to represent the actual content of a digital object or a part of it. The definition of stream is inspired by Navarro and Baeza-Yates [1997], and Goncalves ¸ et al. [2004a] but with some differences, which will be discussed in the following. Definition 6.2. A stream sm is a finite sequence: sm : I = 1, 2, . . . , n  , n  N, where is the alphabet of symbols. We allow the existence of an empty stream esm = . S M is a set of streams and sm  S M is a stream. U S M is a universe set of streams, that is, the set of all the possible streams. It follows that S M  US M .

We define a function hsm : H  2 S M , which maps a handle of a digital object to the streams contained in that digital object. The following constraint must be adhered to:  h  H, hsm(h) = . Each digital object must contain at least one stream, which could also possibly be the empty stream. The relationship between the sets S M and U S M is the same as the one between the sets DO and UDO , described in Section 4.2. The stream is required to be neither a surjective nor an injective function. We can exploit the non surjectivity of the stream in order to use standard sets-- characters, numbers, and so on--as a codomain for a stream; otherwise, if the function were constrained to be surjective, we would be forced to use an "ad hoc" codomain for each different stream. On the other hand, since the stream is not an injective function, it is therefore not invertible: in fact, when given a symbol, we cannot trace this symbol back to its position within the stream. For example, if we consider the following piece of text: 1 2 3 4 5 6 7 8 9 10 Title then we can define the stream sm : I = {1, 2, . . . , 10}  = {A, B, . . . , Z, a, b, . . . , z, }, such that sm(1) = T, sm(2) = i, . . . , sm(10) = t. Note that if the stream was constrained to be surjective, we should use a codomain constituted only by = T, i, t, l, e, , x . In any case, the letters of the piece of text shown above: from a given symbol, for example "t," we cannot unambiguously determine its position in the stream, because the stream is not injective--"t" is given by both sm(3) and sm(10). In particular, we can distinguish two main kinds of streams: -- Logical stream lsm: this is a stream in which each element   represents a logical symbol within the stream. -- Physical stream psm: this is a stream in which each element   represents a physical symbol within the stream. Now we will discuss the distinction between logical and physical streams by means of an example. Although the map shown between natural numbers and letters is quite intuitive, it should be pointed out that the elements of the set are symbols that abstract the underlying encoding of the text. For example, if we consider an ASCII text, each element of the set corresponds to exactly one byte in the physical text stream; thus, we should use the codomain
ASCII Text = {4116 , 4216 , . . . , 5A16 , 6116 , 6216 , . . . , 7A16 , 2016 } nstead of to represent the actual stream. On the other hand, in the case of a UNICODE text, each element of the set corresponds to two bytes in the physical text stream; thus, we should use the codomain: UNICODE = {0016 4116 , 0016 4216 , . . . , 0016 5A16 , 0016 6116 , 0016 6216 , . . . , 0016 7A16 , 0016 2016 } instead of . In this latter case we are forced to define the elements of UNICODE as two-byte pairs in order to map the indices of I into the symbols of UNICODE ; as a result, we would not be able to access each byte individually. If we wish to access each byte of the UNICODE stream, we should define the following domain I = {1, 2, . . . , 20} and codomain UNICODE = {0016 , 4116 , 4216 , . . . , 5A16 , 6116 , 6216 , . . . , 7A16 , 2016 } for the stream; but in this case we would lose the correspondence with the ten letters of the piece of text, because two indices in I would correspond to each letter of the piece of text. This example demonstrates that on the one hand we have a logical stream, which represents the piece of text, and on the other there are one or more physical streams that represent the physical encoding of the piece of text shown. Similar, and even more complex, considerations can be made in the case of audio, image, and video streams, where the complexity of such streams increases the choices available for representing them both in logical and in physical terms. Another example is the compression of streams, where more symbols in one stream correspond to fewer symbols in the other stream. This observation points out the need to carefully define the level of abstraction of a stream and the degree of detail that need to be adopted when defining streams. In other words, should we model the physical encoding of a stream, or some more abstract representation of that stream? Depending on the case, both levels of abstraction may be necessary: for example, when we do a macrocomparison of two digital libraries, we can use more abstract streams; on the other hand, if we want to precisely describe the functioning of some component of a digital library, as a repository, we need to use streams that better represent the physical encoding of the objects in the repository. However, past experience in the field of DBMS teaches us that it is better to keep the logical and the physical levels distinct. This is why we want to distinguish between logical and physical streams. Note that Navarro and Baeza-Yates [1997] make use of logical streams, but they do not specify much about physical streams, leaving them to the implementation of the system. Neither Bottoni et al. [2003] nor Goncalves et al. [2004a] ¸ addressed this problem at all, but in Goncalves and Fox [2002] it turns out ¸ that streams are essentially identified by Multipurpose Internet Mail Extensions (MIME) types [Freed and Borenstein 1996a, 1996a, 1996b; Moore 1996; Freed et al. 1996] and thus, they are substantially physical streams. Finally, since the notion of manifestation used by Castelli and Pagano [2002b, 2002a] refers to a physical file holding part of the content of a digital object, these authors essentially use physical streams also, and in a way that resembles the implementation of streams by Goncalves and Fox [2002]. ¸ We believe that the research field of digital libraries also needs to clearly distinguish between the logical and physical levels; this distinction is a prerequisite for each formal model of DL that aims to be sufficiently clear, expressive and flexible. Moreover, an explicit and formal mechanism for modeling the relationship between logical and physical streams, and the properties of such a relationship is needed. This will be the focus of the next definition. Definition 6.3. Given two streams sm1 , sm2  S M , let us define a stream mapping relation: SMR (sm1 , sm2 ) = {(i, j )  Ism1 × Ism2 | sm1 (i) is mapped to sm2 ( j )}. Let us define a stream mapping set SMS = {SMR (sm1 , sm2 )i } such that: (1)  sm  S M ,  SMR  SMS | SMR (sm, sm) = {(i, j )  Ism × Ism | i = j }. (2)  SMR (sm1 , sm2 )  SMS,  SMR (sm2 , sm1 )  SMS | SMR (sm2 , sm1 ) = SMR-1 (sm1 , sm2 ) = {( j, i)  Ism2 × Ism1 | (i, j )  SMR (sm1 , sm2 )}. (3)  SMR (sm1 , sm2 ) , SMR (sm2 , sm3 )  SMS, SMR (sm1 , sm3 )  SMS | SMR (sm1 , sm3 ) = SMR (sm1 , sm2 )  SMR (sm2 , sm3 ) = {(i, k)  Ism1 × Ism3 | (i, j )  SMR (sm1 , sm2 )  ( j, k)  SMR (sm2 , sm3 )}. Let us define a stream mapping set indicator function: SMS (sm1 , sm2 ) = 1 if SMR (sm1 , sm2 )  SMS 0 if SMR (sm1 , sm2 )  SMS.

Each element (i, j )  SMR (sm1 , sm2 ) represents the fact that the i-th symbol in the first stream is related to the j -th symbol in the second stream. The SMR relation represents and embeds the algorithm that allows us to map symbols of one stream into those of the other. In particular, in the case of the relationship between logical and physical streams, the SMR relation represents the fact that given logical symbols are encoded with given physical symbols. In this way, it clearly models the distinction and the passage from the logical to the physical level. For example, the SMR relation could represent the mapping between the pixels of an image an its Joint Photographic Experts Group (JPEG) encoding. In general, the stream mapping relation allows us to express many-to-many relationships between symbols of two streams. In particular, we are interested in expressing, at least, the following relationships: -- A one-to-one relationship between the symbols of the two streams, as in the previous example of the piece of text, and its ASCII encoding. -- A one-to-many relationship between the symbols of the two streams, as in the previous example of the piece of text and its UNICODE encoding. -- A many-to-one relationship between the symbols of the two streams, as in the case of compression of one stream into another. The stream mapping relation provides us with a further degree of freedom, since we can have symbols in a stream that do not correspond to another stream. In this way, we can model some kind of loss of information due to different encodings.

Finally, the stream mapping relation enables the same logical symbol to be encoded in different ways according to its position in the stream. Consider the letter "t," which in the previous example appears in the third and tenth position of the stream; it could be encoded in two different ways if we apply some compression algorithm to that stream. Definition 6.3 allows us to associate a set of physical streams to the same logical stream, providing us with a mechanism to enable different encodings of the same logical stream. We could also create a chain of streams: we could specify that a logical stream is encoded with a given physical stream, and that this physical stream is mapped to another physical stream and so on. These observations led us to introduce the stream mapping set, which contains the stream mapping relations and holds the intuitive and expected properties for this kind of set: -- Reflexive: for each stream, the obvious mapping of the stream to itself exists. -- Symmetric: if we know how to map one stream to another, we can also map the second stream back to the first one. -- Transitive: if we know the mapping between one stream and another and we also know the mapping between the second stream and a third, we know how to map the first to the third. Now we can study the impact Definition 6.3 has on the set of streams S M and how it contributes to enforcing the distinction between the logical and the physical levels. PROPOSITION 6.4. The following relation: SMS = {(sm1 , sm2 )  S M × S M |  S M S (sm1 , sm2 ) = 1} is an equivalence relation on the set of streams S M . The sets: SM = {sm1 , sm2  S M | (sm1 , sm2 )  SMS} are the equivalence classes of all the streams of S M that are mapped one to another, and S M/SM is the quotient set. PROOF. The relation is: -- Reflexive:  sm  S M ,  SMR (sm, sm)  S M S   S M S (sm, sm) = 1. -- Symmetric:  sm1 , sm2  S M |  S M S (sm1 , sm2 ) = 1,  SMR (sm2 , sm1 )  S M S   S M S (sm2 , sm1 ) = 1. -- Transitive:  sm1 , sm2 , sm3  S M |  S M S (sm1 , sm2 ) =  S M S (sm2 , sm3 ) = 1,  SMR (sm1 , sm3 )  S M S   S M S (sm1 , sm3 ) = 1. Therefore, it is an equivalence relation. We can choose the logical stream as representative of the equivalence class; the SMS equivalence relation allows us to deal only with logical streams. In fact, it removes us from the physical level and hides the details of the representation and the encoding of logical streams into physical ones. Therefore, the SMS equivalence relation enforces the distinction between the logical and the physical levels and provides us with the means of working and reasoning at a logical level, clearly separating it from the physical one. Furthermore, we can iterate this line of reasoning and use this equivalence relation as a basic mechanism for introducing further levels of abstraction and creating a kind of hierarchy among streams. Indeed, on the quotient set SM/SM we could introduce an equivalence relation similar to SMS in order to express the fact that two or more logical streams can be mapped to each other. This is how we can maintain more abstract classes of equivalent logical streams on the quotient set SM/SM, by keeping them distinct from the different ways in which they can be encoded; this different encoding is, in turn, represented by the less abstract equivalence classes on the set S M . This procedure can be repeated as many times as needed in relation to the number of levels of abstraction. For example, suppose we have a piece of text that can be represented either as a sequence of characters or as a scanned image. These are two different logical streams, that can be encoded with many different physical streams. In this case, a first level of abstraction is to put all the physical streams that encode the character streams into one equivalence class created on SM, and all the physical streams that encode the image stream into another equivalence class created on SM. However, a higher level of abstraction is to put both the equivalence class of the text stream and the equivalence class of the image stream into a new and more abstract equivalence class created on S M/SM, in order to express the fact that both of them are representations of the same piece of text. For all these reasons, Definition 6.3 and Proposition 6.4 constitute a step forward with respect to previous models [Castelli and Pagano [2002a, 2002b] Goncalves et al. 2004a, 2004b; Navarro and Baeza-Yates 1997; Bottoni et al. ¸ 2003], which only partially address this issue or do not address it at all. On the other hand, Definitions 6.2, 6.3, and Proposition 6.4 are fully compatible with the definition of stream provided by both Navarro and Baeza-Yates [1997] and Goncalves et al. [2004a]; thus, we can utilize the proposed distinction between ¸ logical and physical streams in both the models provided by Navarro and BaezaYates [1997] and Goncalves et al. [2004a] in order to extend such models, if ¸ necessary. 6.3 Segment The handles discussed in Section 4.3 may be capable not only of uniquely identifying a digital object, but also of indicating a part of the identified digital object. For example, a URL can point to any given anchor within a HTML document, or we can use an XPath expression to point to a specific element within an XML document. On the other hand, parts of a digital object cannot always be identified with an arbitrary degree of detail; for example, a URL cannot point to a given word of a HTML document, if this word is not marked with an anchor. Therefore, we need some further mechanism for identifying parts of a digital object with the necessary degree of detail. The following definition introduces the notion of segment, which is a mechanism for selecting parts of a stream; this mechanism can be partnered with the handle of a digital object to provide access to a digital object with the necessary degree of detail. Definition 6.5. Given a stream sm : I = 1, 2, . . . , n  S M , a segment is a pair: stsm = (a, b) | 1  a  b  n, a, b  N. , n  N, sm 

A stream segment is a restriction, sm|[a,b] , of the stream sm to interval [a, b] associated with the segment stsm . ST is a set of segments and stsm  ST is a generic segment; U ST is a universe set of segments, which is the set of all the possible segments, so that ST  U ST . The relationship between the sets ST and U ST is the same as the relationship between the sets DO and UDO , described in Section 4.2. Definition 6.5 resembles the definition of segment provided in Navarro and Baeza-Yates [1997], and Goncalves et al. [2004a]. ¸ We can assume that logically related symbols of logical streams are contiguous and are in an ascending order. This assumption goes well with Definition 6.5, which selects a series of contiguous symbols. On the other hand, Definition 6.3 allows us to disregard this constraint for the mapping to physical streams, since the stream mapping relation SMR allows us to map the contiguous symbols of the logical stream to noncontiguous symbols of the physical stream. For example, the indices Ilsm = {1, 2, 3, 4, 5} of a logical stream could be mapped to the indices Ipsm = {13, 7, 19, 9, 15} of a physical stream. If we choose the segment stlsm = (2, 4), which is associated with the interval [2, 4] for the logical stream, we are not forced to map it to the interval [7, 9], obtained by mapping the segment stlsm to a corresponding segment stpsm = (7, 9), of the physical stream. On the contrary, we can map each index in the interval [2, 4] to its corresponding index in the physical stream, obtaining the set of indices 7, 19, 9 , which do not fit in the interval [7, 9]. See Navarro and Baeza-Yates [1997] for further explanation about ordering in multimedia streams. This feature is important because symbols that are contiguous in a logical stream can correspond to non-contiguous symbols in a physical stream, due to some kind of compression, for example. In addition, Proposition 6.4 allows us to reason only in terms of logical streams that comply with the assumption made above, without worrying about the physical streams that are in the same equivalence class of the logical stream. This observation further highlights the benefits that may arise by clearly distinguishing between the logical and the physical levels. All of the introduced concepts, namely handle, stream, and segment, provide us with the formal means needed to deal with the linking and anchoring problem related to annotations. By using a handle h we can link an annotation to a digital object; then, the function hsm(h) allows us to select the desired stream sm of the digital object identified by h, be it a physical or a logical view of the actual content of the digital object; finally, a segment stsm enables the fine-tuned anchoring of the annotation to the digital object. Last, we can also rely on these concepts to address the annotation repositioning problem that arises when the content of annotated digital objects changes.

Indeed, the introduced concepts offer us the possibility of modeling what Phelps and Wilensky [2000b] call robust location, which are redundant descriptors of locations within a digital object created by using a number of different data records. Moreover, we can also express the algorithms they propose for reattaching annotations to a digital object when the annotated digital object is modified. Furthermore, these concepts can provide us with a common grounding not only for designing repositioning algorithms, as in the case of Phelps and Wilensky [2000b], but also for studying and describing what users expect an annotation system to do when annotated digital objects change, as done in the user study conducted by Brush et al. [2001]. 

7. MATERIALIZATION 

As in Agosti and Ferro [2003a], we define sign of annotation as the basic way in which an annotation can take shape, the way of representing and materializing the semantics of annotation. For example, we can identify the following basic signs of annotations: -- Textual sign: a textual materialization of the semantics of an annotation, which is expressed by a piece of text added to a digital object. --Graphic sign: the graphic materialization of the semantics of an annotation, which is expressed by a graphic mark added to a digital object. --Video sign: the video materialization of the semantics of an annotation, which is expressed by a video fragment added to a digital object. --Auditive sign: the auditive materialization of the semantics of an annotation, which is expressed by an audio fragment added to a digital object. These basic signs can be combined to express more complex signs of annotation. Consider the example of Figure 2, where two annotations are shown, one in the upper part near the auditive sign bullet, and the other in the lower part near the auditive sign bullet. The first annotation is constituted by both a basic sign and a compound sign. The highlight is a basic graphic sign, while the call-out is a compound sign. It is in turn formed by two graphic signs, the box and the arrow, and by a textual sign, which is the question "Wouldn't also it be useful for visually impaired people?". The second annotation is made by three basic signs: two graphic signs, the arrow and the cloud, and a textual sign, which is the answer "I think so!". In conclusion, by using the notion of sign of annotation, we consider the annotation as possibly complex multimedia, constituted by different parts, each one with its own medium. The following definition formally introduces the concept of sign of annotation. Definition 7.1. A sign of annotation is a stream. S N  S M is a set of signs of annotation and sn  S N is a sign. U S N  U S M is a universe set of signs of annotation, which is the set of all the possible signs of annotation, so that S N  U S N . The relationship between the sets S N and U S N is the same as the relationship between the sets DO and UDO , described in Section 4.2.

Henceforth we will use the term sign of annotation, or briefly stated as sign, to indicate a stream that belongs to an annotation. On the other hand, we will use the term stream to indicate a stream that belongs to a digital object without the need of specifying if the digital object is a document or an annotation. 

8. SEMANTICS 

As in Agosti and Ferro [2003a], we define meaning of annotation as a main feature of the concept of annotation, which identifies conceptual differences within the semantics of the annotation or part of it. For example, looking at the different points of view concerning annotations introduced in Section 2, we can see that they correspond to different and very broad meanings of annotation. Furthermore, we can identify different meanings of annotation within each given viewpoint: for example, within the viewpoint called "annotation as content" we can identify at least the following meanings of annotation, but many others would be possible, also depending on specific domains: -- Comprehension and study: annotating a document is a way of better investigating and understanding a concept. This process principally involves a private scope, because the recipient of an annotation is the person who created it. Other people reading an annotated document may benefit from existing annotations as well. -- Interpretation and elucidation: annotating a document could be a way of adding comments and explaining sentences within it. The aim is to make it more comprehensible and to exchange ideas on a topic; an example could be an expert in literature who explains and annotates the Divine Comedy. This process principally involves a public scope, and the recipients of an annotation are people who are not necessarily related to the creator of the annotation. -- Cooperation and revision: a team of people could annotate a document for various purposes, as they are working on a common document or they are reviewing someone else's work; annotating a text is thus a way of sharing ideas and opinions in order to improve a text. This process principally involves a shared scope, because the recipient of an annotation is a team of people working together on a given subject. As a further example, if we consider annotations as metadata, the meaning of the annotation could be provided by some standard metadata specification, such as the ones provided by the DCMI, which deals with the development of interoperable online metadata standards. Last, it is also possible to organize the meanings of annotations according to some kind of hierarchy, such as a taxonomy or an ontology, in order to provide navigation capabilities among different meanings of annotation. Definition 8.1. M is a set of meanings of annotations, and m  M is a generic meaning of annotation. The meanings graph is a labeled directed graph (G M , l M ), where G M = (M , E M  M × M ) and l M : E M  L M with L M set of labels. The meanings function m : S N  2 M associates each sign of annotation with its corresponding meanings of annotation. The following constraint must be satisfied:  sn  S N , m(sn) = ; each sign of annotation has at least one meaning of annotation. As in the case of the set of access permissions, the set of scopes, and the set of link types LT , the set of meanings M is a time-invariant set, because we assume that meanings represent preexisting knowledge that does not change over time. Therefore, all the needed meanings of annotation are already elements of the set M . The goal of the meanings graph is to provide structure and hierarchy among the meanings of annotation in order to navigate and browse through them. The relation E M can be constrained in many ways to obtain the necessary structure of meanings, which can represent some domain specific knowledge. The labelling function l M can be further exploited to distinguish different kinds of arcs in the set E M in order to better explain the kind of relationship between two different meanings. Goncalves et al. [2004a] introduce the general notion of structure in DLs, rep¸ resented by a labeled directed graph, as a means of expressing different kinds of structure that might be needed in DLs, such as taxonomies, metadata, and so on. Therefore, the meanings graph adheres to this definition of structure, and it is a structure aimed at enabling navigation through the different meanings of annotation. The meanings function allows us to associate each sign of annotation with its corresponding meanings in order to clarify the semantics of the sign. Note that the meanings function is neither injective nor surjective. In conclusion, an annotation is expressed by one or more signs of annotation, which in turn are characterized by one or more meanings of annotation, thus defining the overall semantics of the annotation. The explicit distinction between the meaning and the sign of annotation is quite new in the field of annotations. Indeed, annotations are generally typed as a whole object according to some predefined set of annotation types [W3C 2005a; Kahan and Koivunen 2001; Frommholz et al. 2003; Bottoni et al. 2003, 2004], but there is usually no means for describing the semantics of an annotation with the necessary level of precision. However, this is possible with the meanings of annotation. Furthermore, annotation types do not allow any kind of navigation among different types, while meanings of annotations can be organized to do that. Some helpful information about the choice of distinguishing between meaning and sign of annotation can be obtained from the field of human computer interaction. Bottoni et al. [1999] deal with visual languages and define Characteristic Structures (CS) as sets of image pixels forming functional or perceptual units whose recognition results in the association of the CS with a meaning. They call Characteristic Patterns (CP) the CS along with descriptions of the CS and a relation that associates descriptions to CS and viceversa. The distinction between CS and CP resembles the distinction between sign and meaning of annotation; Fogli et al. [2004] also recognize this correspondence and say that "an annotation is a complex CS interpreted by a human as a CP". On the other hand, Bottoni et al. [2003] also adopt the CS and CP mechanism in the context of annotations, but they use this mechanism to place annotations on information resources rather than to distinguish between the semantics and the materialization of annotations. We are interested in studying the sharing of common meanings among different signs, defined as the basic mechanism for relating and gathering up different signs that express common semantics. This is very helpful in the case of annotations made by two different users. For example, they may use different signs to indicate the importance of a passage--an asterisk or an exclamation mark; knowing that these two different signs have the same meaning allows these two users to communicate and interact with each other. In addition, this would also help us to disambiguate cases where two signs that look exactly the same have two different meanings. For example, consider the case of a user who is performing different tasks: while he is studying a paper, he may highlight a passage to indicate that it is worth further investigation; while he is reviewing a paper, he may highlight a passage to indicate that it is not correct. In both cases, he uses the same kind of sign but with two different semantics. The most immediate way of approaching this issue is to introduce the following relation: M1 = {(sn1 , sn2 )  S N × S N | m (sn1 )  m (sn2 ) = }. This relation clearly highlights the signs that directly share some common meanings. However, this relation is not able to relate two signs that do not directly share a common meaning. Therefore, a step forward also considers both the meanings graph G M =  (M , E M ) and its reflexive transitive closure G  = (M , E M ), so that we can M introduce the following relation: M2 = {(sn1 , sn2 )  S N × S N |  m  M , m1  m(sn1 ), m2  m(sn2 ),   (m1 , m2 )  E M  (m2 , m1 )  E M      ((m, m1 )  E M  (m, m2 )  E M )  ((m1 , m)  E M  (m2 , m)  E M )}. The M2 relation means that two signs, s1 and s2 , are in relation if among their meanings, which are obtained by m(sn1 ) and m(sn2 ), at least: -- one is the ancestor of the other ((m1 , m2 )  E M  (m2 , m1 )  E M ); or   -- they both have a common ancestor ((m, m1 )  E M  (m, m2 )  E M ); or  -- they both are the ancestors of a common meaning ((m1 , m)  E M  (m2 , m)   E M ); or -- two meanings are equal--as in the case of the M1 relation. Indeed, M1  M2 because  s1 , s2  S N | (sn1 , sn2 )  M1   m  m (sn1 )   m (sn2 )  (m, m)  E M  (sn1 , sn2 )  M2 .

M2 is a very broad relation that allows us to relate different signs according to the four strategies outlined above. Where needed, we could use limited versions of M2 that adopt only some of the strategies introduced here--for example, M1 uses only the last strategy. Further strategies can be envisaged to group signs on the basis of their meanings; for example, we could take into consideration the predecessor of a meaning instead of its ancestor, as in M2 . Therefore, the M1 and M2 relations are examples of the utilization of the meanings graph; however, they are not intended to be exhaustive. For example, Rigaux and Spyratos [2004] propose a subsumption relation on the terms of a taxonomy, and a way of navigating through them that can also be very useful in the context of the meanings of annotation.

9. ANNOTATION 

We are now ready to introduce the definition of annotation. Summing up the concepts introduced in the previous sections, we can briefly say that an annotation is expressed by one or more signs of annotation, such as a piece of text or some graphic mark, which are the way an annotation takes shape. The semantics of each sign is, in turn, defined by one or more meanings of annotation. With respect to the linking issue, an annotation must annotate one and only one digital object, identified by its handle, while it may relate to one or more digital objects. Last, the mechanism introduced in Section 4.2 on how to address the time dimension is fundamental to properly define the relationship between the annotation and the annotated digital object. Definition 9.1. An annotation a  A(k) is a tuple:

a = (ha  H(k), aua  U S R(k - 1), G a  2G R(k-1) × P, spa  SP, Aa  S N (k) × LT × ST (k) × S M (k - 1) × H(k - 1)), where: -- ha is the unique handle of the annotation a, i.e. h(ha ) = a. --aua is the author of the annotation a: i.e. ha  au(aua ). -- G a are the groups of users with their respective access permissions for the annotation a, specified by the pairs (G, p) with G  G a and p  P . -- spa is the scope of the annotation a. -- Each n-ple of the Aa relation means that the annotation a by means of a sign in SN (k) and a link type in LT is annotating or relating to a segment in ST (k) of a stream in SM (k - 1) of a digital object identified by its handle in H(k - 1). Note that since  sm  SM (k - 1) |    Aa ,  = (sn, t, stsm , sm, h) must be sm  hsm(h); in other words, the stream sm must be contained in the digital object identified by the handle h. We introduce the following auxiliary sets to simplify the following discussion: -- The set of the signs of annotation that belong to the annotation a: SNa = {sn  SN (k) |    Aa ,  = (sn, l t, stsm , sm, h)} = hsm(ha ). -- The set of the handles of digital objects that are subject to the tasks of the annotation a: Ha = {h  H(k - 1) |    Aa ,  = (sn, l t, stsm , sm, h)}.

The following constraints must be adhered to: (1) The annotation a must annotate one and only one digital object, and it cannot also relate to this digital object, hence: ! h  Ha | ( sn  S Na , !   Aa ,  = sn, Annotate, stsm , sm, h )  ( 1  Aa , 1 = (sn1 , RelateTo, stsm1 , sm1 , h)). (2) A sign in S Na cannot relate to more than one digital object, hence:  sn  S N A |  1 , 2  Aa , 1 = (sn, RelateTo, stsm1 , sm1 , h1 ), 2 = sn, RelateTo, stsm2 , sm2 , h2  1 = 2 . (3) There is no other annotation a1  A(k - 1) that shares signs of annotation with a, hence: a1  A(k - 1) | S Na  S Na1 = . (4) If the annotation a  A(k) annotates or relates to another annotation a1  A(k - 1), then scope and access permission conflicts have to be avoided. Let us define the conflict detector function, cd : A(k)× A(k -1)  0, 1 , so that: cd(a, a1 ) =
0 if there are neither scope conflicts nor access permission conflicts 1 if there are either scope conflicts or access permission conflicts.

Therefore, the following condition must be satisfied:  h  Ha | h(h) = a1  A(k - 1)  cd(a, a1 ) = 0. In conclusion, the first part of the annotation tuple is devoted to providing information about the annotation itself, because it specifies the handle of the annotation, its author, its groups of users with their respective access permissions, its scope, the signs of the annotation, and the link types. On the other hand, the second part of the annotation tuple provides information about the annotated or related digital objects, specifying which segment of which stream of which digital object is being annotated or related to, as shown in the following (we do not use the time dimension notation for space reasons, as it is not needed for this observation):   a = ha , aua , G a × P, spa , Aa  S N × LT × information about the annotation ST × S M × H information about the digital object.

Note that the author, aua , of the annotation is not taken from the set of authors but from the set of users at time k - 1. Indeed, from Definition 5.2, an author is a user who authored at least one digital object; thus, if we had used the set of authors in the definition of annotation, we would have constrained the author of the annotation to have authored at least one other digital object besides the annotation in question. In contrast, if we pick out a user from the set of users at time k - 1, we allow that user to become an author at time k, simply because he is authoring the annotation at hand. Moreover, in contrast to the case of the generic digital object introduced in Section 5.1, the annotation is constrained to be authored by one, and only one, author. In the following section, we will discuss the meaning of the Aa relation and the four constraints introduced in the definition of annotation in more detail. Discussion About the Aa Relation The Aa relation makes extensive use of the mechanism introduced in Section 4.2 for addressing the time dimension. In particular, the Aa relation aims to demonstrate that an annotation must annotate or relate to digital objects that already exist. For this reason, in Definition 9.1, the annotation a belongs to A(k), while the annotated or related digital objects belong to DO (k - 1), and are identified by their handles in H(k - 1). This notation underlines the fact that the annotation belongs to the set of digital objects at time k, but it works with the previously existing digital objects that belong to the set of digital objects at time k - 1. Therefore, an annotation can only annotate or relate to already existing digital objects, which is quite intuitive but needs to be properly formalized. A very important consequence of this choice is that: ha  Ha . In fact, {ha } = H(k) \ H(k - 1) while Ha  H(k - 1): ha is precisely the handle identified by the transition from H(k - 1) to H(k), as explained in Section 4.2. Therefore, an annotation cannot be self-referential: it cannot annotate or relate to itself, since a self-referential annotation would be useless. The Aa relation makes use of the set of signs S N (k) at time k to indicate that they represent the signs created precisely for the annotation a. Furthermore, Aa uses the set of segments ST (k) at time k to indicate that those segments are created solely to allow the annotation a to point to the requested part of the streams contained in S M (k - 1). If we consider the mechanism introduced in Section 4.2 for formalizing the temporal dimension, when at time k - 1 we pick out a new segment stsm  ST (k - 1)  U ST , it can refer to a stream sm  S M (k - 1): in fact, that stream already exists at time k - 1, even though the new segment belongs to the set of segment ST (k) only at time k. Note that the Aa relation uses the set of streams S M (k - 1) at time k - 1 because those are the streams that belong to the digital objects identified by their handles in H(k - 1). In conclusion, we deal with digital objects and their corresponding streams which already exist at time k - 1 and which are annotated or related by using signs and segments that have just been created for the annotation a at time k. In the Aa relation, both segments and streams play a very important role in allowing an annotation to annotate, or relate to, the requested part of a digital object. In this context, the distinction between logical and physical streams and the possibility of using the logical streams as representatives of their equivalence classes, becomes a fundamental issue. We can always suppose that an annotation deals with logical streams, because the mapping to different physical streams is correctly managed by the notion of the stream mapping relation, as introduced in Section 6.2. In this way, an annotation can annotate a logical stream and it is also implicitly annotating all of the physical streams that are in the same equivalence class of the logical stream. Furthermore, as discussed in Section 6.2, an annotation could annotate abstract streams belonging to equivalence classes created on the quotient set S M/SM. In this way, it obtains access to an entire hierarchy of different representations of the content of a digital object. Last, logical streams simplify the use of segments because we can always refer to contiguous indices in the logical streams even though they are not contiguous in the physical streams, as observed in Section 6.3. This possibility makes it easier to determine which part of the digital object is being annotated or related, because we can always make the assumption that we are dealing with contiguous indices in the stream of the digital object. Last, the Aa relation does not explicitly make use of the meanings of the annotation, even though they are a fundamental part of our model. As explained in Section 8, the meanings of annotation represent a kind of preexisting and superimposed knowledge which does not belong to any specific sign of annotation in particular, but rather should be shared by different signs of annotation to support cooperation and interoperability. In this sense, the meanings of annotation are not directly part of any specific annotation; on the other hand, as introduced in Definition 8.1, the meanings function, m, allows us to associate each sign of annotation with its corresponding meaning of annotation. Therefore, for each sign, sn  S Na , we can use m(sn) to obtain its meanings of annotation, and we can then exploit and navigate the meanings graph, if necessary. Discussion about the Constraints of the Annotation. The first two constraints are intra-annotation constraints, because they limit the Aa relation, which is the core of the annotation; the second two constraints instead are inter-annotation constraints, because they regulate the relationships of the annotation with respect to other annotations. The first constraint imposes the existence and uniqueness of the annotated digital object and prevents the annotated digital object from being related as well. In this way, the constraint introduced in Section 6.1: an annotation must annotate one and only one digital object, either a document or another annotation, hence an annotation must have one and only one "annotate link," is complied with. Furthermore, it enforces the distinction between the "annotate link" and the "relate-to link," because the annotated digital object cannot also be related. Therefore, it underlines the fact that the role of the "annotate link" is to express intradigital object relationships, while the "relate-to link" is requested to express only interdigital object relationships. Furthermore, each sign must cooperate--once and only once--in expressing such intra-DO relationships: there is no sign whose only link is the "relate-to link." A consequence of this constraint is:  h1 , h2  Ha |  1 , 2  Aa , 1 = (sn1 , Annotate, stsm1 , sm1 , h1 ), 2 = (sn2 , Annotate, stsm2 , sm2 , h2 )  h1 = h2 . The second constraint aims at keeping the semantics of a sign as clear as possibile: if a sign sn could be related to more digital objects, it would not be clear which of its meanings--given by m(sn)--should be applied to each related digital object. In conclusion, this constraint together with the first constraint states that a sign of annotation must annotate one and only one segment of a digital object and it may relate to one and only one segment of another digital object. The third constraint ensures that the signs of an annotation are not shared with any other annotation to preserve the mechanism of sharing common semantics among annotations. As explained in Section 8, the sharing of meanings of annotation by means of the M1 and M2 relations is the mechanism for pointing out common semantics among annotations; on the other hand, the direct sharing of signs of annotation could be misleading. In fact, a sign is a materialization of a meaning: the same sign might be used by different users with completely different semantics while different signs used by different users might have the same semantics. Consider, for example, two users who use the star symbol: one uses it to indicate an important passage, while the other uses it to indicate a wrong passage; in this case, we have two signs that look exactly the same but have two completely different meanings. Another situation would be where two users use the star symbol and the exclamation mark, both to indicate an important passage; in this case, we have two signs that look different but have the same meaning. Therefore, the cooperation among users happens by sharing common meanings, which are connected by way of the meanings graph, and not by directly sharing signs, which may be misleading or incoherent. Finally, note that this constraint does not prevent the existence of two signs looking exactly the same, but it means that these two signs are different elements in the set S N . The fourth constraint prevents pathologic situations such as, for example, a public annotation that annotates or relates to a private annotation. In such cases there is a scope conflict: using the previously mentioned example, the author of the private annotation can see both the public and the private annotations, but another user can see only the public annotation, which is annotating something hidden to this user. As a further example, a shared annotation, for which a given group has "denied" access permission, should not be annotated by, or related to, another shared annotation, for which the same group has "read and write" access permission, because in this case we would obtain an access permission conflict. The situation may be even more complicated if both scope and access permission conflicts happen at the same time. To avoid such situations, the conflict detector cd function has been introduced, which returns 1 if there is any kind of conflict, and 0 otherwise. As we introduced in Sections 5.2 and 5.3, the concepts of access permission and scope of annotation are not intended to enforce a specific access policy, but rather to provide us with the means of expressing any necessary access policy. As a consequence, the actual definition of the conflict detector function has to be done case by case in order to carry out the necessary access policy: we cannot provide an a priori definition of it that is appropriate for all cases. For example, if we set the following rules: -- A public annotation, a1 , can be freely annotated or related to by any annotation a, without further restrictions. -- A shared annotation, a1 , can be annotated or related to only by a shared or private annotation a. In the case of a private annotation a, the author of the annotation a must belong to at least one of the groups of users sharing the annotation a1 , provided that the access permission for that group is not denied in a1 . In the case of a shared annotation a, all of the groups of users sharing the annotation a must also be sharing the annotation a1 , provided that none of them has been denied access permission in a1 . It follows that a shared annotation cannot be annotated by or related to a public annotation; -- A private annotation a1 can be annotated or related to only by a private annotation a, provided that they have the same author. It follows that a private annotation can be annotated by, or related to, neither a public annotation nor a shared annotation; then we can provide the following definition of the conflict detector function:   0 if spa1 = Public    0 if spa = Shared  spa = Private   (G 1 , p1 )  G a × P |  1 1    aua  G 1  p1 = Denied  cd(a, a1 ) = 0 if spa1 = spa = Shared  (G, p)  G a  (G 1 , p1 )  G a1 |   G = G 1  p1 = Denied     0 if spa = spa = Private  aua = aua  1 1   1 otherwise. 

10. DOCUMENT-ANNOTATION HYPERTEXT 

As explained in Section 3, we consider that existing digital objects and annotations constitute a hypertext. The definition and the properties of this hypertext directly follow from the definition of annotation we provided in the previous sections. Therefore, we can consider the document-annotation hypertext as a kind of view of the set of documents and annotations. The aim is to mask all of the details involved by the definition of the annotation itself, and to provide us with a more abstract representation of the objects we dealt with and of their structural relationships. We will introduce the definition of document-annotation hypertext and we will study its properties by directly using the set of digital objects, DO, and the set of annotations, A. In the following, we will not make use of the set of handles, H, as might be expected from the previous discussion where annotations can be linked to digital objects only by using their handles. This choice allows us to explain the properties of the document-annotation hypertext in a clearer and more intuitive way than doing so by using handles, which would just add a further level of indirection. Indeed, if we used handles instead of digital objects in the explanation, we would have to map each handle back to the corresponding digital object by using the h function, in order to exploit the characteristics of the digital object in the reasoning about the document-annotation hypertext. On the other hand, since the h function is bijective, we are sure that the properties of the document-annotation hypertext demonstrated by directly using digital objects hold, and are valid even in the case of the use of handles. This is quite important because according to the line of reasoning developed in the previous sections we may not directly deal with documents, which could be independently managed by external DLMS, but we will always have the possibility of referring to those digital objects by using their handles. In conclusion, the actual document-annotation hypertext could be constructed by using the handles of the digital objects, even though its properties are better explained and demonstrated by directly using the digital objects. Definition 10.1. graph: The document-annotation hypertext is a labeled directed (Hda = (DO, Eda  A × DO), lda ), where: -- DO = A  D is a set of vertices; -- Ed a = {(a, do)  A × DO |    Aa ,  = (sn, t, stsm , sm, h-1 (do)} is a set of edges; -- ld a : Ed a  LT is a labelling function, such that for each e = (a, d o)  Ed a there is a l t-labeled edge from the annotation a to the generic digital object d o: Annotate if    Aa |  = (sn, Annotate, stsm , sm, h-1 (do)) lda (a, d o) = RelateTo if    Aa |  = (sn, RelateTo, stsm , sm, h-1 (do)). The document-annotation hypertext is constructed by putting an edge between an annotation vertex and a digital object vertex, if the annotation is annotating or relating to that digital object. Note that we used h-1 (do) in Ed a to track the digital object back to its handle; the edge is then labeled with the corresponding link type. Each edge e = (a, do)  Eda always starts from an annotation a  A, while e  Eda , which starts from a document d  D, does not exist. Note that we deal with a graph Hda and not with a multigraph--a graph where multiple edges between the same vertices are allowed--as may happen in the case of an annotation relating to a different part of the same digital object. Therefore, we consider that multiple edges with the same direction between the same vertices are collapsed into a single edge. Table II summarizes the graphical conventions, adopted in the following figures. Figure 3 shows an example of document-annotation hypertext Hd a : -- D = {d 1 , d 2 , d 3 , d 4 , d 5 }; we can assume that the subscript of each document indicates the time in which the document became an element of the set D.

-- A = {a1 , a2 , a3 , a4 , a5 , a6 , a7 , a8 , a9 , a10 , a11 , a12 , a13 , a14 }; we can assume that the subscript of each annotation indicates the time in which the annotation became an element of the set A. -- We can express, for example: -- Annotation sets concerning a document: {a1 , a2 } is an annotation set concerning the document d 1 . --Annotation sets concerning an annotation: {a8 , a9 } is an annotation set concerning the annotation a7 . --Annotation threads concerning a document: {a1 , a3 , a4 } is an annotation thread concerning the document d 1 . -- Annotation threads concerning an annotation. {a8 , a10 } is an annotation thread concerning the annotation a7 ; -- Multiple annotation threads concerning a document: {a7 , a8 , a10 } and {a12 , a13 , a14 } are two different annotations threads, both concerning the document d 3 .

-- Multiple annotation threads concerning an annotation: {a8 , a10 } and {a9 , a11 } are two annotation threads both concerning the annotation a7 . -- Nested annotation threads concerning a document: {a8 , a10 } and {a9 , a11 } are two different and nested annotation threads both concerning the document d 3 . Figure 3 also points out another important feature of the documentannotation hypertext. It can span and cross the boundaries of the single IMS, as discussed in Section 3. IMS1 manages d 1 and d 2 , while IMS2 manages d 3 , d 4 , and d 5 . There are annotations that act as a bridge between two IMSs: for example, a5 annotates d 2 , which is managed by IMS1 , and refers to d 3 , which is managed by IMS2 . PROPOSITION 10.2. properties: The document-annotation hypertext has the following

(1) The graph does not contain loops:  a  A, e = (a, do)  Eda | a = do. (2) Each annotation a is incident with one and only one edge labeled Annotate.  a  A, ! e = (a, do)  Eda | lda (e) = Annotate (3) The graph does not contain cycles: C = a1 ak ak-1 · · · a2 a1 | e1 = (a1 , ak ), ek = (ak , ak-1 ), . . . , e2 = (a2 , a1 )  Ed a , k > 1. (4) Given a set A  A there are at least A edges in Hd a incident on elements of A . Therefore, the following relationship holds for the size of Hd a : (Hd a )  |A| . PROOF. We can show that:

(1) From Definition 9.1, it follows that ha  Ha and, as explained in Section 9,   Aa |  = (sn, t, stsm , sm, ha ); thus, e = (a, a)  Ed a . (2) From Definition 9.1, we have the following constraint ! h  Ha |  sn  S Na , !   Aa ,  = (sn, Annotate, stsm , sm, h); it follows that  a  A, ! d o  DO |    Aa ,  = (sn, Annotate, stsm , sm, h-1 (d o)); thus, there exists a unique edge such that ld a (a, d o) = Annotate. (3) Annotations entail a temporal dimension, since each annotation must annotate or relate to an already existing digital object, as explained in Sections 4.2 and 9. From Definition 9.1, the Aa relation involves the set H(k - 1), of handles of digital objects that already belong to the set of digital objects at time k - 1, while the annotation belongs to the set A(k). Therefore, by means of the Aa relation an annotation a  A(k) can annotate or relate to only digital objects that already exist at time k, an annotation cannot annotate or relate to another annotation that does not already exist at time k. It follows cycles, such as the one shown in Figure 4, where the oldest annotation a1  A(1) annotates or relates to the newest annotation ak  A(k) with k > 1, are not possibile. In fact, when the oldest annotation a1  A(1) was created at time 1, the newest annotation ak  A(k), ak  A(0) did not exist and so it could not have been involved in Aa1 , which makes use of digital objects belonging to the set of digital objects at time 0. Note that this issue does not exist for document d  D vertices, since edges can start only from annotation vertices. (4) Since for item number 2, each annotation a must be incident with one and only one Annotate edge, then for A annotations there are, at least A edges; there may be more if there are also RelateTo edges. In Hd a there are |A| annotations, and therefore (Hd a )  |A|. Proposition 10.2 expresses the constraints imposed on the annotation in Definition 9.1 in terms of a graph: first, the graph does not contain loops corresponding to self-referential annotations that are useless for our purposes; second, each annotation is incident with one and only one edge of the kind "Annotate link," thus formalizing the constraint on the link types introduced in Section 6.1; third, since each annotation can annotate or relate to an already existing DO, the third property ensures that there are no cycles where the oldest annotation a1 annotates or relates to the newest annotation ak , as shown in Figure 4; last, the fourth property sets a lower bound to the size of Hd a . Figure 5 shows the patterns that can be obtained by combining the allowed link types: note that each pattern is characterized by only one edge of the type "Annotate link"; furthermore an annotation is not allowed to exclusively have "RelateTo link" edges. Note that the example of document-annotation hypertext shown in Figure 3 is the result of the combination of these basic allowed annotation patterns. PROPOSITION 10.3. --E Let Hd a = (DO , Ed a ) be the subgraph of Hd a , such that: = {e  Eda | lda (e) = Annotate}. sda -- DO = {do  DO |  e  Ed a , e = (a, do)}.

Hd a is the subgraph whose edges are of the kind, Annotate, and whose vertices are incident with at least one of these edges. Let Hd a = (DO , Ed a ) be the underlying graph of Hd a , which is the undirected version of Hd a . The following properties hold: (1) (Hda ) = (Hda ) = |A|. (2) H is a forest. sda (3) Every tree in Hda contains a unique document vertex d . PROOF. We can show that:

(1) According to Proposition 10.2, (Hd a ) = (Hd a )  |A| but since in Hd a and Hd a , there are only Annotate edges, we have (Hd a ) = (Hd a ) = |A|. (2) Ab absurdo: if Hd a was not a forest, then it would be a cyclic graph. The only way of obtaining a cycle in Hd a is that in Hd a :  a  A,  e1 = (a, d o1 ) , e2 = (a, d o2 )  Ed a , d o1 = d o2 | ld a (e1 ) = ld a (e2 ) = Annotate, an annotation exists in Hd a from which two edges of the kind, Annotate, start, but this contradicts the Definition 10.1 given for the graph Hd a and thus, Hd a is a forest; (3) Since Hd a is a forest, its components are trees. Ab absurdo, suppose that there is a tree T whose vertices are only annotations. A tree T with n vertices has n - 1 edges but, for proposition 10.2, in Hd a (and also in Hd a ), n annotations are incident with n edges; so T cannot be a tree. Therefore, every tree in Hd a contains at least one document vertex d . Suppose now that there is a tree T , which contains two document vertices, d 1 and d 2 , d 1 = d 2 . Since for every two vertices in a tree there is a unique path connecting them, in the path P = d 1 a1 . . . ai . . . ak d 2 there must be an annotation ai from which in Hd a , two edges of the kind Annotate start; since by definition of Hd a there are no edges of the type e = (d m , d n )  Ed a . However, the annotation ai contradicts the definition of Hd a , and thus there is a unique document vertex d in T . Note that if we had not removed the "RelateTo link" edges from the graph Hd a , it could have contained cycles. Consider Figure 3: for example, a cycle would be C = a7 a6 a10 a8 a7 , because in Hd a we do not consider the direction of the edges. Figure 6 shows an example of the Hd a subgraph, obtained from the documentannotation hypertext Hd a of Figure 3.

11. CONCLUSIONS AND FUTURE WORK 

We have discussed the problem of providing users with annotations on different kinds of digital objects managed by IMS that can range from DBMS to DLMS. In particular, we have addressed this problem in the context of DLMS and their evolution [DELOS 2004; Ioannidis et al. 2005; Candela et al. 2006]. To this end, annotations have been studied and formalized as an effective tool suitable for enabling and carrying out the evolution of DLMS. We have introduced the distinction between the meaning and the sign of an annotation. This distinction has allowed us to better describe both the semantics and the materialization of the annotation and to adopt a flexible approach in modeling annotations. In fact, this gives us the opportunity to deal with the semantics of annotations in a flexible way, avoiding predefined types, and making it possible to exploit them as an effective collaboration tool for users. We have proposed a formal model of annotations on digital content, which until now has been absent from the literature concerning annotations. This formal model not only captures all the aspects we have described, but it also effectively formalizes the time dimension entailed by annotations. Furthermore, it introduces the notion of document-annotation hypertext and explores some of its properties. Finally, it provides us with a sound theoretical basis for future research on this matter. Future research work will concern the use of annotations in order to search for documents, and the proposed formal model constitutes the necessary groundwork to be able to design and formalize search algorithms and to express query languages that take annotations into account. Annotations provide us with an additional source of evidence, which is complementary to that already contained in the set of documents. Therefore, we can exploit annotations with the two final goals of retrieving more relevant documents and ranking them better. Furthermore, the paths that connect annotations to documents become the vehicle for moving this further source of evidence towards the documents. In fact, the document-annotation hypertext is the basic infrastructure for combining the sources of evidence that come from documents and annotations. We have already started to work on this problem in the context of data fusion [Agosti and Ferro 2005b, 2006]. This is because we need to combine the source of evidence, which comes from annotations with the source which comes from documents. For the future, we plan to employ both hypertext information retrieval [Agosti and Smeaton 1996] and link fusion techniques [Xi et al. 2004] for designing advanced search algorithms that involve annotations based on our formal model. Once we have developed search strategies that exploit annotations, we will therefore need to evaluate their retrieval performances by using standard information retrieval methodologies. We plan to adopt the Cranfield methodology [Cleverdon 1997], which makes use of experimental collections to measure the performances of an information retrieval system. The performances are measured by using the standard precision and recall figures [van Rijsbergen 1979; Salton and McGill 1983], but according to Hull [1993] we also need a statistical methodology for judging whether the measured performances can be considered statistically significant. The next step will be to investigate the possibility of using measures that differ from precision and recall and are better tailored to the features of annotations. Finally, there is a lack of experimental test collections with annotated digital contents. We have already started to work on this problem [Agosti et al. 2007b] and our future research work will also concern the design and development of this kind of test collection.
A Field Evaluation of an Adaptable Two-Interface Design for Feature-Rich Software
JOANNA MCGRENERE University of British Columbia RONALD M. BAECKER University of Toronto and KELLOGG S. BOOTH University of British Columbia

Two approaches for supporting personalization in complex software are system-controlled adaptive menus and user-controlled adaptable menus. We evaluate a novel interface design for feature-rich productivity software based on adaptable menus. The design allows the user to easily customize a personalized interface, and also supports quick access to the default interface with all of the standard features. This design was prototyped as a front-end to a commercial word processor. A field experiment investigated users' personalizing behavior and tested the effects of different interface designs on users' satisfaction and their perceived ability to navigate, control, and learn the software. There were two conditions: a commercial word processor with adaptive menus and our prototype with adaptable menus for the same word processor. Our evaluation shows: (1) when provided with a flexible, easy-to-use and easy-to-understand customization mechanism, the majority of users do effectively personalize their interface; and (2) user-controlled interface adaptation with our adaptable menus results in better navigation and learnability, and allows for the adoption of different personalization strategies, as compared to a particular system-controlled adaptive menu system that implements a single strategy. We report qualitative data obtained from interviews and questionnaires with participants in the evaluation in addition to quantitative data. 

1. INTRODUCTION 

Desktop applications such as word processors, spreadsheets, and web browsers have become woven into the daily lives of many people in the developed world. These applications have traditionally started "small" in terms of functionality and have "grown" with each new release. This phenomenon, sometimes called creeping featurism [Hsi and Potts 2000; Norman 1998] or bloatware [Kaufman and Weed 1998], is pervasive: a long feature list is seen as essential for products to compete in the marketplace, applications have become more visually complex, menus have multiplied in size and number, and toolbars have been introduced to reduce complexity but they too have grown in a similar fashion. Insufficient attention has been paid to the impact of this functionality explosion on the user. We introduce a design that supports two interfaces between which the user can easily toggle: (1) an interface personalized by the user containing desired features only, and (2) the default interface with all of the standard features. The target for the design is feature-rich productivity applications used by a diversity of users. The design has been tested in a prototype front-end to the commercial word processor Microsoft Word 2000 (MSW2K) and evaluated in a field experiment with 20 participants. The two main goals of the evaluation were: (1) to understand the users' personalization behavior with the new design, and (2) to compare our design to the adaptive interface of MSW2K. Our choice of goals determined our choice of methodology. Our first goal was exploratory in nature; our second goal was comparative. There is a distinction between controlled laboratory evaluation, where statistical significance is the norm, and field evaluation, where qualitative methods are given more weight. We have included comments from participants during interviews, which complement the quantitative data, providing a richer account of their experience during the experiment. Our evaluation shows: (1) when provided with a flexible, easy-to-use customization mechanism, the majority of users do effectively personalize their interface; and (2) user-controlled interface adaptation with our adaptable menus results in better navigation and learnability, and allows for the adoption of different personalization strategies, as compared to the particular systemcontrolled adaptive menu system in MSW2K, which implements a single strategy.

1.1 Design Solutions to Complex Software The traditional "all-in-one" interface has menus and toolbars that are static, so every user, regardless of task or experience, has the same interface. There are a number of alternative interface designs aimed at reducing user interface complexity, although most have received minimal to no evaluation. Design solutions tend to fit into one of two categories: (1) ones that take a level-structured approach [Shneiderman 1997], and (2) ones that offer a personalized interface for each user, most commonly using artificial intelligence. A classic level-structured design includes two or more interfaces, each containing a predetermined set of functions. The user has the option to select an interface level, but not to select which functions appear in that level. Preliminary research suggests that when an interface is missing even one needed function, the user is forced to the next level of the interface, which results in frustration [McGrenere 2002]. We address this limitation in our design, which uses a level-structured approach while allowing the user to modify the contents of one of the levels. There are a small number of commercial applications that provide a level-structured interface (e.g., Hypercard and Framemaker). Some applications, such as Eudora, provide a level-structured approach across versions by offering both Pro and Lite versions. Such product versioning, however, seems to be motivated more by business considerations than by an attempt to meet user needs. The Carroll and Carrithers' Training Wheels interface to an early word processor adopts a level-structured-like approach. Although there was only one interface, all of the functionality that was not needed for simple tasks was blocked off such that when the user clicked on a blocked function, a dialog appeared indicating that the function was unavailable in the training wheels system. The design had the user progressing through two distinct phases. After the first phase, the training wheels were removed launching the user into the full system. Novice users were able to accomplish tasks significantly faster and with significantly fewer errors than novice users using the full version [Carroll and Carrithers 1984]. Despite the promise of this early work, mechanisms to support the transition between the blocked and unblocked states were never investigated. In our design, users can move easily back and forth between the designs. Unlike a classic level-structured user interface, a personalized interface is one that is tailored to each individual user. The two main ways for achieving personalization are through system-initiated adaptation, namely adaptive interfaces, and through user-initiated customization, namely adaptable interfaces. These two approaches have significant differences with respect to the goal of reducing interface complexity. While the broad goal of adaptive and, more generally, intelligent user interfaces is to assist the user by offloading complexity [Miller et al. 1991], a common complaint about adaptive interfaces is that they result in the user perceiving a loss of control [Dieterich et al. 1993; Fischer 1993]. Adaptable interfaces, by contrast, have not typically been designed for the purposes of reducing complexity and so they are often difficult to use. However, they do not suffer the same user control problem [Fischer 1993].

There has been a debate in the user interface design community between those who promote the use of artificial intelligence in the interface and those who promote "comprehensible, predictable, and controllable interfaces that give users the sense of power, mastery, control and accomplishment" [Shneiderman and Maes 1997]. We briefly survey adaptable and adaptive interfaces in turn before describing our design and evaluation. In terms of adaptable interfaces, many commercial applications allow the user to reconfigure the interface in predetermined ways, such as by adding/removing functions to/from the menus and toolbars, and by moving functions from one menu/toolbar to another. Despite the prevalence of such customization facilities, there has been relatively little research into their design. A common complaint, however, is that the mechanisms for customizing are complex and can therefore require significant time for both learning and doing the customization. Thus, only the more sophisticated users are able to customize. Mackay [1990, 1991] found the latter to be true in the case of UNIX customization. She identified a small group of users, which she called the translators, who shared their customizations with the rest of the organization. Others have identified this role and assigned their own names: tinkerer [MacLean et al. 1990], and local developer [Gantt and Nandi 1992]. By contrast, Page et al. [1996] found that 92% of participants in a large field study customized their word processor. Closer examination of their work shows, however, that a very broad notion of customization was used; for example, changing the zoom setting in a dropdown button on the toolbar was considered a customization. This points to a need for a better understanding of the various forms of customization. In our study, customization is narrowly defined as adding/deleting items to/from the menu/toolbar, which is, at least in many modern graphical user interfaces, significantly more difficult to do than parameter adjustments. Relative to adaptable interfaces, adaptive interfaces have enjoyed considerable attention by the research community; given the breadth of work, we are unable to do it justice in this short review. Instead, we summarize some relevant trends and highlight selected projects. For greater depth, the reader is referred to Browne et al. [1990] and Schneider-Hufschmidt et al. [1993] for early books on the topic. More recent developments are discussed by Karat et al. [2004]. One well-known limitation of early work in adaptive interfaces was that it was too technology focused­systems were built but relatively little user testing was conducted [Maybury and Wahlster 1999]. This can partially be explained by the fact that evaluation of adaptive interfaces is more complex than that of standard interfaces; there is greater variability with adaptive interfaces and the evaluation methodology needs to accommodate this variability [Greenberg and Witten 1985; Maybury and Wahlster 1999]. Some early examples of adaptive interfaces include the Adaptive Telephone Directory in which the hierarchy of names in the directory adapted to the user's interactions such that the most frequently accessed names were located at the upper levels of the hierarch and the least frequently accessed names were located at the lower levels [Greenberg and Witten 1985]. Adaptive Prompting augmented an interface by providing a permanently visible, dynamic menu that included only the most appropriate and most likely to be chosen actions based on the user's context [Malinowski et al. 1993]. Both the AIDA system [Cote-Munoz 1993] and the Skill Adaptive Interface [Gong and Salrendy 1995] dynamically adjusted the balance of functionality offered to the user through graphical elements (icons and menus) and the command line, depending on the user's level of expertise. The Eager system detected a repetitive activity and highlighted menus and objects on the screen to indicate what it predicted the user would do next [Cypher 1991]. Of the adaptive designs described above, there was user testing reported for the Adaptive Telephone Directory and Eager. For the former, the results strongly favoured the adaptive directory compared to a static one. User testing of Eager showed promise in its ability to detect and highlight the correct menus and objects, however, "the most striking finding was that all subjects were uncomfortable with giving up control when Eager took over" (p. 37). The work on the Adaptive Telephone Directory is a constrained example, but does provide an existence proof for the efficacy of adaptive interfaces. Another limitation of early adaptive user interface research is that it focused largely on prototype systems [Thomas and Krogsoeter 1993]. We note some exceptions here. The AID project included an adaptive front end for the British Telecom electronic mail system [Browne et al. 1990]. Among other things, it provided adaptive help based on the user's level of expertise via an application expert. User testing over three half-hour sessions, each separated by three days, showed relatively poor results. An independent expert judged that only 7% of the adaptations made by the system based on inferred user difficulties and expertise were useful. Flexcel was a modified version of MS Excel that provided a separate adaptation toolbar that allowed the user to define new menu entries and new key shortcuts for function parameterization [Krogsoeter et al. 1994]. In addition, there were system-generated adaptation suggestions, which the user accessed at her convenience. User testing showed some acceptance of the adaptation but revealed that the transition between the user accepting systemdefined adpation suggestions to actually initiating adaptations him/herself was not satisfactory. Debevc et al.'s [1996] adaptive toolbar for MS Word proposed command icon changes based on frequency and probability of specific command use. User testing comparing the adaptive toolbar to a "fixed toolbar" to which users could somehow add/delete functions showed that the adaptive bar improved performance for certain tasks and that users were generally satisfied with the adaptive bar. (Similar adaptive toolbars for MS Word have been also been proposed [Lim et al. 2005; Miah et al. 1997]). Lastly Linton et al.'s [2000] recommender system alerted users to functionality in MS Word currently being used by co-workers doing similar tasks. No user testing has been reported. We note that all the user testing mentioned above has been done in the lab, and with the exception of the AID project, it has all been single session. As seen above, the Microsoft Office suite of applications is a common target for adaptive user interface research. It is not surprising therefore that MSW2K introduced an adaptive user interface, namely menus that adapt to each individual's usage [Microsoft Office 2000 Products Enhancements Guide 2000]. When a menu is initially opened a "short" menu containing only a subset of the full menu contents is displayed by default. To access the "long" menu one must hover over the menu with the mouse for a few seconds or click on the arrow icon at the bottom of the short menu. When an item is selected from the long menu, it will then appear in the short menu the next time the menu is invoked. After some period of non-use, menu items will disappear from the short menu but will always be available in the long menu. Users cannot view or change the underlying user model maintained by the system; their only control is to turn the adaptive menus on/off and to reset the data collected in the user model. (Csinger et al. [1994] have investigated the utility of an inspectable user model.) The work documented in this article aims to address a number of the shortcomings in the literature reported above. We compare an easy-to-use adaptable two-level interface (that was designed specifically to reduce complexity) to the adaptive menus in MSW2K. The adaptable model is a fully functioning frontend to MSW2K that enabled us to conduct a longitudinal field study and collect data reflecting actual personalization behavior as well as self-reported data on preferences. 

2. DESIGN OVERVIEW 

2.1 Conceptual Design What makes our design unique is the combination of three design elements: (1) There are two interfaces, one that is personalized (the Personal Interface) and one that contains the full set of functions (the Full Interface); there is a switching mechanism between interfaces that requires only a single button click. (2) The Personal Interface is adaptable by the user with an easy-to-understand adaptation mechanism. (3) The Personal Interface begins small and, unless the user adds many functions, it remains a minimal interface relative to the Full Interface. The only difference between the two interfaces is the functions that are displayed visually in the menus and toolbars. The set of functions in a particular menu in the Personal Interface is always a subset of those in the same menu in the Full Interface, and the relative ordering of functions is preserved. Thus, the only choice users make with respect to their Personal Interfaces is whether or not to include particular functions. The conceptual design was proposed by McGrenere and Moore [2000] based on a study of 53 members of the general population who used MSWord 97. They found that while many users were having a negative experience with the feature-richness of the software, the majority of users would not choose a word processor that gave them only the functions that they are currently using. Users want the ability to discover new functions. The proposed design allows users to work in a personalized interface with a reduced feature set while providing one-button access to the standard interface with all features. By default, the Personal Interface is displayed when the application is launched. There are several reasons for having a user-controlled personalizable interface rather than a predetermined static small interface. Not only do users typically use very few features [Linton et al. 2000; McGrenere and Moore 2000], but the overlap between the command vocabulary of different users is minimal, even for users in the same group who perform similar tasks and who have similar computer expertise [Greenberg 1993]. The limited command overlap between users suggests that determining appropriate personal interfaces a priori is not possible. Many users do not take advantage of customization features [Mackay 1991], likely because of complexity inherent in customization. This is a primary argument for an adaptive interface. Our goal has been to make an easy-to-understand adaptable interface instead. By starting users with a small Personal Interface, users are encouraged to customize and take control of their interfaces, right from the outset. Although our proposed multiple-interface design may seem at first glance to be somewhat awkward and nonintuitive, it was motivated by our earlier research findings and other results from the literature. The experiment was designed to assess the effectiveness of the design and to do a first comparison with the adaptive interface of MSW2K. 2.2 Implementation Our conceptual design is intended to generalize to any productivity application used by a diversity of users with a broad range of tasks. We chose to implement our design as a front-end to MSWord (1) because word processing tends to be a canonical example in HCI research, (2) because MSWord is relatively easy to program through Visual Basic for Applications (VBA), and (3) because MSWord dominates in the marketplace so we believed that participants would be easy to find. In order to evaluate this design in a field experiment with participants who were already users of MSW2K, our prototype was implemented so that it did not interfere with any customization that participants had already made to their MSWord interface. It was also designed to be easily installed on top of an existing installation of MSWord. This was accomplished by placing the required VBA code in a specialized document template that was loaded into MSWord on each startup. If necessary, a participant could remove the prototype by simply deleting this template and re-launching Word. The information about function availability in the Personal Interface was stored in a flat file, enabling the prototype to be effectively stateless; this would facilitate the quick reconstruction of a Personal Interface should a problem occur with the software. Figures 1 and 2 show screen captures of the two interfaces as well as the personalizing mechanism. 

3. PARTICIPANTS 

Twenty experienced MSWord users participated in this evaluation. Participation was solicited electronically through newsgroups and listservs that were broadcast across the University of Toronto campus and the surrounding city. In order to participate, users had to meet the following base criteria: they had to have been using MSW2K for at least 1 month, they had to do the majority of their word processing on a single computer, they had to spend a minimum average of 3 hours word processing per week, they had to have MSWord expertise above the novice level, they had to be at least 20 years of age, and they had to live at most one half hour's drive from campus. In order to ensure that these criteria were satisfied, prospective participants completed an online screening questionnaire. Ninety-eight people completed this questionnaire. They were considered in the order in which they applied. A participant's level of expertise was assessed with a Microsoft Office screening questionnaire1 that was embedded within the online preliminary questionnaire. The screening questionnaire categorizes expertise into five groups: novice, beginner, intermediate, advanced, and expert. A particpant had to be ranked at least as a beginner in order to participate in our evaluation. Personality differences with respect to feature-rich software were considered. We included 10 feature-keen participants and 10 feature-shy as assessed by an instrument developed by McGrenere and Moore [2000]. A person is categorized as feature-keen, feature-neutral, or feature-shy based on his/her response to statements about: (1) having many functions in the interface, (2) the desire to have a complete version of MSWord (i.e., not a "Lite" version), and (3) the desire to have an up-to-date version of MSWord. There was no sampling frame of the user population available to us, so we weren't able to achieve a simple random sample, that is, a representative sample. We have therefore described our sample because it may suggest limits to generalizability. An aggregate description of the participants is found in Table I. The participants are described individually in Table II. Independent samples t-tests showed that there were no statistically significant differences between the feature-shy and feature-keen participants in terms of gender distribution, age, education, MSOffice expertise, or number of years using MSWord. There were, however, a number of attributes of our sample that lead us to believe that it was not fully representative. On average, the participants appeared to be highly educated (a rating of 5 equals the completion of an undergraduate degree) and long-term users of MSWord. There were no administrative assistants, roles that clearly include many users who do word processing, although an earlier study we conducted did include them. We do not have a definitive reason why we did not get participants in these roles. Perhaps the Call for Participation did not reach these groups or they were reached but individual users did not feel that they could participate in this evaluation. Another likely point of difference between our sample and a representative sample is that graduate students make up one quarter of the participants. This is easily explained by the fact that the Call for Participation was sent to newsgroups on a university campus. 

4. EXPERIMENT PROTOCOL 

We prioritized our two main evaluation goals as follows: (1) To understand the participants' personalization behavior with the new design. (2) To compare our design to the adaptive interface of MSW2K, a commercial interface design for feature-rich software.

Given this prioritization, a field experiment was chosen instead of a laboratory experiment because it was expected that true personalizing behavior would be significantly more likely to occur with users doing their own tasks in their normal work context rather than in a lab setting with prescribed tasks that would likely be artificial and unfamiliar. To the extent that it was possible, we did introduce some controls in the experiment and so our protocol is best described as a quasi-experimental design.2 There are several differences between the two interface designs that we compared, most notably the degree of user control of customization and the use of a dual versus single interface. As will become evident, our study was not designed to tease these factors apart. We instead elected to examine the overall efficacy of an adaptable approach using the combination of factors that we believed offered the best alternative to the existing adaptive interface. Isolating the impact of each factor, once we were able to show that the features together offer an advantage, is an obvious next step. Each participant was involved for approximately 6 weeks; they used MSW2K prior to the start of the evaluation, worked with our new design for 4 weeks, and returned to MSW2K for the remaining 2 weeks. Participants met with the experimenter on three occasions and completed a series of short on-line questionnaires, Q1­Q8, to assess experience with the software, and a final in-depth semistructured interview. Figure 3 provides a timeline for the experiment protocol. First Meeting and Questionnaire Q1. The participant completed a printed version of questionnaire Q1 that assessed the participant's experience with MSW2K. At the same time that the participant was filling in the questionnaire four programs were installed on the participant's machine--the prototype software which we called MSW Personal,3 a software logger for capturing usage, a small script to transfer the log files to a backup server on the Internet, and a script to delete the prototype in the event of any technical malfunction. Each participant's Personal Interface contained only six functions initially: Exit and the list of most-recently-used files in the File menu (considered a single function), Open and Save on the Standard Toolbar, and Font and Font Size on the Formatting Toolbar. These six functions were selected judgmentally such that two frequently used functions were included for each of the File menu and the two toolbars. Questionnaires Q2 through Q6. These questionnaires assessed MSW Personal. Q2 was completed within 2 days of the First Meeting and was intended to capture the participant's first impression of MSW Personal. Q3, Q4, Q5 and Q6 were completed 1, 2, 3, and 4 weeks respectively from the First Meeting and were intended to capture the participant's experience of MSW Personal over time. Second Meeting. The Second Meeting was held within 1 day of Q6 being completed. MSW Personal was uninstalled leaving the participant with MSW2K. The log files were collected on diskette. Questionnaire Q7. Q7 assessed the participant's experience of MSW2K one week following the Second Meeting. It was intended to capture the participant's reaction to returning to MSW2K. Questionnaire Q8. Q8 asked the participant to rank MSW2K and MSW Personal in terms of each of the dependent measures 2 weeks following the Second Meeting. (In contrast to the first seven questionnaires, which captured participants' feedback on just one of MSW2K or MSW Personal, Q8 captured feedback on both interfaces, in particular rankings of the interfaces.) Third Meeting. The Third Meeting was held within 1 day of Q8 being completed. The log files were collected on diskette and the participant's machine was completely restored to the state it was in prior to the experiment. A final in-depth semi-structured debriefing interview was conducted with each participant. Instructions Given to the Participants. In advance of the experiment, participants were only told that some changes would be made to their word processing software but they were not told the nature of the changes. At the First Meeting they were told that a new version of the software had been installed--MSW Personal--which contained two interfaces. The experimenter toggled between the two interfaces once as a brief demonstration. It was pointed out that the Personal Interface contained very few functions initially but that functions could be added or deleted with the Modify button. The process of personalizing, however, was not demonstrated. Participants were told that there was no right or wrong way to use the interfaces and it was specifically mentioned that they could choose to use just one of the two interfaces and essentially ignore the other or they could switch between the interfaces in any way that suited their needs. Participants were not told that MSW Personal would be uninstalled at the Second Meeting. The impression intended was that it would be used for the entire duration of the experiment. In addition to providing the instructions verbally, printouts of the text of the instructions were given. No information about the goals or objectives of the evaluation was provided to the participants at any time during the experiment.

Scheduling. In order to ensure the timely completion of questionnaires and meetings, an individual web page was constructed for each participant that contained all the necessary due dates as well as URLs to all the questionnaires. This acted as a shared resource between the researcher and each participant. In addition, email reminders were sent by 9:00 AM on the due date of each questionnaire with the participant's web page URL directly embedded in the email, which facilitated quick access to the questionnaires. Reminders for each of the three meetings were sent one business day in advance. The participants' web pages were updated regularly to reflect completed activities. There was some flexibility in the scheduling: if a participant was unable to complete a questionnaire or attend a meeting on the scheduled date, the date could be adjusted slightly. Adjustments to the schedule were mostly made during the First Meeting. Our goal with respect to scheduling was to avoid any confusion about the time and dates of meetings and the due dates of questionnaires. In general, we were very successful in that very few meetings had to be rescheduled during the evaluation and there were few questionnaires that arrived late. Compensation. Each participant received a $100 gift certificate for a local department store as compensation. In addition, there was one $100 gift certificate awarded as a prize to the participant who completed the most number of questionnaires on time. Formal Design. The logistical constraints in conducting this experiment in the field precluded the counterbalancing of word processor conditions. The design is a 2 (personality types, between subjects) × 3 (levels, levels 1,3 = MSW2K, level 2 = MSW Personal, within subjects) design where level 2 is nested with five repetitions. The fact that there was no control group made this a quasi-experimental design rather than an experimental design [Campbell and Stanley 1972]. 

5. MEASURES 

The dependent measures were based on logging data and data collected from the eight questionnaires. From the logged data, we extracted the total time spent doing word processing, the time spent in each interface, the number of toggles between interfaces, the trace of the modifications made to the Personal Interface, the trace of functions used, and summary statistics of function use. The on-line questionnaires included a number of self-reported measures. Each questionnaire presented the same series of 13 statements that were rated on a five-point Likert scale (Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree). The statements are given below in the order in which they appear in the questionnaires: [ease of use] [menu control] [learnability] [navigation] This software is easy to use. I am in control of the contents of the menus and toolbars. I will be able to learn how to use all that is offered in this software. Navigating through the menus and toolbars is easy to do.

This software is engaging. The contents of the menus and the toolbars match my needs. Getting started with this version of the software is easy. This software is flexible. Finding the options that I want in the menus and toolbars is easy. It is easy to make the software do exactly what I want. Discovering new features is easy. I get my word processing tasks done quickly with this software. This software is satisfying to use.

Q2 through Q6 also included statements specific to MSW Personal. These were rated on the same five-point Likert scale: [personalizing mechanism easy to use] The mechanism for adding/deleting functions to/from my Personal Interface is easy to use. This mechanism for adding/deleting functions to/from my Personal Interface is intuitive. This mechanism for adding/deleting functions to/from my Personal Interface is flexible--I can modify my Personal Interface so it is exactly how I want it. This mechanism for switching between interfaces is easy to use. This concept of having two interfaces is easy to understand. Having two interfaces is a good idea.

6. HYPOTHESES 

The hypotheses below are categorized according to the two main goals of the evaluation: (1) to understand the participants' personalization behavior with the new design; and (2) to compare our design to the adaptive interface of MSW2K. Pilot testing in the field with 4 participants who each used an earlier version of the prototype for 2­3 months assisted in the formulation of our hypotheses. 6.1 Personalization Behavior with Multiple-Interfaces Design We wanted to understand whether users could use the MSWord Personal design effectively. Effectiveness relates to being able to personalize according to command usage in a way that is not overly cumbersome. We also wanted to see whether MSWord Personal is sufficiently flexible to accommodate a variety of personalization strategies. H1 Usage Hypothesis. The majority of the participants will choose to use their Personal Interface--they will find the personalizing mechanism easy to use, intuitive, and flexible enough such that they will use the mechanism to include all frequently used functions and will spend the majority of their time in their Personal Interface. H2 Approaches to Personalization Hypothesis. The multiple-interfaces design will allow for different approaches to personalization. Individual differences (feature-shy vs. feature-keen) will influence the strategies adopted. H3 Growth of Personal Interface Hypothesis. Modifications to participants' Personal Interfaces will be dominated by additions and the size of the Personal Interfaces will reach a steady state. Users will not continually need to modify their personal interfaces. H4 Modification Triggers Hypothesis. Related to growth, there will be identifiable triggers4 that prompt participants to modify their Personal Interface. For example, and most obviously, there will be an initial trigger to add functions because the Personal Interface will otherwise be almost unusable. The last hypothesis, while not directly related to effectiveness and flexibility, was deemed important because if such triggers could be identified, they could provide a design basis for user-assisted personalization. 6.2 Comparison to Adaptive Interface We hypothesized about how the multiple-interfaces design of MSW Personal compares to one particular instance of an adaptive design, namely MSW2K. While we expected differences in satisfaction, we also expected that because users operate within a small individually constructed interface they would find navigation easier and would feel a greater overall sense of control with MSW Personal. We also expected the ability to learn features would be positively impacted because operating in a user-controlled small interface acts as a training wheels interface to the full interface. With many of these hypotheses, we expected individual differences to play a role. H5 Satisfaction Hypothesis. Feature-shy participants will be more satisfied with MSW Personal than with MSW2K. The feature-shy will be more satisfied than the feature-keen with MSW Personal. H6 Navigation Hypothesis. Both feature-shy and feature-keen participants will feel that they are better able to navigate the menus and toolbars with MSW Personal than with MSW2K.

H7 Control Hypothesis. Both feature-shy and feature-keen participants will feel a better sense of control with MSW Personal than with MSW2K. H8 Learnability Hypothesis. Feature-shy participants will feel that they are better able to learn the available features with MSW Personal than with MSW2K. H9 Three-way Comparison Hypothesis. When asked to compare their overall preference for MSW Personal, MSW2K, and MSW2K with the adaptive menus turned off (standard all-in-one interface), feature-shy participants will prefer the multiple-interfaces design to an all-in-one design but will prefer all-in-one to adaptive. Feature-keen participants will prefer all-in-one to both the adaptive and multiple-interfaces designs.

7. RESULTS 

Most of the logging data for one participant, Participant 9, was lost due to technical reasons. We collected his interface toggling and Personal Interface modification data but missed his function usage data. Where this is relevant, we note N = 19; otherwise, N = 20 can be assumed unless otherwise stated. The analysis in Section 7.2, which focuses on uncovering the participants' personalization behavior with MSW Personal, relies on descriptive statistics. The analysis in Section 0, which focuses on the comparison between our design and the adaptive design in Word 2000, includes both inferential and descriptive statistics. Findings that are p < 0.05 are deemed to be significant. However, due to the qualitative nature of our field evaluation, our inability to control many of the variables, and our small number of participants, we accept a looser criterion for borderline significant results, using the range of 0.05  p  0.10. Many qualitative studies are done on so few participants that no analyses can be done; we are able to do some preliminary statistical analyses to suggest phenomena for future investigation. Both sections conclude with discussions that incorporate the qualitative questionnaire and interview data. 7.1 Experience of Multiple-Interfaces Design Our primary goal was to understand personalization behavior with the new design. We examine the results for these hypotheses first. H1 Usage Hypothesis. In the 4 weeks MSWord Personal was installed, the 19 participants each spent on average 596 minutes word processing (SD 554 min, histogram in Figure 4). Fourteen of the participants (74%) spent 50% or more of their word processing time in their Personal Interface (histogram in Figure 5). These same participants added all frequently used functions to their Personal Interface (those functions used on at least half of the usage days).

There are approximately 203 functions in MSW2K that users could include in their Personal Interfaces.5 Participants did add most of their frequently used functions; the more frequently a function was used, the more likely it was added, as shown in Figure 6. For example, 19 participants had on average 29.8 functions that were used on 25% or fewer of their usage days, and on average participants added 19.7 (66%) of those functions to their Personal Interfaces.

The percentage of functions added increases to 90%, 96%, and 100% respectively for the next three quartiles. Although the majority of the frequently used functions were added to the Personal Interfaces, there were relatively few such functions. Figure 6 shows that on average participants only had 0.7 functions that were used 75% or more of their usage days.6 Questionnaire data indicated that participants found the personalizing mechanism easy to use, intuitive, and flexible. These three attributes had mean ratings out of 5 of 4.3, 4.1, and 4.0, respectively. Note that all the data reported above represents aggregate data for both the feature-keen and the feature-shy participants. Independent sample t-tests were first run to see if there were any significant differences between the groups of participants for the dependent measures investigated, and no statistically significant differences were revealed. The largest Personal Interface was 75 functions, by a feature-keen participant. There was no correlation between time spent using the MSWord Personal and the number of functions added to the Personal Interface.

Terminology as follows: frequently-used = add frequently-used functions only; all = add all functions that are used; upfront = add majority of functions right away; as-you-go = add functions as they are needed; none = participant did not personalize; gave up = abandoned desired approach. (N = 20).

Summary: Participants personalized their interfaces according to the frequency with which they used functions; 74% of the participants spent 50% or more of their time in their Personal Interface; and participants agreed that the personalizing mechanism was easy to use, intuitive and flexible. Hypothesis Supported: Yes. H2 Approach to Using Two Interfaces. Participants were not told how they should use the two interfaces in MSW Personal and were therefore able to approach it in any way that met their needs. Analysis of the log data and the debriefing interviews allows us to approximately discern the general approaches that participants took to constructing their Personal Interfaces. In general, each approach can be broken down into two independent components: (1) which functions were added, namely, only the most frequently-used functions or all used functions, and (2) when functions were added, namely, upfront within the first few days of usage or gradually as functions were needed (as-you-go). The top of Table III gives a detailed breakdown, and an overall summary is given at the bottom of the table. We look first at which functions were added. Including both participant groups, we see that participants were almost twice as likely to add all used functions (12 participants) as only the frequently used functions (7 participants). Participants who added all used functions generally expected to use their Personal Interface exclusively whereas those who added only regularly used functions expected to switch to the Full Interface when irregularly used functions were needed. Taking individual differences into account, feature-shy were unexpectedly almost evenly divided between only including frequently used functions (4 participants) and all functions (5 participants). We thought feature-shy participants would want as minimal an interface as possible; that is, to only add their frequently used functions. Feature-keen participants, on the other hand, were more than twice as likely to want all their used functions in their Personal Interface (7 participants) rather than just the ones they used frequently (3 participants). Their preference for all functions was expected. In terms of when, the functions were added, including both participant groups, 6 participants took the approach of adding the great majority of functions that they expected to add upfront, and then only rarely adding additional functions. By contrast, 13 participants took the approach of adding some functions in the beginning and then gradually adding additional functions (as-you-go). Accounting for individual differences, feature-shy participants clearly favored the add as-you-go strategy (7 participants) to the add upfront strategy (2 participants). Feature-keen participants also appeared to favor the add as-you-go strategy (6 participants to 4) but not as decisively as the feature-shy. Seven participants gave up on their desired approach to some degree. For most this meant that they stopped personalizing, or only added very few functions beyond their first few days of using MSW Personal. These participants used their Personal Interface to the extent that they could but would then switch to the Full Interface when a function not available in their Personal Interface was needed rather than taking the time to add it. For example, Participant 1 is categorized as "all, as-you-go, gave up." She wanted to have all the functions she used in her Personal Interface but in the end she realized she was using a lot more functions than she expected, some of which she was learning for a new contract she started during the evaluation. She did continue to personalize throughout the experiment but ended up just adding the most frequently used functions, which was not her desired approach. More typical behavior of participants who gave up is described here by a participant who is categorized as "all, as-you-go, gave up": I would start out of the personal. At the beginning I was adding things pretty regularly to the personal but I felt that I was just continually adding things and so eventually I would just start out and use the personal as long as it was convenient and then I would just switch once I felt like I needed to add another function. [Interview, Participant 10] Participant 14 was categorized as "none, gave up" because he used the Full Interface almost exclusively and clearly wasn't willing to spend the time to explore his Personal Interface. It was obvious from his comments in the debriefing interview that he really did not understand the concept of a Personal Interface and by the time of the interview he had completely forgotten that he himself had added four functions to his Personal Interface during the First Meeting. Summary: Participants adopted various approaches to personalization in terms of which functions were added to their Personal Interfaces and when they were added. No one strategy dominated outright. All, as-you-go was the most popular (8 participants), but the other three combinations were also adopted: frequently-used, upfront (2 participants); frequently-used, as-you-go (5 participants); and all, upfront (4 participants). Individual differences appear to play a role in the strategy adopted, but not a decisive role. Feature-keen participants were more likely to add all used functions, rather than just their frequently used functions. Seven participants gave up to some extent on their preferred approach. Hypothesis Supported: Partially. The multiple-interfaces design did allow for different approaches to personalization; but individual differences did not strongly influence strategy. H3 Growth of Personal Interface Hypothesis. The size of participants' Personal Interfaces, with the exception of seven deleted functions, increased monotonically. All participants added functions and only 3 participants deleted a combined total of seven functions. For the purposes of understanding how Personal Interfaces evolved, we directed our attention exclusively to the 13 participants who did not give up their desired approach to personalization as these participants set up their Personal Interface in a way that met their needs. This group includes Participant 9 for whom we only have partial logging data and so we omit his data from this analysis, which leaves 12 participants. For this group of participants, there was an initial period when modifications were made regularly. This series can be defined by a first modification followed by subsequent modifications that were at most 2 usage days from the previous modification. Table IV shows that the average duration of this initial period lasted 2.8 days, and for the participant who had the longest initial period it only lasted 5 days. Of the total number of functions that these participants added during the 4 weeks, on average each had added 82% of his/her total by the end of the initial period. On average, participants personalized on 3.8 of the days that word processing occurred, with the participant who most frequently personalized doing so on 6 days. The size of participants' Personal Interfaces did approximately reach steady state in that there was a point at which the size of the Personal Interfaces did not increase/decrease by more than 10%. We expected this point to be within the initial period for the majority of the 12 participants. The results show, however, that the steady state point was reached by the end of the initial period for only half of the 12 participants (for 2 participants steady state was equal to the initial period and for 4 it was less than the initial period). On average this steady state period came after 4.8 days of usage, which was on average 31% of the way through the 4 weeks in which MSW Personal was used. On average, the last modification came 75% through the 4 weeks. The data can be viewed on a day-by-day basis. The cumulative total number of functions added by the 12 participants was 485. Figure 7 shows that within the first 2 days that MSW Personal was used, 81% of all 485 functions had been added. Summary: Personalization was dominated by additions; participants added on average 90% or more of the functions that they were going to add within 4.8 usage days, but participants were on average not finished personalizing until almost three quarters of the way through the 4 weeks. Participants were not continually personalizing. Hypothesis Supported: Yes. H4 Modification Triggers Hypothesis. In order to evaluate this hypothesis we began by identifying the patterns of usage with respect to addition, in particular, whether a function was used before or after it was added and, if so, how soon it was used before or after if was added. A detailed analysis of these usage patterns enabled us to define four triggers for addition: Immediate-need trigger. The user has an immediate need to use a function that is not currently in his/her Personal Interface and therefore adds it. This is shown clearly in the log file by the pattern: <add function x> followed directly by <use function x>. Initial trigger. The user desires to add functions when first using MSW Personal. Any function added within the first 2 days of usage that does not satisfy the immediate-need-trigger satisfies this trigger. Previously-used trigger. The user has already used a function and expects to use it in the future. Any function that has been used before it is added and does not satisfy the immediate-need or initial triggers satisfies this trigger. Future-use trigger. The user expects to use a function in the future and so adds it to the Personal Interface. Any function that does not satisfy any of the first three triggers satisfies this trigger. Table V summarizes our findings with respect to triggers for the addition of functions. We see that the initial trigger and the immediate-need trigger together accounted for almost 86% of the functions added. The previously used and future-use triggers accounted for the addition of the remaining functions, which were typically added while an immediately-needed function was being added. There were two triggers for deletion: Mistaken-addition trigger. A function was added by mistake. It was not intended or was the wrong function. Non-use trigger. A function is not being used. Only 3 participants deleted a combined total of seven functions: one, two and four functions respectively. In all seven cases the function was deleted directly after it was added. When queried in the debriefing interview the participant who had deleted four functions indicated that she had been testing the personalizing mechanism, and the other 2 participants said that the deleted functions had initially been added by mistake. Thus, the mistaken-addition trigger applied to the deletion of three functions and the non-use trigger did not apply to any deleted functions. One might initially assume that the lack of the non-use trigger is explainable by the fact that MSW Personal was only used for one month. To counter this, however, the great majority of participants indicated in the debriefing interview that they would not likely have bothered to delete functions even if they were not being used. Finally, to double check that our participants were not simply customizing when reminded of their study participation, we investigated a potential correlation between the times when subjects personalized their interfaces and times when questionnaires were due. For questionnaires Q1 and Q2, 68% and 63% of the subjects respectively did perform at least one customization on the days when those questionnaires were answered. This is not surprising, however, given the minimal starting Personal Interface and the strong initial trigger. Excluding customization that overlapped with Q1 and Q2, there was only an average 17.3% overlap of customization with Q3 through Q6 (N = 19). If we consider only those who didn't give up, that average is only slightly higher at 20.1% (N = 12). Thus, the data suggests that personalization was not being triggered by questionnaire completion but was done at various times during the roughly six days preceding the days when each of the Q3, Q4, Q5 and Q6 questionnaires were answered. Summary: The initial trigger accounted for the majority of additions (77%). When participants added an immediately needed function (9%), they would typically also add a function they expected to use in the future (13%) or one they had already used (1%). Seven functions were deleted, four to test the mechanism and three because they had originally been added by mistake. Hypothesis Supported: Yes. Discussion and Additional Qualitative Feedback. Here we discuss our findings about personalization behavior, and specifically how the multiple-interfaces design impacted this behavior. We include selected comments made by the participants about MSW Personal, both from the open-ended sections in questionnaires Q1 through Q8 and in the final debriefing interview. The goal is to bring the quantitative data to life by placing it in the context of the qualitative data. Approach to Personalization. MSWord Personal appears to provide sufficient flexibility to allow users to personalize as they see fit. Of the 13 participants who did not give up, there was almost an even distribution between the four combinations of when and which: frequently-used, upfront (2); frequently-used, as-you-go (4); all, upfront (4); all, as-you-go (3). We speculate that this flexibility played a key role in the success of the multiple-interfaces design. Having said that, it is interesting to note that none of the participants who took the approach of adding functions upfront ended up giving up. This suggests that personalizing upfront may be a more effective strategy than as-you-go. User-Assisted Personalization. Four participants suggested having some automated assistance in the construction of their Personal Interfaces, but some of them did acknowledge loss of control as a potential negative side effect. Interestingly, the quote below is from Participant 7; he was annoyed by the adaptive menus and yet he was still hopeful that the Personal Interface could be built automatically. So if I could have something where it automatically creates the personal based on my use without having to point and click buttons around, which is easy enough but a bit of a pain, if it automatically could do it for me so that over time I created a personal interface by default so my usage pattern creates it without that annoying short menu stuff, that would be nice. Because then I wouldn't actually have to think about it, I'd just use Word and it would create it as I go. [Interview, Participant 7] It is not yet clear whether fully automated interface construction is effective, but user-assisted personalization has potential. Knowledge about customization triggers should play a role in how the assistance is designed. We return to this topic in the final section of this paper. Role of the Full Interface. We checked whether having the Full Interface was considered important. Customizable interfaces do not generally provide easy access to the entire set of functionality, except through a full reset, which results in loss of the customization effort. We found that having the Full Interface was generally well liked. While there were two participants who did not make use of it at all, the great majority did use it at least once and appreciated having it available. Example representative comments include: I'd always want to have the full interface to go to just as like a baseline kind of thing. . . . Because that Word [the full interface] was there it was this safety net of--yah I know it's over there if I ever do need it anyways. [Interview, Participant 11] I like the security blanket of having the full interface but over a longer period of time I probably would have extinguished my use of it, pretty much. [Interview, Participant 17] What I would really hate is if the personal one--if you couldn't go back and forth. The fact that you could back and forth and that it was so easy to go back and forth, that was very good. [Interview, Participant 13] So although our evaluation did not assess an easy-to-customize Personal Interface in the absence of an easy-to-access Full Interface, our data suggests that the Full Interface is an integral part of the design and that personalizing behavior would be significantly altered without having a full interface available. Initial Personal Interface. We initialized the Personal Interface to be small with the goal of forcing customization. Two participants felt that the Personal Interface should have initially included more functions--those functions that are used by everyone. For example: If there was an interface, maybe it differs by industry, I don't know, but with the most common functions--like print. Everyone prints. Everyone makes things bold. So if there was a kind of pre-selected simple menu that wasn't overwhelming. I would maybe be tempted to prefer something like that. Like it would make my decision harder to decide should I use the full 2000 interface or the personal interface and then be able to switch. That distinction would be a little less clear in my mind. So if there was a pre-selected bunch of functions that everyone happens to use and I happen to fit into that everyone category. . . [Interview, Participant 3] The implicit assumption by these participants is that such a set exists. Research by Greenberg into UNIX command usage showed, however, that there is only minimal command overlap even between participants within the same group who are performing similar tasks [Greenberg 1993]. We expect that similar results would be found for MSWord command usage,7 but additional research would be required to substantiate that claim. Assuming for the moment the existence of a "reasonable" set of functions that could be used to initialize the Personal Interface, it would be interesting to see how personalizing behavior might change with such a relatively large initial Personal Interface. On the one hand, users would not have to take the time to do any initial customization. On the other hand, users would not be taking ownership of their Personal Interfaces from the outset, which could diminish their overall engagement in the personalization process. Another possible research avenue would be to investigate appropriate initial Personal Interfaces based on individual differences (feature-keen/shy). Usability of MSWord Personal. Our participants identified some basic usability improvements to the implementation of the personalizing mechanism. Three participants commented that it was somewhat cumbersome. The confirming dialog box that appears after the selection of each function was seen to be unnecessary. For example: The double menu that you get. . . I found confusing, a little bit, but it was easy to use. . . . It's a little clunky. [Interview, Participant 17] I mean it wasn't difficult. It was just you had to click, and you had to click again, and click again, and go back and go forth--it was just bulky. [Interview, Participant 22] Three participants wanted to be able to add an entire menu at once. The current design requires each menu item to be added one at a time, and because of the number of steps involved this can be time consuming if the majority of a menu is desired. Four participants felt that MSW Personal was "a good start" but in addition to simply selecting a subset of functions for their Personal Interfaces, they wanted to be able to restructure the menu hierarchies: I would like to be able to rewrite the stupid menu structure of the MS Word program, not just select the options that I want within the stupid tree structure. [Q4, Participant 7] I would have liked to put things in different places you know. And this is bad because I do this in Word because I don't like what they have decided is on this menu. [Interview, Participant 11] Both of these participants were surprised when they were informed at the end of the evaluation that this restructuring functionality was available through MSWord's native customization facility. (It is worth noting that these comments were made by Participants 7 and 11, both of whom are expert longterm users of MSWord.) One participant requested the ability to have more than two interfaces--she wanted different Personal Interfaces related to the different tasks she performed: One thing that would have been cool is if I could have had different settings. Like if you have the default Word and then a personal. . . because I work on charts and I work on just regular reports. If I could have two kind of different settings--say this is going to be my flowchart settings. And those would have draw, all the draw and shape tools and everything. And then this is my report interface and that would just have regular stuff. That would have been cool. And I don't know if a lot of people work like that. Because sometimes . . . I'm just writing reports or writing proposals and stuff like that and then other times I'm just doing very different things and I need to switch my orientation around and like have all this drawing stuff. And that just doesn't fit with regular writing. [Interview, Participant 11]8 Our design goal for the customization mechanism was to make it straightforward/understandable so we opted for a design that offered only basic functionality (adding/deleting functions) and that could be learned quickly through trial and error. Despite the comments above, all participants noted how easy it was to use the addition/deletion mechanism. Thus, we believe that it was easy to use in the sense that it was easy to figure out what to do and no errors occurred, but there were too many steps required. One participant pinpointed our trade-off: "The Add/Delete procedure seems slow and redundant for some reason, but is rather idiot-proof." We could rectify the "clunkiness" by removing the confirmation dialog box and designing a new form of menu such that when the user is selecting items from a menu to add to the Personal Interface, the menu stays open and check boxes appear adjacent to each item indicating its availability in the Personal Interface. Currently, in order to add a menu item the user selects the item as in normal menu usage; after selection, the menu disappears and the user must reopen the menu in order to add another item. We believe that personalization was affected by the design of the personalizing mechanism. Some participants would not likely have given up if functions could have been added more quickly. Some wanted more flexibility, but to support menu hierarchy restructuring and more than one Personal Interface would likely make the personalizing mechanism inaccessible to nonadvanced users. The native MSWord customization facility does allow for some of this, but relative to our personalizing mechanism it requires substantially more skill to use. We had expected the flexibility ratings recorded in the questionnaires to reflect the limitations of the personalizing mechanism, but the participants rated flexibility 4.0 out of 5.0. So although some users did articulate preferences for additional flexibility, the quantitative data shows that the mechanism was sufficiently flexible for most participants. One alternative design to explore would be two levels of customization: basic and advanced. Greater flexibility would be available through the advanced level, but users would by default start in the basic level. Individual Differences. The expected differences between the feature-shy and the feature-keen participants did not play out in any substantial way in how they personalized and used MSW Personal and what they had to say about their experience using it. Significant differences between the two groups of participants did appear, however, in terms of how MSW Personal was compared to other interfaces, as will be shown in the next section.

7.2 Comparison with the Adaptive Interface We turn now to the remaining hypotheses, which cover our secondary evaluation goal, namely to compare the multiple-interfaces design of MSW Personal to the adaptive interface of MSW2K. The first four of these hypotheses (H5­H8) compare the two interfaces with respect to satisfaction, navigation, control, and learning. The means of each of these four dependent measures at Q1 through Q7, separated by personality type, are given in Figure 8. A series of three factorial ANOVAs (Analysis of Variance) was run to test for significant differences: (1) Q1 vs. Q6: Compares measures after extended time in each condition. Q1 responses reflect usage of 1 month or more with MSW2K. Q6 reflects 1 month's use of MSW Personal. (2) Q6 vs. Q7: Compares measures as an initial reaction of returning to MSW2K after 1 month's use of MSW Personal. (3) Q2, Q3, Q4, Q5, Q6: Compares measures at regular intervals during 4-week usage of MSW Personal. In addition to reporting statistical significance, we report the effect size etasquared (2 ), which is a measure of the magnitude of the effect of a difference that is independent of sample size. Both Landauer [1997] and Vicente and Torenvliet [2000] note that effect size is often more appropriate than statistical significance in applied research such as Human-Computer Interaction. The commonly accepted metric for interpreting eta-squared is: 0.01 is a small effect, 0.06 is medium, and 0.14 is large. H5 Satisfaction Hypothesis. The MSWord versions impacted the satisfaction of the two groups of participants differently (Figure 8). There was a borderline significant cross-over interaction for Q1 vs. Q6 (F(1,18) = 4.12, MSE = 0.98, p = 0.057, 2 = 0.19) prompting us to test the simple effects for each group of participants independently. The Q1 vs. Q6 comparison was not significant for the feature-keen participants, however, the increase in satisfaction was borderline significant for the feature-shy (F(1,9) = 3.65, MSE = 1.34, p = 0.089, 2 = 0.29). Two further tests compared the satisfaction of the feature-shy participants to the feature-keen participants at Q1 and then at Q6. The feature-shy were found to be (borderline) significantly less satisfied than the feature-keen while using MSW2K at Q1 (t(18) = -2.04, p = 0.056). However, there was no significant difference detected between the two groups while using MSW Personal at Q6. When participants returned to MSW2K (Q6 to Q7), the feature-shy appear to have dropped in satisfaction and the feature-keen had effectively no change, but this cross-over interaction was not significant. Summary: The analysis suggests that the feature-shy participants were less satisfied than the feature-keen participants when using MSW2K, however, the feature-shy participants experienced an increase in satisfaction while using MSW Personal. The feature-keen participants did not experience any change in satisfaction when they switched to MSW Personal.

Hypothesis Supported: Partially. Feature-shy participants were more satisfied with MSW Personal than with MSW2K, but they were not more satisfied with MSWord Personal than the feature-keen participants. H6 Navigation Hypothesis. The version of MSWord had a significant main effect on participants' perceived ability to navigate in both the Q1 vs. Q6 comparison (F(1,18) = 5.76, MSE = 1.05, p = 0.027, 2 = 0.24) and the Q6 vs. Q7 comparison (F(1,18) = 8.02, MSE = 1.22, p = 0.011, 2 = 0.31) (Figure 8). Both comparisons favored MSW Personal. There was a borderline significant learning effect in Q2 through Q6 (F(4,72) = 2.38, MSE = 0.18, p = 0.06, 2 = 0.12) indicating that navigation became easier over time; unsurprisingly, none of the post hoc pairwise comparisons with the Bonferonni error correction were significant. Summary: The analysis suggests that both the feature-keen and the featureshy participants found it easier to navigate the menus and the toolbars using MSW Personal than MSW2K. Hypothesis Supported: Yes. H7 Control Hypothesis. The results of the Q1 vs. Q6 comparison of control are dominated by a borderline significant interaction (F(1,18) = 4.38, MSE = 0.82, p = 0.051, 2 = 0.20) (Figure 8). Testing the simple effects found the Q1 vs. Q6 comparison to be nonsignificant for the feature-keen participants, however, the feature-shy perceived a significant increase in control (F(1,9) = 11.17, MSE = 0.64, p = 0.009, 2 = 0.55). Two further tests compared control for the feature-shy participants to the feature-keen participants at Q1 and then at Q6. The feature-shy reported significantly less control than the feature-keen while using MSW2K at Q1 (t(18) = -2.72, p = 0.014). However, there was no significant difference detected between the two groups while using MSW Personal at Q6. There was a main effect for control from Q6 to Q7 (F(1,18) = 5.89, MSE = 0.51, p = 0.026, 2 = 0.25) suggesting that both groups of participants felt a loss of control when returning to MSW2K. The statement being rated reflects a participant's general sense of control over the software and not simply their control of the menus and toolbars. Summary: The analysis suggests that at the outset the feature-shy participants felt that they were less in control of the MSW2K software than did the feature-keen participants, however, the feature-shy participants experienced an increase in control with MSW Personal. The feature-keen participants did not experience a change in control when they switched to MSW Personal. Both groups of participants appear to have experienced a loss of control when they switched back to MSW2K after having used MSW Personal for 4 weeks. Hypothesis Supported: Partially. Feature-shy participants felt a better sense of control with MSWord Personal, but this was not the case for the feature-keen participants. H8 Learnability Hypothesis. In the Q1 vs. Q6 comparison the MSWord version had a borderline significant main effect on learnability (F(1,18) = 4.13, MSE = 0.61, p = 0.057, 2 = 0.19) showing that both groups of participants' perceived ability to learn the available functions was greater with MSW Personal than with MSW2K (Figure 8). Personality type also had a borderline significant main effect on learnability (F(1,18) = 4.07, MSE = 0.60, p = 0.059, 2 = 0.18) showing that, independent of software version, feature-keen participants felt better able to learn the functionality offered than did the feature-shy participants. The Q6 vs. Q7 comparison showed that the software version had a borderline significant main effect (F(1,18) = 3.08, MSE = 0.20, p = 0.096, 2 = 0.15) whereby participants' perceived ability to learn decreased when they returned to MSW2K. Summary: The analysis suggests that the feature-keen participants generally find it easier to learn functions than do the feature-shy participants, and that overall it was easier to learn functions with MSW Personal than with MSW2K. Hypothesis supported: Yes. H9 Three-way Comparison Hypothesis. In the final debriefing interview, participants were asked if they could explain how what they called the "changing menus" worked (MSW2K's adaptive menus). Although all participants were aware of the short and long menus and could explain how to expand the menus, 7 of the 20 participants (35%) had to be informed that the short menus were in fact adapting to their personal usage. Given our sample, which included no novice users, this was particularly suprising. Participants were then asked to rank according to preference MSW Personal, MSW2K with adaptive menus, and MSW2K without adaptive menus (the standard "all-in-one" style interface).

Figure 9 shows the frequency of the three-way rankings. Of the six possible rankings, only five occurred. We analyzed the frequency with which each menu condition was ranked first, by calculating the Chi-square statistic to determine if actual frequencies were significantly different from the case in which all frequencies are equal. If we consider all 20 participants, there was a significant overall preference for MSW Personal (13 participants, 65%,  2 (2, 20) = 9.10, p = 0.011). We cannot apply the Chi-square statistic independently for the feature-keen and feature-shy because of our small sample sizes. Instead, we next describe the data for each group to indicate possible trends. To make two-way comparisons between the interfaces for each of the personality types, we aggregated across the rankings. For example, by looking at the two leftmost ranking orders in the figure we see that 7 feature-shy participants preferred MSW Personal to the other two designs. From the remaining ranking orders we see that 3 feature-shy participants ranked the all-in-one design before the MSW Personal design. This shows that for the feature-shy there was preference for the MSW Personal to the all-in-one design: 7 participants to 3 participants. One can repeat the same steps to find that the feature-shy preferred the all-in-one to the adaptive design (8 to 2). However, the feature-keen did not prefer the all-in-one to both the adaptive and MSW Personal designs as expected. In fact, MSW Personal was preferred to adaptive (7 to 3) and preferred to the all-in-one (6 to 4) but the adaptive was preferred to the all-in-one (7 to 3). Only 2 of the feature-shy ranked adaptive before all-in-one as compared to 7 of the feature-keen. Summary: Although all participants were aware of the short and long menus in MSW2K, 35% had to be told that the contents of the short menus were adapting to their function usage. MSWord Personal was preferred by the majority of feature-shy and feature-keen participants, 65% of all participants. Feature-shy's overall ranking was: adaptable, all-in-one, adaptive. Feature-keen's overall ranking was: adaptable, adaptive, all-in-one. Hypothesis Supported: Partially. Feature-shy participants did rank adaptable, all-in-one, and then adaptive; but the feature-keen participants did not rank all-in-one before adaptive and adaptable. Discussion and Additional Qualitative Feedback. Here we discuss our findings related to the comparison of the two interfaces. As before, we include participants' comments, both from the open-ended sections in questionnaires Q1 through Q8 and in the final debriefing interview, to provide more context for the quantitative results. Adaptive Menus. The adaptive menus of MSW2K were liked by some and strongly disliked by many, but others had little opinion either way. There were three participants who ranked the adaptive menus in Word 2000 first. Two had very positive comments when asked if they were aware of these menus and if they knew how they worked. For example: Yes [I have noticed the "changing" menus], love that. It does it on my operating system as well. . . Yes [I know how they work], it seems that the functions that you use most often are the ones that show up. Or I don't know if they are the ones that I use most often or the ones that are used most often. I haven't figured that one out yet. . . . . Actually, I don't think it is the ones that I use most often. I think that it is a standard small set and then you click on the bottom and the whole set comes up. [Interview, Participant 22] It seems like it just responds to whatever functions you use most recently. It gives you the most recent five or whatever. I like that kind of personalization because it is more dynamic and it just seems that I am always changing what I am doing from day to day. [Interview, Participant 10] Interestingly, both participants were expert long-term users of MSWord and although they were aware of the adaptive menus, neither of them could fully explain how they worked. Participant 22 suspected that the menus adapted to her usage but then questioned whether this was right. Participant 10 knew that they were adapting but implied that there was a maximum number of items that could be shown when in fact if one had used all of the menu items recently, they would all appear in the short menu. This suggests that the user does not have to fully understand the conceptual model of an adaptive interface in order to be satisfied with the interface. If the adaptation "works well enough", then understanding the underlying mechanics is not important, at least for some users. There were seven participants who had very negative experiences of the adapting menus. The first comment below refers to the ordering of items when the menu expands from the short version to the long one. If a desired item is not found in the short menu, then a user will likely have to rescan the full long menu because the newly-visible items are interspersed with those that have already been scanned. This usability problem might be fixed by highlighting the newly-visible items in such a way that they would be scanned first.

I don't know why [I dislike the adaptive menus], because I know that. . . well I have some idea why but. . . Well okay, first of all, part of the advantage of having these mega menus is that you can hunt through them and I realize that the stuff you use more tends to show up at the top. But when you click the open, the adaptive menus, the menu shows up in the way it is normally. So if there's--if you use two functions and they are right side by side and then there's actually a bunch of stuff in between, that will show up in between. And so I find it's like I have to start all over again looking through the menus for the functionality, which I find really annoying. And I don't know why. It's confusing, I just find it more confusing. I think that's ultimately it. And I don't think of myself as a na¨ve user and I don't know why it bothers me so much it just does. [Interview, i Participant 3] . . . [T]he adaptive menus are hell, I don't like them at all. So like that's a definite No--like that's almost a zero choice. I would never pick that, like I just hate it. [Interview, Participant 7] I hate the menus where only your most recently used items show up first!!! [Q7, Participant 11] Well the first thing is with the Word 2000--I really really really dislike the-- I mentioned this with my questionnaire before--the frequency of use menu. I was often making mistakes and because they only give you, I don't know, a fixed number, maybe six menu items, I tend to use a lot of different functions regularly all of the time. So I was always, you know, using that little piece [down arrow icon], and I was always making a mistake going--where is it? Where is it? Where is it? It's gone! It fell off. I found that just. . . I still find that incredibly frustrating. So I would rather not do that and Word Personal didn't do that. So I much prefer it. [Interview, Participant 16] Four additional participants felt negatively about the adaptive menus but not to the same extreme as the previous seven participants. For example: I don't really feel one way or another about that. In fact I'd rather it didn't do that because sometimes I forget like I'm looking for something and I'm like-- oh, I can't find it, where is it? And I can't find it because it's a hidden thingy. [Interview, Participant 6] One participant did appreciate having only some options visible through the shortened menus but ultimately found that MSW Personal provided a better balance for him: This feeling that you will forget that certain functions are there if you leave [the adaptive menus turned] on but also the menus are way too long if you leave everything on [i.e., adaptive menus turned off so that you have the full menu]. So it's a balance between the two. That's why the Personal gave me the balance I wanted. [Interview, Participant 17] To summarize, 13 participants expressed opinions about the adaptive menus in MSW2K. For two participants, these menus worked very well. They were very strongly disliked by seven, and four were mildly negative. One possible explanation is that the adaptive model behind the menus provided a "better fit" for the usage patterns of the two satisfied participants than for the other participants' usage patterns. Understanding the required "degree of fit" of an adaptive interface in order to achieve usability is an area of future investigation. Individual Differences in Satisfaction and Control. Unlike the results from the first set of hypotheses, a number of differences between the feature-shy and the feature-keen participants are suggested in the self-reported measures from Q1 through Q7. Results for perceived control and satisfaction were dominated by interactions, whereby feature-shy participants experienced an increase in both satisfaction and control while using MSW Personal and the feature-keen did not experience any significant difference. One way this can be interpreted is that MSW Personal appears to have improved satisfaction and sense of control for the feature-shy without negatively affecting the feature-keen. Once they had used MSWord Personal for 4 weeks, the feature-shy were able to achieve a comparable level of satisfaction and perceived control to the feature-keen. This suggests that through the redesign of the user interface we can improve the experience of one group of users without negatively affecting the experience of another group. Navigation The comparison of the Q1 and Q6 data showed a strong effect of navigation, for both the feature-shy and the feature-keen. Some of our participants specifically noted the time savings when there were fewer options to navigate through in the menus and toolbars. Representative comments include: I really like having only the tools I use very frequently on my interface if I so choose. It makes me more efficient as I don't have to look around for the function I need. [Q2, Participant 15] While I use a standard set of features for most of my work, I am pleasantly surprised when I go to use a feature I haven't used for a while and find it's the only one in the menu. It makes my task faster and less frustrating. [Q5, Participant 12] I'd be in the Microsoft Word interface and it'd be like--oh God just too many buttons. Like I don't think that Microsoft does a really good job of making their icon match what the button actually does. And I will sit there and I will have to hover over the button and wait for the explanation to come up. And it's like oh man, what a waste of time! So that's when I'd find myself getting like, okay, I don't need all this crap right now. It's too hard to find things on all the menubars and that's when I'd switch back to the personal. [Interview, Participant 11] These comments suggest that the difference in navigation between MSWord Personal and MSW2K is not a subtle difference. Learnability. When participants were asked to assess the learnability of the multiple-interfaces design and the adaptive design in questionnaires Q1 to Q7, the multiple-interfaces design had significantly higher ratings. In the debriefing interview, however, the all-in-one style interface was presented as an option alongside the other two interfaces.

Fifteen participants indicated that the all-in-one interface was best and the standard reason given was that seeing all of the menu items all of the time gives one a sense of what is available and thereby promotes learning the available functions. Some participants specifically mentioned learning through exploration. Two representative comments are: 2000 without the changing menus [is best] just because you can see all of your options so you know what all of the features are. [Interview, Participant 2] I want to learn them all or nothing. . .In general I think that if they are there you are more likely to say--what is this?--and use it. So maybe a little bit. Because I have explored. The only way I've learned to use the program is by playing with it. So I saw the indexes and I went--how do you do that?--so now I know how to make an index. I guess if I never saw it, I'd probably never have played with it. [Interview, Participant 22] Two participants indicated that having everything available in the Full Interface within MSW Personal supported learning equally as well as did the allin-one interface. They did not need to be accessing the full menus all of the time: I think if I can switch to the full interface like that, it's very convenient. So I think the learning ability [in MSW Personal] shouldn't be impacted. . . . Just one click. [Interview, Participant 5] I want it all or I want mine [personal interface]. In the same way, I don't want the computer deciding what it's going to show me. I want to decide myself. If I don't know how to do something then I want to go and use the full interface more as like a reference or something and have it all kind of there. [Interview, Participant 11] Two participants indicated that they learn through exploration but that they are not in exploratory mode all of the time. Having a Personal Interface forced them to take ownership of learning as they actively decided when to enter exploratory mode by switching to the Full Interface. For example: I think the one with the adaptive menus doesn't support it [learning] at all because it just disappears on you and you don't even know that it is there. I would say that it is probably similar between the regular Word long menu and the Personal one. Because you still have to think that you need something different and find it. Often my strategy around that is that somebody says--Oh, try this-- or--there must be a way to do this--and then go to help or whatever. [What about learning by remembering labels you have seen in the long menus?] That's not been my experience myself. . .I must say that whether it's the Personal sort of thing or the long menus, for me at least it's--oh, I have to go exploring, I'm going to go look. Because with your sort of routine daily functions I am not using the menus, I'm not paying attention to the menus. So I'm not in explore mode. I'm not even in attentive mode, I wouldn't even notice if I've seen something related or not. So I wouldn't usually notice. [Interview, Participant 16] None of the participants thought that the adaptive menus best supported learning, which is a strong statement.

Understanding learnability is a rich area for further research. Our participants certainly did not perceive that minimalist interfaces provide scaffolding for learning; we saw no general perception of a Training Wheels effect [Carroll and Carrithers 1984]. The adaptive interface provides minimalist short menus, and they were ranked last for learnability by all of our participants. While five participants ranked the adaptable interface first, the majority thought that allin-one best supported learning. One possible next step would be to evaluate the learnability afforded by these different interfaces with novice users, and to use objective measures of learning rather than self reports. We did not include any novice users in our study. Cost/benefit Trade-off to Personalization. Personalization can be framed in terms of a cost/benefit trade-off.9 The goal is that the cost of personalizing (time, attention away from primary task) will be outweighed by the benefits of personalizing. We have already mentioned some of the benefits, which include reduced navigation time. We note here that some of the participants were analogously very aware of the costs; for example, the cost to set up the Personal Interface: The personal interface is an interesting concept but seems time consuming to completely set up. I am still adding features to it. [Q3, Participant 18] Initial configuration was time-consuming but it is ok if it only must be done once. [Q8, Participant 16] Another cost is the additional complexity added to the interface. While the goal of multiple-interfaces is to allow users to work predominantly in a simplified interface, there is additional functionality that needs to be included in the interface in order to make this possible. For some, that cost dominated: The thing is for me was that I want my software to be pretty much invisible to me and the personal required more visibility than I would have liked it. [Interview, Participant 13] For others, however, the benefit clearly dominated: I think something like that [MSW Personal] should be made available. It's a nice thing--it's a nice interface. I mean, you know, I don't know how easy it would be to be available to many people. I guess that you would have to package it or whatever. But it was a nice addition. I actually enjoyed it. [Interview, Participant 15] I think that Word XP10 needs a personal edition even more than 2000. [Q8, Participant 17] I would like to have Personal re-enabled on my machine! [Q8, Participant 16] Thus, for some users, even if the cost to personalizing was relatively high, there was sufficient benefit derived from a Personal Interface to make it worthwhile. For others, the cost outweighed the benefit. The difficulty for design is that the perceived cost and benefit are both dependent on individual users and difficult to determine a priori. Overall Interface Preference. MSWord Personal was preferred by the majority of our participants, seven feature-shy and six feature-keen. Having such strong support by the feature-keen was unexpected. However, as noted above, the two groups of participants differed in their second ranking. Only two of the feature-shy ranked adaptive before all-in-one as compared to seven of the feature-keen. This can perhaps be explained in part by the fact that six of the seven participants who were unaware of the adapting short menus were feature-shy participants. This indicates that lack of knowledge that adaptation is taking place likely contributes to overall dissatisfaction with an adaptive interface. Interestingly, of the 13 participants who expressed an opinion about the adaptive menus beyond a simple ranking, only two were positive. Yet three participants ranked adaptive first and 9 ranked it second, so adaptive menus did have supporters. The imbalance in the comments about these menus suggests that those who disliked the menus had a more extreme or passionate dislike as compared to those who liked the menus. The implication for user interface design could be that even if a design works sufficiently well for a large part of the user population, if that same design is perceived by others in the user population to work very poorly, the negative views will dominate. Not surprisingly, the group of 13 participants who ranked Personal first is almost identical to the group of 13 who did not give up on their desired approach to personalization. (One participant who didn't give up did not rank MSW Personal first and one participant who did give up did rank it first.) Given that this group spanned the possible personalization strategies, it suggests that flexibility of personalization played a role in the interface ranking. Users have the ability to personalize when they want and what they want in MSWord Personal. There is no such flexibility in MSW2K, which implements a single personalization strategy. Independence of Variables. One might argue that our dependent measures of satisfaction, navigation, control, and learning are at least somewhat related to our independent variable of personality type (feature-keen and feature-shy). For example, it may not be entirely surprising that the feature-shy were significantly less satisfied than the feature-keen with MSW2K at the time that Q1 was administered. After all, MSW2K is a feature-rich application. There was another independent variable, however, namely the two interface conditions. It is the impact of those conditions on the dependent measures that is most interesting in the findings we have reported in this section. The results of this evaluation are promising, however, there were inherent limitations and constraints to the experiment design that may have affected the results. Four threats to validity in our experiment are: (1) Reactive effect: Participants were fully aware of their participation in the experiment and some may have adjusted their interactions and responses.

(2) Multiple-treatment interference: Participants were exposed to two versions of MSWord and there were likely effects of having used one version that were not erasable when using the second. (3) Researcher interference: A single researcher performed the role of the experimenter for this experiment. There may have been something specific to the particular researcher that systematically affected the results. (4) Selection bias: Participants were a self-selected group because we did not have a sampling frame of all MSW2K users and therefore could not draw a simple random sample. The best way to ensure that there wasn't anything incidental in our experiment execution that determined the results would be to replicate the experiment. Ideally we would want to conduct a longer field experiment with a different person acting in the researcher role. Counterbalancing the order in which software versions are used would be ideal. In addition, using a different application, whether it be another word processor or another general productivity application, would go a long way to ensuring the generalizability of the results to the class of word processing applications or general productivity applications as a whole. 

8. CONCLUSIONS AND FUTURE WORK 

We conclude with some final thoughts about MSWord Personal, adaptive versus adaptable designs, individual differences, user assisted personalization, and other scenarios of use for multiple interfaces designs. MSWord Personal. The multiple-interfaces design of MSWord Personal performed very well in our field evaluation. Unlike previous work by Mackay [1991], which showed that users customize very little, the majority of our participants did personalize and they did so according to their function usage. The fact that MSW Personal offers a new style of interface, unfamiliar to all our participants, and requires effort to use, did not preclude the majority of participants (65%) ranking it first, preferring it to both the adaptive interface of MSW2K or an all-in-one style interface. We expect that had it been possible to add functions faster, even more participants would have ranked Personal first. That almost as many of the feature-keen (6 participants) as the feature-shy (7 participants) ranked Personal first is particularly encouraging. Adaptive vs. Adaptable. Despite the breadth of research into adaptive user interfaces, there has been relatively little empirical comparison between adaptive and adaptable interfaces, and to date all investigations have been relatively short laboratory experiments (e.g., Debevc et al. [1996]). Our experiment allowed us to compare one instance of each of these design alternatives in the context of a commercial software application with users carrying out real tasks in their own environment. Results favored the adaptable design but the adaptive interface definitely had support. The adaptable design implemented in MSWord Personal combines several design elements: two interfaces (one personal interface, one full interface), a simple toggle to switch between the interfaces, an easily adaptable personal interface under full user control, and a small initial personal interface. The interface of MSW2K, by comparison, has a single interface, which is adapted solely by the system, with the exception that the user can easily open a short menu into a long menu. Our evaluation did not isolate the effects of the different interface design elements, although where possible we did get qualitative feedback on those elements. Our findings suggest that MSW Personal was preferred to MSW2K because user-controlled interface adaptation results in better navigation and learnability, and allows for the adoption of different personalization strategies, as compared to system-controlled interface adaptation, which implements a single strategy. Because there were several differences between the two conditions compared, we do not assert that two interfaces are always better than one, nor that adaptable is always better than adaptive. A 2 × 2 experimental design, comparing one/two interfaces by adaptive/adaptable, would be required to tease this apart. We did not do this, for the reasons given earlier. Based on the qualitative feedback in our evaluation, however, we strongly believe that the two-interface aspect of MSW Personal was a key contributing factor to its success; it allowed users the flexibility to adopt different personalization strategies. We did in fact observe different personalization strategies. We note that the effect of the dual interface, namely support to easily move back and forth between a personalized interface and an interface with default settings, could have been achieved through a single-button "factory reset" operation if the reset was undoable at any time by the user. We believe that conceptually this model would be more difficult for users, especially novices, to understand and trust. Informative next steps in the research include comparing only two-interface designs, one where the personal interface is under user control and the other where it is under system control, as well as comparing a two-interface design to a factory reset model that achieves the same outcome but with a different conceptual model for the user. More recent laboratory research investigating adaptive designs shows conflicting evidence. Findlater and McGrenere [2004] found that adaptive split menus [Sears and Shneiderman 1994] were slower than static split menus, and slower than adaptable split menus in most circumstances. Subjects also preferred the adaptable menus to both the static and adaptive ones. Gajos et al. [2006] found an adaptive interface to be faster than a static one, but no adaptable alternative was evaluated. They also suggest some reasons for the conflicting evidence, for example, the frequency with which adaptations occur, but more work is needed to tease these issues apart. Alpert et al. [2003] investigated user attitudes regarding a user-adaptive e-commerce website and found that users were unenthusiastic toward system attempts to infer user needs and provide adapted content accordingly. A strong desire to have full and explicit control of the content and interaction was expressed. Jameson and Schwarzkopf [2002] compared adaptable and adaptive systems for adding items to a hotlist for a conference web site. While there was no performance difference, anecdotal evidence showed that some subjects strongly preferred the adaptive system, while others strongly preferred the adaptable.

Individual Differences. The existence of individual differences with respect to software features is an idea that has been proposed in the literature [Kaufman and Weed 1998; McGrenere and Moore 2000] but has undergone minimal evaluation. Based on our research it appears to have construct validity. One of the most interesting observations is that while there were no substantial differences between the feature-keen and the feature-shy participants in terms of how they used the two interfaces and how they approached the task of personalizing, there were a number of differences observed in terms of the self-reported measures. The feature-shy felt more satisfied and experienced a greater sense of control with MSW Personal than with MSW2K, whereas there were no differences detected for the feature-keen on these measures. Further work is required to validate the instrument used to assess the individual differences and to understand how this aspect of personality relates to other well-documented personality differences. User-Assisted Personalization. In order to shift the cost benefit ratio of personalization, one needs to increase the benefit and decrease the cost. Benefit can be increased by ensuring that the personalized interface is always a "good fit" for the user, and that the cost is minimal. One way to achieve both of these is for the system to assist the user in personalizing: the system provides adaptation suggestions based on usage information and allows the user to accept or reject the suggestions. This moves the design in the direction of user-assisted personalization that relies on user-modeling technology (this was sometimes called user-controlled self adaptation in the early literature [Dieterich et al. 1993] and more recently mixed-initiative interaction [Horvitz 1999]). The advantage of this approach is that the user retains ultimate control and the system does the bookkeeping to manage the knowledge of function usage and changing patterns in usage, a task at which the system is particularly adept. This approach has been investigated by others, for example, in Flexcel [Krogsoeter et al. 1994] and the adaptive toolbar [Debevc et al. 1996], both with mixed user testing results. This research was conducted over 10 years ago but has not been commercialized. It has recently resurfaced in the research community [Lim et al. 2005; Miah et al. 1997], suggesting that further exploration of mixed-initiative interaction is underway. Key aspects of ongoing research will be to inform how and when to provide personalization suggestions. In terms of when, we know from the current study that there was both a strong initial trigger to add functions, and a need to amortize the cost of customizing an immediately-needed function by, at the same time, adding functions that are expected to be used in the future. These trigger points would be naturally occuring user behaviors upon which user-assisted personalization research could build. Other Scenarios for Multiple Interfaces. We believe the concept of multiple interfaces has potential beyond the level-structured design seen today in some commercial applications. A possible scenario of use for multiple interfaces is to support users making the transition to a new version of an application; for example, MSW2K could include the MSW 97 interface. Often users delay upgrading their software because of the time required to learn a new version. By allowing users to continue to work in their old interface with single-button access to a new interface, users would be able to transition at a self-directed pace. Multiple interfaces might also be used to mimic a competitor's interface in the hopes of attracting new customers; for example, MSWord could offer the full interface of a different word processor such as WordPerfect (or vice-versa) with a single button click, in order to support users making the transition to a new product. In all three scenarios, help facilities could take advantage of the fact that both interfaces are accessible to show the user how functions in one interface map to functions in the second interface. This is a good example of how the adaptable nature of a multiple interface design leaves the user more in control of the interface. We believe this is especially important during critical transitions such as from novice to experienced user, from one version of a product to the next, and from one product to a competing or replacement product. There are of course other interface differences beyond menus and toolbars that need to be considered for the multiple-interface paradigm. This too is an area for future work.
