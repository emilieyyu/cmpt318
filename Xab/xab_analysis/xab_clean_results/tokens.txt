delay
and
can
immediately
of
updates
also
a
common
disadvantage
updated
to
design
or
reply
to
the
request
of
cscasr
when
a
sender
generates
bursts
pattern
the
advantage
over
abcast
is
even
greater
on
the
part
of
the
table
being
it
is
not
hard
to
using
cbcast
is
that
the
sender
needs
mutual
exclusion
our
experience
suggests
that
if
mutual
exclusion
a
single
locking
however
applications
have
strong
benefits
may
suffice
to
have
this
property
operation
for
a
whole
series
structuring
the
data
of
multicasts
and
in
some
case
locking
can
be
entirely
avoided
just
by
the
appropriate
application
itself
this
translates
to
a
huge
benefit
for
many
asynchronous
presented
in
bss91
the
distinction
settings
between
causal
and
total
event
orderings
system
as
seen
in
the
performance
cbcast
and
abcast
have
parallels
a
causal
delivery
ordering
in
other
as
part
of
although
isis
was
the
first
distributed
to
enforce
isis
applications
locks
are
used
primarily
for
mutual
exclusion
on
possibly
conflicting
operations
such
as
updates
on
related
data
items
in
the
case
of
replicated
data
this
results
in
an
algorithm
similar
to
primary
copy
update
in
which
the
primary
copy
changes
dynamically
the
execution
model
is
non-transactional
thesis
no
need
for
read-locks
or
for
two-phase
locking
rule
this
is
discussed
further
in
sec
7
19
a
communication
time
moreover
subsystem
bir85
the
approach
draws
on
lampon
s
respects
anticipated
prior
work
on
logical
notions
of
replication
the
approach
bhg87
was
in
some
similarly
software
by
work
on
primary
copy
both
to
lamport
s
in
database
approach
systems
close
synchrony
is
related
state
machine
discussed
to
developing
distributed
sclo0
architecture
and
to
the
database
serializability
has
yielded
a
memory
model
further
below
work
on
a
parallel
processor
update
model
called
weak
in
the
cache
-dsb86
th90
which
uses
a
causal
dependency
and
a
causal
abhn91
principle
to
increase
the
parallelism
of
a
parallel
processor
memory
discussion
multiprocessors
property
has
been
used
in
work
on
lazy
database
systems
jb89
lls90
a
more
detailed
in
sch88
bj89
and
distribution
of
the
conditions
under
which
can
be
used
in
place
of
ai
vr
summary
of
benefits
due
to
virtual
synchrony
brevity
precludes
algorithms
a
more
detailed
discussion
of
virtual
synchrony
or
how
it
is
used
in
developing
the
benefits
of
the
model
distributed
within
isis
however
it
may
be
useful
to
summarize
allows
code
to
be
developed
supports
a
meaningful
assuming
a
simplified
closely
synchronous
execution
model
replicated
notion
of
group
state
and
state
transfer
both
when
groups
manage
is
dynamically
partitioned
among
group
members
data
and
when
a
computation
asynchronous
treatment
pipelined
communication
process
group
membership
changes
and
failures
through
a
single
of
communication
execution
model
event-oriented
failures
handling
through
a
consistent
subsystem
this
is
in
contrast
presented
system
membership
list
integrated
with
the
communication
to
the
usual
approach
of
sensing
failures
through
timeouts
and
channels
breaking
which
would
not
guarantee
consistency
the
approach
also
has
limitations
reduced
availability
during
lan
partition
failures
only
allows
progress
in
a
single
partition
and
requires
that
a
majority
of
sites
be
available
in
that
partition
risks
incorrectly
classifying
an
operational
site
or
process
as
faulty
the
virtual
synchrony
theoretical
model
is
unusual
in
offering
these
benefits
within
a
single
framework
moreover
evade
arguments
exist
that
no
system
that
provides
consistency
our
experience
distributed
behavior
can
completely
these
limitations
has
been
that
the
issues
addressed
applications
and
that
the
approach
by
virtual
synchrony
is
general
complete
are
encountered
and
theoretically
in
even
the
simplest
distributed
system
the
isis
toolkit
provides
a
collection
and
implementing
group-based
of
higher-level
for
forming
and
managing
process
groups
of
the
approach
by
discussing
of
a
distributed
database
software
this
section
illustrates
the
specifics
the
styles
of
group
supported
by
the
system
and
giving
application
a
simple
example
isis
is
not
the
first
system
to
use
process
groups
as
programmers
developed
cheriton
s
v
system
had
received
wide
visibility
cz83
more
recently
group
mechanisms
the
chorus
operating
system
become
common
exemplified
by
the
ameoba
system
kthb89
system
developed
raa
88
ibm
s
aas
the
psync
system
pbs
9
system
cdg0
a
high
availability
by
ladin
and
liskov
llsg0
and
transis
adkm91
nonetheless
solutions
isis
was
first
to
propose
the
virtual
to
a
wide
variety
of
problems
synchrony
model
the
high
performance
approach
is
now
gaining
through
its
toolkit
wide
acceptance
5
1
styles
of
groups
styles
of
groups
of
a
distributed
system
is
limited
by
the
information
this
was
a
consideration
simplicity
in
developing
available
to
the
protocols
employed
for
efficient
communication
the
isis
process
group
interface
of
accurate
where
trade-off
information
about
had
to
be
made
between
group
membership
introduces
of
the
interface
and
the
availability
as
a
consequence
interact
for
use
in
multicast
address
expansion
the
isis
application
interface
in
four
styles
of
process
groups
that
differ
in
how
processes
writing
our
group
is
working
with
the
software
with
the
group
illustrated
of
a
new
version
that
the
time
of
technology
foundation
which
on
integration
into
mach
the
osf
1/ad
version
and
with
unix
international
plans
a
reliable
group
mechanism
for
ui
atlas
fig
8
anonymous
groups
are
not
distinguished
from
explicit
groups
this
level
of
the
system
isis
is
optimized
to
detect
and
handle
each
of
these
cases
efficiently
peer
groups
these
arise
where
a
set
of
p
cooperate
closely
for
example
to
replicate
data
the
membership
is
often
used
as
an
input
to
the
algorithm
used
in
handling
requests
as
for
the
concurrent
earlier
with
any
group
given
the
group
s
name
and
of
a
group
will
multicast
to
it
repeatedly
better
database
search
described
client-server
groups
in
isis
any
process
can
communicate
however
if
a
non-member
appropriate
permissions
performance
to
optimize
diffusion
group
is
obtained
by
lust
registering
the
group
addressing
protocol
the
sender
as
a
client
of
the
group
a
diffusion
group
is
a
client-server
group
in
which
the
clients
register
themselves
but
in
which
the
members
of
the
group
send
messages
hierarchical
groups
a
hierarchical
application
to
the
full
client
set
and
the
clients
are
passive
sinks
component
groups
the
group
is
a
structure
built
from
multiple
that
use
the
hierarchical
group
initially
reasons
of
scale
are
subsequently
contact
its
root
group
but
be
redirected
to
one
of
the
constituent
subgroups
group
data
would
normally
be
partitioned
among
the
subgroups
of
the
hierarchy
the
most
common
although
tools
are
provided
for
multicasting
communication
pattern
involves
interaction
with
the
full
membership
between
a
client
and
the
members
of
some
subgroup
there
is
no
requirement
executed
that
the
members
of
a
group
be
identical
or
even
coded
in
the
same
language
and
an
individual
or
on
the
same
architecture
moreover
multiple
groups
can
be
overlapped
process
can
belong
to
as
many
as
several
hundred
different
groups
although
scaling
is
discussed
further
below
this
is
uncommon
5
2
the
toolkit
interface
as
noted
earlier
pipelining
the
performance
of
a
distributed
system
of
asynchronous
less
efficient
is
often
limited
by
the
degree
of
communication
problems
can
be
tricky
and
for
this
reason
the
toolkit
paradigms
these
tool
by
achieved
the
development
solutions
to
distributed
than
risk
errors
many
isis
users
would
rather
employ
includes
asynchronous
implementations
solutions
of
the
more
important
distributed
programming
include
a
synchronization
for
managing
tool
that
supports
a
form
of
locking
based
on
distributed
tokens
a
replication
primary-backup
server
design
that
load-balances
replicated
data
a
tool
for
fault-tolerant
making
different
group
members
act
as
the
primary
for
different
programming
requests
and
so
forth
a
partial
list
appears
in
isis
manual
even
non-experts
distributed
software
in
table
i
using
these
tools
and
following
have
been
successful
in
developing
examples
fault-tolerant
highly
asynchronous
process
groups
create
delete
join
transferring
cbcast
abcast
collecting
state
0
1
quorum
or
all
replies
0
replies
gives
an
group
multicast
asynchronous
multicast
synchronization
locking
with
symbolic
strings
to
represent
locks
deadlock
detection
or
avoidance
must
be
addressed
at
the
application
level
token
passing
replicated
processes
data
implemented
by
broadcasting
updates
to
group
having
copies
dynamic
spooling
system
reconfiguration
transfer
values
to
using
replicated
that
join
using
state
transfer
facility
data
in
an
update
facility
logging
configuration
monitoring
for
state
recovery
after
failure
monitor
watch
a
process
a
site
trigger
actions
site
failures
and
etc
after
failures
and
recoveries
changes
to
process
group
membership
distributed
multiple
automated
execution
facilities
redundant
computation
primary/backup
all
take
the
same
action
subdivided
among
servers
coordinator-cohort
recovery
when
site
recovers
program
automatically
restarted
if
first
to
recover
state
loaded
from
logs
or
initialized
by
state
join
active
process
group
and
transfer
wan
communication
reliable
long-haul
message
passing
and
file
transfer
facility
the
example
the
procedures
messages
is
in
standard
c
the
server
initializes
main
loop
in
isis
and
declares
incoming
that
will
handle
update
and
inquiry
requests
dispatches
to
these
procedures
generation
as
needed
other
styles
of
main
loop
are
also
supported
scanning
is
specific
to
the
c
interface
the
formatted-i/o
is
not
style
of
message
available
where
type
information
at
runtime
the
current
contents
of
the
database
to
a
server
that
an
existing
server
the
state
transfer
routines
are
concerned
with
sending
has
just
been
started
and
is
joining
the
group
in
this
situation
do
a
state
transfer
invoking
will
cause
to
an
invocation
the
message
its
state
sending
of
its
state
isis
arbitrarily
selects
each
call
that
this
procedure
makes
side
in
our
example
on
the
receiving
the
latter
simply
passes
and
update
to
the
update
procedure
the
same
message
format
is
used
by
send
state
it
is
possible
of
course
there
are
many
variants
on
this
basic
scheme
for
example
only
certain
servers
to
join
and
so
forth
the
client
program
does
a
pg
lookup
to
find
the
server
should
be
allowed
to
handle
to
indicate
to
the
system
that
processes
state
transfer
requests
to
refuse
to
allow
certain
calls
to
its
query
and
update
subsequently
procedures
are
mapped
into
messages
to
timeserver
here
are
some
programming
examples
on
how
isis
handles
incoming
sql
queries
for
the
group
-
abcast
in
this
case
the
database
server
of
figure
9
uses
a
redundant
style
of
execution
request
and
will
receive
multiple
identical
replies
from
all
copies
in
which
the
client
broadcasts
in
practice
each
client
will
wait
for
the
reaction
to
failure
but
reply
and
ignore
all
others
such
an
approach
provides
the
fastest
possible
solution
the
disadvantage
of
consuming
n
times
the
sources
of
a
fault-intolerant
process
group
an
alternative
would
have
been
to
subdivide
the
search
so
that
each
server
performs
the
work
here
the
client
would
combine
fails
instead
of
replying
a
condition
isis
interfaces
have
been
developed
exist
for
unix-workstations
responses
from
all
the
servers
in
isis
repeating
the
request
if
a
server
readily
detected
for
c
c
fortran
common
lisp
ada
and
smalltalk
from
all
major
vendors
and
ports
of
isis
and
mainframes
as
well
as
for
mach
chorus
isc
and
is
represented
in
the
sco
unix
the
dec
vms
system
binary
format
used
by
the
sending
if
necessary
automatically
and
honeywell
s
machine
lynx
os
data
within
messages
and
converted
to
the
format
of
the
destination
upon
reception
transparently
6
who
uses
isis
and
how
this
section
briefly
reviews
several
isis
applications
looking
at
the
roles
that
isis
plays
6
1
brokerage
a
number
of
introduction
underlying
isis
users
are
concerned
figure
11
illustrates
employed
with
financial
computing
systems
such
as
the
one
cited
in
the
perspective
in
which
groups
one
such
a
system
now
seen
from
an
internal
by
the
broker
become
streams
of
data
evident
the
services
the
architecture
is
a
client-server
in
which
the
servers
filter
and
analyze
and
reorganize
themselves
so
that
service
is
not
interrupted
fault-tolerance
here
refers
to
two
very
different
specific
aspects
of
the
application
first
financial
systems
must
rapidly
restart
after
failed
components
or
hardware
failures
second
software
system
functions
rebooting
that
require
fault-tolerance
at
the
level
of
files
or
database
such
as
a
guarantee
that
after
isis
was
designed
a
file
or
database
manager
will
be
able
to
recover
local
data
files
at
low
cost
to
address
the
first
sort
of
problem
but
includes
several
tools
for
solving
the
latter
one
generally
information
periods
the
approach
taken
is
to
represent
key
services
using
process
groups
replicating
service
state
so
that
even
if
one
server
process
fails
the
other
can
respond
to
requests
on
its
behalf
when
n
service
programs
are
operational
one
can
often
exploit
the
redundancy
to
improve
response
must
pay
for
fault-tolerance
begins
to
outweigh
failures
the
benefit
is
time
thus
rather
than
asking
how
much
such
an
application
pilate
questions
concurrency
something
concern
the
level
of
replication
acceptable
at
which
the
overhead
assuming
and
the
minimum
performance
approach
k
component
fault-tolerance
of
a
side-effect
of
the
replication
computing
a
significant
communication
theme
in
financial
primitives
is
the
use
of
a
subscription/publication
style
the
basic
isis
numbing
over
the
do
not
spool
messages
for
future
replay
hence
an
application
ti
s
functionality
a
dynamically
predicting
varying
collection
system
the
news
facility
has
been
developed
a
final
aspect
of
brokerage
systems
to
support
that
service
a
firm
may
work
with
dozens
or
hundreds
of
financial
models
instruments
needed
to
be
waded
under
varying
market
market
behavior
for
the
financial
will
be
only
a
small
subset
consists
of
a
processor
of
these
services
at
any
time
thus
systems
conditions
of
this
sort
generally
pool
on
which
services
execution
and
load
a
remote
can
be
started
as
necessary
balancing
mechanism
and
this
creates
a
need
to
support
an
automatic
of
typical
network
complicates
the
heterogeneity
by
introducing
or
require
special
pattern
matching
processors
i
e
certain
programs
may
be
subject
to
licensing
for
some
specific
hardware
configuration
described
or
may
simply
have
been
compiled
this
problem
is
solved
using
the
isis
network
resource
manager
an
application
later
in
this
section
6
2
database
replication
and
database
triggers
although
the
isls
computation
model
differs
from
a
transactional
model
see
also
sec
7
isis
is
useful
in
constructing
distributed
database
applications
in
fact
many
as
half
of
the
applications
as
with
which
we
are
familiar
are
concerned
with
this
problem
focus
on
replicating
a
database
for
fault-tolerance
the
database
system
or
to
support
need
not
be
typical
uses
of
isis
in
database
applications
concurrent
searches
for
improved
performance
in
such
an
architecture
that
aware
isis
is
present
database
clients
access
the
database
through
a
layer
of
software
that
multicasts
updates
servers
are
supervised
to
the
set
of
servers
by
a
process
while
issuing
queries
directly
to
the
least
loaded
server
clients
of
load
changes
in
the
server
updates
isis
supervises
the
restart
of
a
failed
server
from
a
checkpoint
and
log
of
subsequent
addresses
it
is
interesting
to
realize
that
even
such
an
unsophisticated
need
among
database
users
would
require
extending
standards
beyond
database
replication
approach
to
database
replication
support
addresses
a
widely
perceived
such
as
this
long
run
of
course
comprehensive
execution
for
applications
isis
to
support
a
transactional
model
and
to
implement
the
xa/xopen
isis
users
have
developed
by
monitoring
wan
databases
by
placing
a
local
database
system
traffic
on
a
lan
updates
of
importance
to
each
lan
in
a
wan
system
remote
users
can
be
intercepted
monitors
for
incoming
control
the
update
and
distributed
through
the
isis
wan
architecture
to
avoid
costly
updates
and
applies
developers
send
them
to
the
database
server
as
necessary
concurrency
problem
of
applications
such
as
these
normally
partition
the
database
so
that
the
data
associated
with
each
lan
is
directly
updated
only
from
within
that
lan
on
remote
lan
s
for
many
applications
a
trigger
is
a
query
that
if
such
data
can
only
be
queried
and
could
be
stale
but
this
is
still
sufficient
a
final
use
of
isis
in
database
is
incrementally
specified
evaluated
becomes
settings
is
to
implement
database
triggers
against
the
database
as
updates
true
for
example
position
exceeds
condition
occur
causing
some
action
immediately
to
be
sounded
a
broker
might
request
that
an
alarm
some
threshold
as
data
enters
the
financial
database
by
the
brokerage
such
a
query
would
be
evaluated
repeatedly
the
role
of
isis
is
in
providing
programs
tools
for
reliably
notifying
capable
applications
when
such
a
trigger
becomes
enabled
and
for
developing
of
taking
the
desired
actions
despite
failures
6
3
major
isis
based
utilities
in
the
above
subsection
we
alluded
to
some
of
the
fault-tolerant
utilities
that
have
been
built
over
isis
there
are
currently
five
such
systems
news
this
application
supports
a
collection
of
communication
topics
to
which
users
can
subscribe
with
file-system
address
style
a
replay
of
recent
postings
or
post
messages
topics
are
identified
using
and
it
is
possible
to
post
to
topics
on
a
remote
network
a
mail
notation
thus
a
swiss
brokerage
application
of
messages
joins
firm
might
post
some
quotes
to
/geneva/quot
/ibm
new-york
it
creates
a
process
group
for
each
topic
monitoring
posted
to
it
for
replay
to
new
subscribers
each
such
group
to
maintain
a
history
using
a
state
transfer
when
a
new
member
nmgr
this
program
manages
this
involves
monitoring
batch-style
jobs
and
performs
load
sharing
in
a
distributed
into
a
processor
pool
setting
and
the
candidate
machines
which
are
collected
scheduling
on
the
pool
job
machines
are
suitable
one
this
criteria
can
readily
be
opposed
failed
to
run
batch-style
parallel
a
pattern
matching
mechanism
is
used
for
job
placement
for
a
given
job
a
criteria
based
on
load
and
available
memory
be
changed
when
employed
to
manage
each
service
critical
is
used
to
select
services
as
system
jobs
the
program
monitors
make
is
an
example
application
and
automatically
program
restarts
that
uses
components
of
a
distributed
application
nmgr
for
job
placement
this
system
compiles
by
fanning
out
compilation
subtasks
to
compatible
sbm89
provides
fault-tolerant
nfs-compatible
file
storage
replicates
files
are
replied
for
both
to
increase
performance
tolerance
the
level
of
replication
meantime
is
varied
depending
on
the
style
of
access
detected
any
files
managed
are
automatically
by
the
system
after
a
failed
node
recovers
file
replication
brought
up
to
date
the
file-system
reactive
control
interface
applicator
or
approach
conceals
mera/lomrra
lions
mcwb91
environment
monitored
from
the
user
who
sees
a
compatible
system
for
building
fault-tolerant
meta
is
an
extensive
woo91
it
consists
of
a
layer
for
instrumenting
a
sensor
a
distributed
application
by
defining
sensors
by
the
system
sensors
and
actuators
an
actuator
is
any
entity
capable
of
taking
an
action
the
status
of
software
user-defined
is
any
typed
value
that
can
be
polled
on
request
built-in
sensors
include
the
load
on
a
machine
and
the
set
of
users
on
each
machine
the
raw
sensors
layer
and
hardware
components
of
the
system
and
actuators
extend
this
initial
set
sensors
by
an
intermediate
facility
this
layer
sensors
such
actuators
of
the
lowest
layer
are
mapped
to
abstract
a
simple
database-style
interface
which
also
supports
an
entity-relation
fluency
and
a
triggering
supports
as
polling
data
model
and
fault-tolerance
and
conceals
sensors
many
of
the
details
of
the
physical
can
be
aggregated
for
example
by
taking
the
average
load
on
the
servers
that
manage
a
replicated
language
which
will
initiate
a
pre-specified
database
the
interface
supports
a
simple
trigger
is
detected
terms
called
action
when
a
specified
for
specifying
control
condition
running
over
mm
a
is
a
distributed
language
lomrra
lomrra
code
is
embedded
actions
in
a
high-level
interpreter
triggered
into
the
unix
csh
command
at
runtime
lomrra
by
events
that
can
control
statements
is
expanded
into
distributed
finite
state
machines
local
to
a
sensor
or
system
components
a
process
group
is
used
to
implement
when
a
monitored
condition
aggregates
perform
these
state
transition
and
to
notify
applications
and
for
saving
messages
this
subsystem
arises
mb90
and
is
responsible
for
wide-area
communication
it
conceals
to
groups
that
are
only
active
periodically
communication
interface
link
failures
present
an
exactly-once
6
4
other
isis
applications
although
this
section
covered
a
variety
of
isis
applications
over
the
system
in
addition
a
systematic
review
of
the
full
range
of
sorwate
that
has
been
developed
to
the
problems
cited
above
isis
has
been
applied
to
systems
reliable
to
telecommunications
replacement
switching
for
the
aegis
and
intelligent
aircraft
tracking
networking
and
combat
applications
engagement
military
systems
system
medical
control
weather
such
as
a
proposed
graphics
and
virtual
reality
applications
management
and
resource
scheduling
seismology
for
shared
factory
automation
facilities
and
production
and
a
wide-area
popular
computing
prediction
computing
and
storm
tracking
system
at
laboratories
1oh93
thog0
asc92
isis
has
also
proved
for
scientific
as
a
beam
such
as
cern
and
los
alamos
accelerator
and
has
been
applied
to
such
problems
that
combine
a
highly
parallel
focusing
system
for
a
particle
with
a
vectorized
management
it
should
architecture
atmospheric
a
weather-simulation
ocean
model
and
resource
model
and
displays
output
on
advanced
graphics
workstations
facilities
on
lan
issues
software
for
shared
supercomputing
that
although
also
be
noted
the
paper
has
focused
compose
isls
also
supports
a
wan
cited
and
has
been
used
in
wans
as
lan
solutions
of
up
to
ten
lans
many
of
the
applications
by
a
reliable
but
less
responsive
wan
layer
above
are
structured
interconnected
7
isis
and
other
distributed
computing
technologies
our
discussion
has
overlooked
next
generation
issues
that
arise
in
the
advanced
automation
system
casd85
which
also
uses
a
process-group
model
compares
the
sorts
of
real-time
air-traffic
similarly
one
might
wonder
how
the
isis
execution
models
unfortunately
these
are
complex
technology
issues
like
the
one
used
in
aas
differs
from
isis
in
providing
strong
real-time
control
based
computing
system
being
developed
with
transactional
model
by
ibm
for
the
faa
cd90
it
would
be
difficult
to
do
justice
database
execution
to
them
without
a
lengthy
proof
briefly
a
process
that
experiences
a
timing
fault
in
the
aas
model
could
receive
messages
because
the
criteria
for
accepting
violations
or
rejecting
if
faulty
of
such
will
reject
or
reject
messages
guarantees
provided
that
timing
assumptions
this
can
lead
to
consistency
resynchronized
it
from
initiating
others
accept
uses
the
value
of
the
local
clock
e
g
could
the
clock
is
subsequently
spread
nothing
that
other
processes
a
message
is
transient
a
process
accept
deadline
prevents
with
other
clocks
moreover
the
inconsistency
which
other
processes
will
be
maintained
isis
on
the
other
hand
guarantees
that
consistency
will
be
achieved
the
relationship
and
transactional
offered
between
isis
and
transactional
are
order-based
focus
on
isolation
of
concurrent
transactions
but
not
that
real-time
delivery
however
where
the
tools
persistent
between
data
and
rollback
abort
mechanisms
members
of
groups
failure
handling
by
a
database
system
from
one
another
in
the
fact
that
both
virtual
synchrony
bhg87
systems
execution
originates
models
those
offered
in
isis
are
concerned
with
direct
cooperation
reconfigure
and
ensuring
that
a
system
can
dynamically
itself
to
make
serializability
the
wan
architecture
of
isis
ii
similar
to
the
wan
structure
bet
because
wan
partition
are
more
common
encourages
a
more
synchronous
programming
style
wan
communication
and
link
state
is
logged
to
disk
files
unlike
wan
communication
wan
issues
are
which
enables
isis
to
retransmit
messages
lost
when
wan
partition
to
suppress
duplicate
messages
to
discuss
in
more
detail
in
mb90
persistency
of
data
is
a
big
issue
in
database
systems
but
a
commit
of
a
multicast
forward
progress
when
partial
failures
occur
is
a
form
of
reliable
multicast
while
delivery
much
less
so
in
isis
for
example
serializability
and
permanence
of
the
transaction
being
committed
in
isis
provides
much
weaker
guarantees
8
conclusions
we
have
argued
that
the
next
generation
of
distributed
computing
systems
semantics
exceed
will
benefit
from
support
for
process
groups
and
group
programming
arriving
the
abilities
or
the
reliability
would
be
a
difficult
problem
application
performance
development
and
implementing
those
semantics
for
a
process
group
mechanism
of
many
distributed
systems
either
the
operating
system
must
implement
applications
of
group-structured
is
unlikely
to
be
acceptable
with
process
groups
the
isis
system
provides
tools
for
programming
leads
us
to
the
following
conclusions
process
synchronized
groups
should
embody
strong
semantics
a
simple
and
powerful
communication
distributed
for
group
membership
and
synchronous
execution
is
a
synchronized
model
can
be
based
on
closely
but
high
performance
heavily
pipelined
the
virtual
synchrony
approach
in
which
communication
using
a
closely
style
of
execution
combines
these
benefits
synchronous
execution
safely
be
relaxed
efficient
protocols
non-experts
model
but
deriving
a
substantial
performance
benefit
when
message
ordering
can
have
been
developed
for
supporting
virtual
synchrony
this
paper
is
being
written
as
the
first
phase
of
the
isis
effort
approaches
system
has
demonstrated
achieves
levels
the
feasibility
of
a
new
style
of
distributed
to
those
afforded
a
resulting
system
relatively
easy
to
use
we
feel
that
the
initial
in
bss91
isis
computing
suitable
for
integration
incorporate
on
the
same
platforms
as
reported
of
performance
comparable
by
standard
technologies
rpc
and
streams
looking
to
the
future
we
are
now
developing
operating
rbg92
systems
an
isis
microkernel
into
next-generation
a
security
architecture
such
as
mach
and
chorus
this
new
system
will
be
a
real-time
communication
that
operate
on
distributed
to
implement
high-reliability
suite
the
programming
model
will
be
unchanged
group
programming
could
ignite
a
wave
of
advances
platforms
in
reliable
distributed
computing
and
of
applications
developers
using
current
technologies
it
is
impractical
to
employ
software
self-managing
or
to
develop
software
for
typical
replicated
after
distributed
systems
that
reconfigures
data
or
simple
coarse-grained
parallelism
automatically
failure
or
recovery
consequently
although
current
resources
deficient
distributed
the
programmers
are
severely
constrained
software
who
develop
software
infrastructure
by
removing
these
unnecessary
environments
obstacles
embody
tremendously
powerful
networks
computing
a
vast
groundswell
of
reliable
application
development
can
be
unleashed
model-based
evaluation
of
expert
cell
phone
menu
interaction
robert
st
amant
and
thomas
e
horton
north
carolina
state
university
and
frank
e
ritter
the
pennsylvania
state
university
we
describe
concepts
to
support
the
analysis
of
cell
phone
menu
hierarchies
based
on
cognitive
models
of
users
and
easy-to-use
optimization
techniques
we
present
an
empirical
study
of
user
performance
on
five
simple
tasks
of
menu
traversal
on
an
example
cell
phone
two
of
the
models
applied
to
these
tasks
based
on
goms
and
act-r
give
good
predictions
of
behavior
we
use
the
empirically
supported
models
to
create
an
effective
evaluation
and
improvement
process
for
menu
hierarchies
our
work
makes
three
main
contributions
a
novel
and
timely
study
of
a
new
very
common
hci
task
new
versions
of
existing
models
for
accurately
predicting
performance
and
a
search
procedure
to
generate
menu
hierarchies
that
reduce
traversal
time
in
simulation
studies
by
about
a
third
1
introduction
there
are
2
billion
cellular
telephones
in
use
today
and
this
number
is
expected
to
reach
3
billion
in
2008
diprima
2006
cell
phones
are
used
for
more
than
making
calls
they
now
include
tools
for
managing
contact
information
voice
mail
and
hardware
settings
and
often
software
for
playing
games
browsing
the
web
and
connecting
to
specialized
information
services
the
market
penetration
of
cell
phones
is
much
higher
than
that
of
conventional
computers
which
raises
significant
opportunities
and
challenges
for
hci
this
article
presents
techniques
for
evaluating
and
improving
cell
phone
usability
in
particular
the
usability
of
the
hierarchical
menus
that
provide
access
to
most
functionality
aside
from
dialing
and
data
entry
while
cell
phone
menu
interfaces
may
appear
simple
at
first
glance
they
pose
a
nontrivial
design
problem
consider
the
menu
hierarchy
for
the
kyocera
2325
cell
phone
the
first
25
items
of
which
are
shown
in
table
i
if
we
count
as
terminals
those
selections
that
open
an
application
e
g
a
game
a
list
of
data
e
g
recent
calls
or
a
set
of
choices
in
the
cell
phone
equivalent
of
a
dialog
box
e
g
for
setting
the
ringer
volume
then
this
hierarchy
contains
98
terminals
reachable
through
22
intermediate
selections
the
longest
menu
contains
12
items
all
associated
with
the
selection
of
different
sounds
the
shortest
menu
contains
a
single
item
for
entering
a
voice
memo
terminals
in
the
hierarchy
are
up
to
four
levels
deep
and
the
mean
number
of
actions
to
reach
an
item
scrolling
plus
selection
over
all
98
terminals
is
13
3
taking
on
the
order
of
7
seconds
for
an
experienced
user
this
menu
hierarchy
is
as
large
as
that
of
a
moderately
sized
desktop
application
e
g
eudora
5
2
with
103
items
this
is
not
unusual
for
cell
phones
the
menu
hierarchy
for
the
samsung
mm-a800
which
includes
a
digital
camera
contains
a
remarkable
583
items
pogue
2005
designing
menu
systems
for
any
platform
including
desktop
systems
can
be
challenging
but
for
cell
phones
the
problem
is
made
more
difficult
by
several
factors
--
discrete
selection
actions
in
the
form
of
button
presses1
are
usually
needed
to
move
between
menu
items
because
most
cell
phones
lack
more
direct
selection
capabilities
e
g
a
mouse
or
touch
screen
--
cell
phone
displays
are
small
allowing
only
a
few
menu
items
to
be
displayed
at
a
single
time
many
cell
phones
lack
functionality
for
paging
up
or
down
making
display
limitations
even
more
significant
--
there
is
less
standardization
in
hardware
supporting
menu
traversal
for
cell
phones
than
for
desktop
machines
some
phones
have
two-way
directional
buttons
others
four-way
some
have
a
labeled
menu
button
while
others
rely
on
a
button
with
overloaded
functionality
button
placement
can
vary
significantly
with
cancel
and
ok
buttons
reversed
from
one
phone
to
another
if
interfaces
are
developed
for
the
lowest
common
denominator
independently
of
specific
hardware
which
is
common
practice
at
the
mobile
application
level
then
even
cell
phones
with
elaborate
interaction
support
become
less
efficient
1
we
use
the
terms
button
presses
and
key
presses
interchangeably
these
factors
suggest
that
cell
phone
menu
interfaces
deserve
close
analysis
and
that
they
need
specialized
techniques
for
their
development
and
evaluation
which
this
article
takes
up
in
two
parts
in
section
2
we
describe
an
empirical
study
of
the
traversal
of
cell
phone
menus
along
with
three
models
for
predicting
user
performance
a
fitts
law
model
fitts
1954
a
goms
model
john
and
kieras
1996a
1996b
kieras
1999
and
an
act-r
model
anderson
et
al
2004
all
the
models
give
good
accounts
of
qualitative
patterns
in
user
behavior
and
the
latter
two
models
give
good
to
very
good
quantitative
predictions
of
behavior
at
both
aggregate
and
detailed
levels
of
analysis
in
section
3
we
use
our
empirical
results
to
define
a
novel
evaluation
metric
for
the
efficiency
of
cell
phone
menu
traversal
we
define
a
search
procedure
that
generates
improvements
to
a
menu
hierarchy
with
respect
to
a
given
set
of
characteristic
user
profiles
this
article
makes
several
contributions
to
the
hci
literature
a
novel
and
timely
study
of
a
very
common
new
hci
task
menu
use
on
cell
phones
new
models
for
accurately
predicting
performance
on
this
task
and
a
simple
theoretically
motivated
search
procedure
that
generates
menu
hierarchies
that
reduce
traversal
time
in
simulation
studies
by
a
third
which
should
be
generally
applicable
to
all
menu-based
systems
2
a
performance
study
our
interest
is
in
expert
i
e
practiced
and
error-free
use
of
cell
phone
menu
systems
for
control
purposes
it
was
not
feasible
to
collect
data
from
experienced
users
on
their
own
cell
phones
with
all
the
potential
differences
in
hardware
and
software
as
a
compromise
we
had
users
practice
a
small
number
of
tasks
so
that
all
tasks
could
be
remembered
easily
and
then
carry
them
out
on
a
single
cell
phone
though
restrictive
these
conditions
give
a
reasonable
starting
point
for
an
empirical
study
and
model
validation
we
used
a
kyocera
2325
as
shown
in
figure
1
at
the
top
level
of
its
internal
menu
the
kyocera
display
shows
a
single
selectable
icon
the
ok
button
selects
the
current
item
on
the
four-way
scrolling
button
right
and
left
move
through
the
item
list
horizontally
for
lower-level
menus
three
items
are
displayed
at
a
time
oriented
vertically
each
new
menu
list
is
displayed
with
the
top
item
highlighted
the
ok
button
on
the
left
is
used
to
select
the
currently
highlighted
item
in
these
menus
while
the
clr
button
on
the
right
returns
to
the
previous
level
in
the
hierarchy
the
up
and
down
regions
of
the
four-way
button
move
through
the
menu
downward
scrolling
is
incremental
with
items
appearing
one
at
a
time
at
the
bottom
of
the
screen
2
1
procedures
we
recruited
fourteen
experienced
cell
phone
users
for
our
study
students
who
took
part
for
course
credit
the
first
two
users
acted
as
participants
in
a
pilot
phase
of
the
experiment
in
which
software
for
data
collection
and
analysis
was
tested
and
procedures
were
refined
their
data
were
also
used
in
developing
but
not
validating
the
models
described
in
later
sections
the
remaining
twelve
users
male
undergraduates
in
computer
science
provided
the
main
body
of
data
for
the
study
all
were
right
handed
all
but
one
used
their
right
hand
to
hold
the
cell
phone
and
all
used
the
thumb
of
the
right
hand
to
press
keys
to
collect
data
we
recorded
the
tone
produced
by
each
key
press
as
transmitted
through
the
earphone
jack
of
the
cell
phone
collection
was
initiated
by
the
first
key
pressed
by
the
participant
and
ended
with
the
last
key
pressed
the
onset
of
each
key
press
is
detectable
by
a
threshold
test
on
the
audio
output
waveform
from
the
earphone
jack
using
software
we
wrote
for
this
purpose
each
tone
lasts
approximately
0
095
s
during
which
time
the
display
changes
before
the
key
is
released
system
responses
are
much
faster
than
key
presses
and
are
treated
as
occurring
within
elementary
key
press
actions
and
not
contributing
to
the
duration
of
user
actions
participants
started
with
a
practice
stage
in
which
they
familiarized
themselves
with
the
cell
phone
and
its
menu
system
we
gave
each
participant
a
paper
form
describing
how
five
terminal
menu
items
were
to
be
reached
as
shown
in
the
first
column
of
table
ii
each
represents
a
scrolling
action
with
commas
separating
consecutive
selection
actions
reaching
each
of
the
terminal
items
those
at
the
end
of
each
sequence
constituted
a
task
in
the
study
participants
practiced
each
task
until
they
could
carry
it
out
three
times
in
a
row
without
error
each
trial
in
the
study
required
reaching
one
of
the
five
target
terminal
items
without
access
to
the
paper
form
tasks
were
presented
to
participants
in
a
randomized
order
we
obtained
five
correct
trials
per
participant
i
e
without
errors
or
extraneous
actions
discarding
fewer
than
10
trials
across
all
participants
less
than
3
of
the
data
this
means
that
our
cleaned
dataset
contains
only
ok
and
right/down
key
press
actions
2
280
observations
in
total
2
280
12
users
5
repetitions
10
9
3
8
8
actions
per
task
table
ii
shows
the
mean
duration
per
task
over
all
participants
in
the
study
user
performance
is
much
slower
than
for
single-level
menu
selection
with
a
mouse
on
a
standard
desktop
platform
byrne
2001
which
highlights
the
importance
of
specialized
models
for
this
task
as
we
discuss
below
2
2
models
of
user
behavior
we
predicted
user
performance
with
three
models
each
supported
by
a
considerable
background
literature
a
fitts
law
model
a
goms
model
and
an
act-r
model
were
developed
independently
of
each
other
based
on
data
from
one
task
carried
out
by
one
of
the
users
in
the
pilot
stage
of
the
experiment
2
the
three
models
run
in
the
same
software
framework
that
evolved
over
the
course
of
this
research
the
framework
provides
a
common
specification
of
the
kyocera
cell
phone
including
the
sizes
and
positions
of
keys
and
the
distances
between
them
as
measured
on
the
physical
device
the
framework
also
supports
a
common
representation
of
the
menu
hierarchy
shown
in
table
i
the
models
use
the
same
software
environment
that
includes
a
simulation
of
the
cell
phone
s
interface
and
produces
output
in
a
consistent
form
2
2
1
a
fitts
law
model
our
model
is
based
on
mackenzie
s
2003
version
of
fitts
law
for
one-finger
typing
for
text
entry
on
mobile
phones
movement
time
in
seconds
for
thumb
input
is
where
d
represents
the
distance
amplitude
of
the
movement
and
w
the
width
of
the
target
the
value
for
d
in
our
study
was
14
5
mm
which
separates
the
ok
button
and
the
down
button
area
with
widths
w
of
6
mm
and
10
mm
as
provided
by
the
cell
phone
specification
this
model
as
with
the
other
models
described
below
makes
the
simplifying
assumption
that
all
scrolling
actions
can
be
represented
by
down
key
presses
even
though
the
first
action
is
a
right
key
press
with
a
slightly
different
size
and
distance
from
the
ok
button
to
execute
the
fitts
law
model
for
each
of
the
five
tasks
a
path
is
generated
from
the
root
of
the
menu
hierarchy
to
the
terminal
item
for
the
task
each
step
on
the
path
is
associated
with
a
movement
action
or
a
key
press
action
durations
for
all
the
steps
are
accumulated
to
produce
an
overall
task
duration
2
2
2
a
goms
model
the
second
model
is
a
goms
model
kieras
1999
john
2003
goms
methods
for
task
analysis
produce
hierarchical
descriptions
of
methods
and
operators
needed
to
accomplish
goals
some
goms
models
have
been
strikingly
successful
in
critical
hci
domains
gray
et
al
1993
in
our
model
a
method
is
defined
for
each
task
in
the
study
all
of
the
methods
are
automatically
generated
from
the
menu
hierarchy
specification
based
on
the
same
path
traversals
used
for
the
fitts
law
model
within
a
method
each
step
corresponds
to
the
selection
of
a
menu
item
the
goms
method
for
selecting
the
terminal
item
ringer
volume
is
shown
at
the
top
of
figure
2
each
of
the
steps
in
this
method
in
turn
decomposes
into
a
selection
method
such
as
select
menu
or
select
sound
which
involves
scrolling
until
a
specific
item
in
the
sequence
is
reached--selection
in
a
menu
at
a
single
level
there
is
one
selection
method
for
each
menu
item
from
select
settings
to
select
ringer
volume
all
of
the
selection
methods
have
the
same
form
as
shown
in
the
example
at
the
bottom
of
figure
2
specifications
of
these
lower-level
methods
are
created
automatically
from
a
generic
template
2
preliminary
versions
of
the
goms
and
act-r
models
described
in
an
earlier
conference
paper
st
amant
et
al
2004b
contained
minor
inconsistencies
these
inconsistencies
were
removed
in
revision
performance
was
altered
by
no
more
than
a
few
percentage
points
the
qualitative
behavior
of
the
models
and
comparisons
between
them
remain
unchanged
from
their
earlier
description
processing
in
a
selection
method
involves
iterating
through
a
sequence
of
four
exhaustive
tests
of
whether
or
not
the
target
intermediate
or
terminal
item
is
currently
highlighted
and
whether
the
finger
is
on
the
appropriate
key
for
selection
or
scrolling
the
durations
of
the
steps
follow
the
guidelines
established
by
kieras
1999
in
his
work
on
gomsl
and
glean3
including
a
version
of
fitts
law
each
test
in
a
decision
step
requires
0
050
s
plus
the
time
to
execute
any
actions
in
the
body
of
the
decision
if
the
test
succeeds
steps
that
involve
key
presses
last
0
280
seconds
plus
the
duration
of
tests
or
auxiliary
operations
0
330
seconds
in
total
moving
to
the
down
key
lasts
0
083
seconds
0
133
seconds
in
total
moving
to
the
ok
key
lasts
0
113
seconds
0
163
seconds
in
total
movement
times
are
based
on
the
movement
component
of
the
fitts
law
model
in
the
previous
section
the
model
assumes
negligible
system
response
time
and
that
there
are
no
verification
steps
further
the
initial
visual
action
to
acquire
the
first
menu
item
occurs
before
the
first
key
press
timing
begins
at
the
first
key
press
and
as
the
highlighted
menu
item
changes
no
visual
re-acquisition
is
needed
during
selection
or
scrolling
activity
processing
is
entirely
sequential
with
no
overlapping
of
steps
modeling
results
based
on
the
description
above
are
generated
by
a
goms
interpreter
that
we
implemented
specifically
for
this
project
while
there
would
have
been
some
benefit
to
using
existing
goms
modeling
tools
and
environments
e
g
glean3
kieras
1999
we
judged
that
the
value
of
a
single
simulation
and
modeling
framework
despite
its
limitations
for
the
fitts
law
goms
and
act-r
models
and
the
phone
simulation
would
provide
a
worthwhile
degree
of
consistency
across
our
evaluation
2
2
3
an
act-r
model
the
third
model
is
based
on
the
act-r
5
0
cognitive
architecture
anderson
et
al
2004
we
picked
act-r
as
a
representative
cognitive
modeling
architecture
and
as
a
common
choice
in
hci
work
act-r
integrates
theories
of
cognition
visual
attention
and
motor
movement
and
has
been
the
basis
for
a
number
of
models
in
hci
e
g
ritter
and
young
2001
act-r
models
simulate
the
time
course
and
information
processing
of
cognitive
mechanisms
such
as
changes
of
attention
and
memory
retrievals
as
well
as
external
actions
such
as
movement
of
the
fingers
roughly
speaking
act-r
models
provide
details
that
can
explain
behavior
in
cognitive
terms
at
a
level
not
addressed
by
the
coarser
goms
representation
in
our
act-r
model
a
virtual
simulated
display
maintains
a
representation
of
the
current
items
in
the
cell
phone
s
menu
interface
hierarchy
menu
items
are
presented
in
a
vertical
list
and
one
of
the
menu
items
is
always
highlighted
all
items
are
presented
for
each
list
independent
of
the
physical
display
size
when
an
item
is
selected
in
the
virtual
display
the
list
is
refreshed
with
the
appropriate
submenu
the
model
s
memory
is
initialized
with
a
set
of
declarative
memory
chunks
that
represent
the
parent-child
relationships
between
the
intermediate
menu
items
needed
to
reach
terminal
items
for
example
for
the
ringer
volume
task
pairs
of
declarative
memory
chunks
for
the
menu/settings
settings/sounds
and
sounds/ringer
volume
relationships
are
included
chunks
representing
the
parent
child
relationships
are
generated
automatically
via
traversal
of
the
menu
hierarchy
specification
act-r
s
model
of
the
hand
is
initialized
with
the
thumb
on
the
ok
button
procedural
knowledge
in
the
act-r
model
consists
of
eleven
productions
--
find-top-item
searches
the
visual
field
for
a
highlighted
menu
item
immediately
after
a
selection
action
--
find-next-item
searches
for
the
next
item
below
the
one
currently
attended
immediately
after
a
scrolling
item
--
attend-item
causes
the
location
of
the
highlighted
item
to
be
visually
attended
--
encode-item
encodes
the
text
for
the
attended
menu
item
so
that
its
content
i
e
the
name
of
the
item
in
text
form
becomes
accessible
to
the
model
--
respond-select-target
fires
when
the
currently
highlighted
item
is
recognized
as
the
terminal
item
--
recall-item-association
retrieves
an
association
if
it
exists
between
the
currently
highlighted
menu
item
and
its
subordinate
item
along
the
path
to
the
terminal
item
--
respond-select-ancestor
recognizes
an
intermediate
menu
item
along
the
path
to
the
terminal
item
i
e
one
of
its
ancestors
--
respond-continue-down
fires
when
the
highlighted
item
is
neither
the
next
item
to
be
selected
nor
along
the
path
to
the
terminal
item
--
move-down
causes
the
motor
module
to
press
the
down
key
--
select-target
causes
the
motor
module
to
press
the
ok
key
on
the
terminal
menu
item
ending
model
execution
--
select-ancestor
causes
the
motor
module
to
press
the
ok
key
on
an
intermediate
menu
item
the
model
starts
with
the
goal
of
selecting
a
specific
terminal
menu
item
the
simulation
environment
shows
a
single
highlighted
item
the
model
first
retrieves
a
target
intermediate
item
to
be
selected
it
then
searches
for
the
currently
highlighted
menu
item
in
its
field
of
view
once
found
the
visible
item
is
attended
and
then
encoded
so
that
its
text
representation
becomes
accessible
if
the
text
matches
the
target
item
and
this
is
the
same
as
the
terminal
item
then
the
model
initiates
motor
actions
to
move
the
thumb
to
the
ok
button
if
necessary
and
press
it
model
execution
completes
at
this
point
if
the
text
matches
the
target
item
but
it
is
not
the
terminal
item
then
this
means
that
the
currently
highlighted
item
is
on
the
path
to
the
terminal
the
ok
button
is
pressed
and
another
target
item
is
retrieved
from
memory
visual
processing
repeats
as
before
if
the
text
of
the
highlighted
item
does
not
match
the
target
item
then
motor
actions
are
initiated
to
move
the
thumb
to
the
down
button
and
press
it
control
is
transferred
to
the
visual
search
action
as
before
in
the
model
manual
scrolling
actions
can
trail
behind
visual
processing
by
an
unspecified
amount
determined
by
processing
in
the
model
such
as
memory
retrievals
the
visual
and
manual
modules
become
synchronized
when
a
new
menu
is
presented
user
errors
such
as
pressing
an
incorrect
key
are
not
modeled
model
execution
is
deterministic
with
no
noise
parameters
used
our
model
is
defined
in
the
act-r
modeling
language
but
its
execution
depends
on
an
extension
to
perceptual-motor
processing
in
the
architecture
the
perceptual
and
motor
components
of
act-r
5
0
have
some
bias
toward
desktop
activities
such
as
selecting
menu
items
with
the
mouse
and
navigating
through
windows
and
dialog
boxes
anderson
et
al
2004
byrne
2001
in
act-r
the
keyboard
is
represented
as
an
array
of
locations
neighboring
keys
are
a
unit
distance
apart
in
a
rectilinear
arrangement
and
each
key
has
unit
width
and
height
standard
key
presses
are
modeled
as
finger
movements
from
locations
on
the
home
row
to
the
location
of
a
target
key
to
handle
interaction
with
a
cell
phone
keypad
more
flexibility
is
needed
in
models
of
finger
movements
and
the
keyboard
we
extended
the
act-r
environment
representation
to
support
a
layout-based
keypad
in
which
the
size
and
placement
of
keys
can
be
specified
individually
the
new
representation
allows
us
to
build
specifications
of
different
cell
phone
keypads
that
can
be
integrated
with
act-r
motor
processing
in
a
straightforward
way
fingers
are
modeled
as
moving
between
locations
which
in
the
case
of
this
experiment
are
key
locations
but
may
be
arbitrary
if
needed
2
3
model
performance
we
can
describe
the
performance
of
the
models
at
two
levels
the
accuracy
with
which
the
models
predict
the
overall
duration
of
tasks
and
the
accuracy
of
their
predictions
of
the
duration
of
individual
actions
these
two
levels
are
discussed
in
the
sections
below
other
factors
commonly
explored
by
modeling
such
as
learning
behavior
and
the
occurrence
of
errors
are
excluded
by
the
design
of
the
experiment
2
3
1
task-level
performance
table
iii
shows
summary
model
performance
and
user
data
broken
down
by
task
figures
3
through
7
show
more
detailed
views
of
the
same
data
in
graphical
form
both
the
goms
and
act-r
models
give
good
approximations
of
user
performance
goms
predictions
are
within
the
99
confidence
interval
for
mean
overall
task
duration
for
all
target
items
except
view
day
act-r
predictions
are
within
this
interval
for
two
of
the
target
items
the
fitts
law
model
does
less
well
for
reasons
that
are
worth
discussing
many
models
of
cell
phone
interaction
such
as
keypad
dialing
and
one-finger
text
entry
mackenzie
2003
have
been
based
on
fitts
law
which
motivated
this
aspect
of
our
evaluation
our
fitts
law
model
performs
relatively
poorly
despite
the
success
of
such
models
elsewhere
the
fitts
law
model
produces
times
that
are
about
half
of
the
observed
times
this
is
not
surprising--much
of
the
activity
of
this
menu
selection
task
is
outside
the
scope
of
the
model
silfverberg
et
al
2000
describe
a
comparable
example
of
where
fitts
law
models
break
down
in
a
discussion
of
text
entry
on
mobile
phones
for
some
cell
phones
text
entry
is
aided
by
lexicon-based
word
disambiguation
while
typing
the
user
ordinarily
refers
to
the
display
in
order
to
decide
whether
the
system
has
correctly
disambiguated
the
word
being
typed
in
text
entry
such
cognitive
processing
may
not
be
needed
by
expert
users
familiar
with
the
disambiguation
system
in
this
menu
selection
task
however
we
assume
that
users
confirm
their
actions
in
other
words
significant
visual
and
cognitive
processing
is
necessary
at
each
step
in
the
process
but
this
is
not
captured
by
the
fitts
law
model
though
it
is
represented
in
the
goms
and
act-r
models
the
consistent
linearity
of
user
performance
across
tasks
in
figures
3
through
7
suggests
a
straightforward
way
to
measure
the
predictive
power
of
the
goms
and
act-r
models
through
comparison
with
a
least
squares
linear
model
because
the
models
predictions
apply
to
each
task
a
harsh
way
to
test
them
is
to
compare
them
to
a
linear
regression
model
fit
to
data
from
each
task
table
iv
shows
the
coefficient
of
determination
r
2
for
these
linear
models
for
each
button
press
the
linear
model
predicts
time
t
by
the
number
of
key
presses
k
plus
a
constant
the
remaining
columns
show
analogous
values
for
the
act-r
and
goms
models
a
linear
model
can
also
be
fit
to
the
aggregation
of
performance
data
for
all
tasks
and
all
users
this
aggregate
linear
model
has
an
r
2
of
0
834
although
neither
the
goms
nor
the
act-r
model
accounts
for
as
much
variance
as
a
general
linear
equation
both
are
close
the
comparable
values
are
0
809
for
the
goms
model
and
0
796
for
the
act-r
model
the
aggregate
linear
model
has
appealing
conceptual
and
computational
simplicity
we
use
eq
2
in
section
3
for
just
this
reason
as
a
general
model
of
performance
though
it
has
several
shortcomings
in
comparison
with
the
goms
model
the
act-r
model
and
even
the
fitts
law
model
first
the
latter
are
a
priori
models--they
were
not
tuned
specifically
to
the
data
second
as
we
discuss
later
in
this
section
the
models
give
predictions
at
a
more
detailed
level
than
the
linear
model
can
provide
including
the
ability
to
carry
out
actions
to
produce
this
behavior
third
and
most
important
the
goms
act-r
and
fitts
law
models
have
theoretical
underpinnings
that
give
them
explanatory
power
in
the
case
of
goms
performance
is
explained
by
the
specific
tasks
that
are
represented
the
hierarchical
structure
in
which
they
are
combined
and
dependence
on
a
cognitive
processing
framework
that
provides
specific
timing
predictions
e
g
for
fitts
law
movements
the
act-r
model
extends
the
level
of
detail
in
its
explanations
in
accounting
for
the
interval
between
actions
by
explicit
visual
processing
and
memory
retrievals
and
in
modeling
visual
processing
and
motor
actions
as
proceeding
in
parallel
for
scrolling
actions
but
synchronizing
with
selection
actions
this
allows
performance
on
multiple
tasks
to
be
based
on
models
of
single
tasks
e
g
salvucci
2001
like
all
model-generated
explanations
these
are
provisional
and
subject
to
further
testing
in
particular
the
predictiveness
of
a
linear
model
raises
a
warning
flag
because
task-level
performance
accumulates
the
durations
of
actions
in
sequence
almost
any
reasonable
cumulative
function
is
likely
to
have
the
same
qualitative
shape
the
accuracy
of
the
models
over
tasks
of
different
durations
suggests
that
the
models
have
some
generality
but
this
is
far
from
conclusive
if
as
with
computing
the
purpose
of
modeling
is
insight
rather
than
numbers
hamming
1962
we
should
look
more
closely
at
our
results
2
3
2
action-level
performance
table
v
shows
the
predictions
of
the
mean
time
between
user
key
presses
that
each
model
makes
over
all
the
menu
selection
tasks
there
are
three
different
categories
all
actions
aggregated
over
all
tasks
only
selection
actions
and
only
scrolling
actions
the
actr
and
goms
models
both
provide
good
predictions
at
this
level
with
differences
of
at
most
0
045
s
about
8
error
although
the
fitts
law
model
is
qualitatively
correct
in
predicting
that
selection
actions
take
longer
than
scrolling
actions
it
underpredicts
user
reaction
time
in
all
categories--our
discussion
in
this
section
will
therefore
mainly
focus
on
the
act-r
and
goms
models
the
results
in
table
v
are
limited
in
two
ways
first
they
distinguish
only
between
classes
of
actions
in
the
abstract
independent
of
the
task
context
in
which
they
are
executed
second
the
results
neglect
the
inherent
variance
in
user
performance
even
under
identical
task
conditions
the
actions
of
different
users
or
a
single
user
in
different
trials
may
have
different
durations
we
address
these
two
limitations
in
the
remainder
of
this
section
one
way
to
describe
the
execution
of
a
menu
navigation
task
is
as
a
simple
repeated
pattern
each
task
is
carried
out
through
a
number
of
scrolling
actions
followed
by
a
selection
action
repeated
until
a
terminal
item
is
reached
we
define
a
selection
run
as
a
sequence
of
scrolling
actions
leading
up
to
a
selection
action
as
in
our
per-task
analysis
user
performance
is
basically
linear
for
the
overall
duration
of
selection
runs
and
is
well
predicted
by
both
the
goms
and
act-r
models
with
respect
to
overall
duration
what
is
more
interesting
is
differences
in
duration
for
actions
of
the
same
type
in
selection
runs
the
first
scrolling
action
after
a
selection
action
lasts
much
longer
than
subsequent
scrolling
actions
as
shown
in
figure
8
a
general
linear
model
see
eq
2
would
be
a
straight
line
at
531
ms
per
action
two
factors
appear
to
contribute
to
the
longer
duration
of
the
first
scrolling
action
one
factor
is
movement
time
in
that
for
the
first
scrolling
action
the
thumb
must
move
from
the
ok
key
to
the
down
button
movement
between
keys
is
unnecessary
for
further
scrolling
the
other
factor
is
visual
processing
each
time
that
a
selection
action
takes
place
a
new
set
of
menu
items
appears
on
the
display
and
must
be
read
all
of
the
models
include
a
movement
component
and
thus
reflect
the
general
qualitative
pattern
but
they
vary
in
how
they
handle
visual
processing
the
act-r
model
carries
out
an
explicit
visual
search
which
occurs
in
parallel
with
the
motor
movement
the
goms
model
does
not
include
an
explicit
visual
processing
step
but
each
selection
entails
decision-making
tests
as
well
as
a
call
to
a
new
method
adding
to
the
duration
of
motor
movement
both
models
produce
slight
underpredictions
of
the
duration
of
the
first
scrolling
action
an
alternative
explanation
for
this
pattern
is
offered
by
the
epic
architecture
but
is
based
on
different
low-level
cognitive
assumptions
hornof
and
kieras
1997
a
less
obvious
pattern
is
also
present
in
figure
8
for
users
the
first
scrolling
action
lasts
the
longest
the
second
the
shortest
and
all
succeeding
actions
in
between
we
believe
that
three
factors
explain
the
increase
in
duration
between
the
second
and
remaining
actions
the
first
factor
is
an
environmental
constraint
on
users
visual
processing
because
only
three
menu
items
are
presented
on
the
display
at
a
time
we
can
expect
the
duration
of
the
fourth
and
succeeding
scrolling
actions
to
be
slower
than
the
second
and
third
because
the
items
to
be
traversed
are
not
immediately
available
for
visual
processing
the
second
factor
is
a
possible
strategy
that
users
took
in
dealing
with
menus
that
are
known
via
practice
to
be
long
users
may
quickly
execute
several
scrolling
actions
with
less
attention
to
the
display
until
the
approximate
region
of
the
target
item
is
reached
the
third
factor
is
parallelism
in
visual
processing
and
motor
action
related
experiments
on
menu
selection
with
a
mouse
byrne
2001
suggest
that
eye
movement
is
not
strictly
synchronized
with
motor
activity
in
the
discrete
menu
traversal
actions
of
our
domain
this
means
that
the
eyes
may
scan
ahead
of
the
items
being
highlighted
by
button
presses
the
first
factor
is
not
reproduced
by
our
simulation
environment
and
the
second
is
not
yet
included
in
the
model
the
third
factor
parallelism
in
motor
and
visual
behavior
is
represented
in
the
act-r
model
this
parallelism
accounts
for
the
increase
in
duration
after
the
fourth
scrolling
action
as
the
duration
of
motor
actions
dominates
that
of
visual
processing
the
exact
point
in
the
user
data
at
which
the
increase
occurs
is
not
captured
by
any
of
these
models
neither
is
the
remaining
variability
in
the
duration
of
scrolling
actions
finally
we
note
that
the
cognitive
and
visual
processing
component
of
actions
in
selection
runs
is
much
higher
than
the
movement
component
the
fitt
s
law
model
provides
a
baseline
for
movement-only
duration
but
underpredicts
the
duration
of
user
actions
by
a
factor
of
two
to
three
the
goms
and
act-r
models
for
reasons
discussed
above
come
much
closer
to
user
performance
overall
the
mean
error
for
mean
action
duration
during
all
selection
runs
is
about
14
for
the
goms
model
19
for
the
act-r
model
and
60
for
the
fitts
law
model
our
results
so
far
show
that
the
goms
and
act-r
models
give
accurate
predictions
of
the
mean
duration
of
user
actions
when
actions
are
grouped
into
classes
or
as
in
the
analysis
of
selection
runs
associated
with
a
specific
task
context
these
predictions
however
give
no
information
about
the
accuracy
of
the
models
for
specific
instances
of
actions
in
evaluating
model
predictions
at
the
action
level
the
issue
of
inherent
variability
in
user
performance
is
perhaps
the
most
important
two
common
measures
of
error
that
give
insight
into
this
issue
are
root
mean
squared
error
rmse
and
mean
absolute
error
mae
table
vi
shows
rmse
and
mae
for
the
three
models
for
all
actions
and
for
the
categories
of
scrolling
and
selection
actions
the
prediction
error
for
the
goms
and
act-r
models
per
action
duration
is
much
higher
than
for
mean
duration
about
42
of
the
mean
action
duration
as
given
in
table
v
for
the
rmse
measure
and
32
for
mae
the
values
are
similar
for
the
subcategories
of
scrolling
and
selection
actions
by
these
measures
the
models
are
very
close
in
performance
with
neither
goms
or
act-r
having
an
obvious
advantage
while
these
values
of
rmse
and
mae
are
disappointingly
high
for
the
goms
and
act-r
models
it
is
worth
asking
how
much
they
might
be
improved
as
in
our
task-level
analysis
we
can
define
a
post
hoc
model
based
on
the
user
data
for
comparison
with
the
models
we
have
built
we
begin
by
observing
that
the
predictions
of
the
models
abstract
away
performance
differences
between
individual
users
and
across
trials
for
example
a
model
will
give
the
same
predicted
duration
for
the
sixth
action
in
the
ringer
volume
task
regardless
of
which
of
the
twelve
users
or
which
of
their
five
trials
is
involved
the
variance
in
the
60
data
points
per
unique
action
in
context
gives
rise
to
the
error
measured
in
table
vi
how
well
would
an
optimal
model
perform
for
a
sample
of
data
points
the
best
estimator
with
respect
to
mean
squared
error
is
simply
the
mean
of
the
sample
the
best
post
hoc
model
with
respect
to
least
squares
error
thus
returns
the
mean
duration
for
each
unique
action
over
users
and
trials
the
error
for
this
model
is
shown
in
the
md
mean
duration
columns
of
table
vi
these
errors
are
lower
than
but
still
relatively
close
to
those
produced
by
the
goms
and
act-r
models
as
percentages
of
mean
action
duration
the
models
might
improve
from
42
to
35
with
respect
to
rmse
and
from
32
to
26
for
mae
this
comparison
suggests
that
the
goms
and
act-r
models
are
performing
almost
as
well
as
is
possible
in
predicting
user
behavior
at
the
action
level
given
the
variance
in
the
user
data
2
4
discussion
all
of
the
models
we
have
presented
have
proved
robust
in
our
analysis
though
at
a
sufficiently
detailed
level
they
break
down
as
all
models
do
our
results
indicate
that
detailed
rigorous
models
of
low-level
interaction
with
cell
phones
is
possible
and
that
such
models
make
good
predictions
aside
from
the
use
of
this
work
as
a
possible
exemplar
of
the
application
of
cognitive
modeling
techniques
to
hci
evaluation
we
can
note
a
few
observations
modelers
need
to
consider
the
trade-off
between
modeling
effort
and
the
value
of
increasingly
veridical
results
the
goms
model
developed
here
is
as
good
as
or
better
than
the
act-r
model
and
was
much
cheaper
to
build
for
modeling
efficiency
a
reasonable
heuristic
is
to
apply
simple
formalisms
to
model
simple
procedures
this
is
especially
relevant
if
the
simple
formalism
can
predict
all
the
observable
information
or
all
the
needed
behavior
all
the
data
we
have
in
this
study
keystroke
times
by
expert
users
can
be
predicted
by
both
act-r
and
goms
though
in
other
situations
e
g
if
we
had
eye-tracking
data
and
wanted
to
predict
eye
movements
or
to
model
concurrent
tasks
goms
would
be
at
a
distinct
disadvantage
further
goms
offers
considerable
flexibility
in
modeling
a
coarser
formalism
does
not
necessarily
imply
stricter
constraints
on
modeling
which
is
perhaps
an
unintuitive
observation
rather
the
reverse
can
be
the
case
in
our
goms
model
for
example
the
specific
ordering
of
decision
steps
as
shown
in
figure
2
is
not
governed
by
cognitive
constraints
a
different
ordering
e
g
one
that
tested
whether
an
ok
key
press
was
appropriate
before
rather
than
after
scrolling
would
have
produced
different
predictions
it
turns
out
in
our
case
that
user
behavior
is
sufficiently
regular
that
the
goms
model
we
developed
for
a
single
user
s
behavior
generalized
very
well
to
a
larger
sample
if
this
had
not
been
the
case
the
modeling
flexibility
we
describe
would
not
have
been
helpful
our
act-r
model
for
the
same
task
does
not
allow
such
direct
fine-tuning
to
be
carried
out
in
the
same
way
because
of
tighter
architectural
constraints
on
the
interactions
between
visual
and
motor
actions
the
remaining
differences
between
the
models
predictions
and
the
data
suggest
further
improvements
to
the
models
are
possible
most
importantly
the
comparison
in
figure
8
shows
that
only
the
act-r
model
starts
to
account
for
the
faster
second
keystroke
and
none
of
the
models
predict
this
or
the
later
changes
very
well
there
are
limitations
to
this
work
so
far
aside
from
model
performance
for
example
many
cell
phones
have
additional
interaction
features
such
as
shortcut
menus
and
non-linear
graphical
icon
displays
that
are
not
captured
by
the
models
we
have
built
further
studies
perhaps
extending
to
include
novice
users
could
take
error
types
and
error
distributions
into
account
to
help
extend
the
range
of
application
of
these
models
we
believe
nevertheless
that
our
work
lays
out
clear
directions
for
future
research
one
issue
we
have
begun
to
explore
is
the
performance
differences
between
the
goms
and
actr
models
as
can
be
seen
in
the
evolution
of
cognitive
modeling
architectures
such
as
act-r
and
epic
kieras
and
meyer
1997
there
is
considerable
overlap
in
basic
assumptions
about
the
way
that
perceptual-motor
constraints
should
be
modeled
byrne
2001
kieras
2002
and
so
it
is
not
unreasonable
that
the
models
produce
similar
predictions
nevertheless
because
act-r
represents
behavior
at
a
greater
level
of
detail
than
goms
the
act-r
model
is
capable
of
more
detailed
performance
predictions
than
the
goms
model
that
goms
outperforms
act-r
in
some
areas
of
our
study
is
disappointing
from
a
cognitive
modeling
standpoint
but
not
entirely
unexpected
for
the
reasons
described
above
further
the
models
were
developed
independently
of
each
other
and
different
modeling
paradigms
and
modelers
can
lead
to
different
opportunities
for
errors
in
modeling
to
occur
ritter
1991
there
has
been
recent
work
toward
automatically
translating
between
models
at
different
levels
of
abstraction
which
would
help
reduce
or
at
least
formalize
such
errors
but
this
research
is
in
its
early
stages
john
et
al
2004
st
amant
et
al
2004a
ritter
et
al
2006
salvucci
and
lee
2003
3
user
profiles
and
search
once
models
of
menu
traversal
have
been
built
the
models
can
be
applied
toward
improving
menu
hierarchies
so
that
end
users
can
traverse
menus
more
quickly
this
is
a
key
concern
for
developers
who
may
be
less
interested
in
modeling
theory
or
model
development
than
in
the
pragmatic
issues
of
increasing
usability
an
evaluation
of
a
menu
hierarchy
independent
of
usage
patterns
would
be
uninformative
different
users
choose
different
items
and
items
are
chosen
with
varying
frequency
in
other
words
different
usage
patterns
favor
different
designs
we
define
a
user
profile
to
be
a
probability
distribution
over
the
set
of
terminal
items
in
a
menu
hierarchy
that
specifies
the
probability
of
each
terminal
being
chosen
relative
to
the
entire
set
each
user
profile
is
also
associated
with
the
frequency
that
the
menu
system
is
accessed
for
the
entire
population
of
users
of
the
menu
hierarchy
there
may
be
many
different
user
profiles
some
more
common
than
others
a
distribution
captured
by
the
coverage
of
individual
profiles
as
an
example
imagine
that
20
of
the
users
of
a
given
cell
phone
access
only
two
items
recent
calls
and
view
all
contacts
each
on
average
twice
a
day
in
the
probability
distribution
of
the
profile
for
this
set
of
users
these
two
items
have
probability
0
5
and
all
others
have
0
0
the
coverage
of
the
profile
is
0
20
and
its
frequency
is
4
a
value
that
becomes
meaningful
in
the
context
of
the
per-day
usage
values
of
other
profiles
in
formal
terms
the
design
problem
involves
the
construction
of
a
mapping
in
the
form
of
a
hierarchical
ordering
h
between
t
the
set
of
terminal
menu
items
and
u
the
set
of
all
user
profiles
defined
on
t
a
reasonable
evaluation
measure
for
a
given
menu
hierarchy
h
is
its
efficiency
the
expected
cost
of
reaching
a
terminal
item
this
turns
out
to
be
straightforward
to
represent
expected
cost
is
given
by
where
p
t
is
the
probability
of
the
occurrence
of
a
specific
terminal
t
and
ch
t
is
the
cost
of
reaching
terminal
t
in
hierarchy
h
in
some
situations
it
may
be
possible
to
estimate
p
t
directly
through
usage
statistics
across
user
profiles
this
would
mean
maintaining
a
local
log
of
menu
selections
on
individual
cell
phones
to
be
uploaded
opportunistically
to
a
central
repository
or
making
these
local
actions
visible
remotely
as
they
are
carried
out
if
this
is
not
practical
due
to
storage
or
bandwidth
constraints
an
alternative
is
possible
we
can
express
the
probability
p
t
as
follows
that
is
the
probability
of
the
occurrence
of
t
is
the
conditional
probability
of
its
occurrence
in
a
specific
user
profile
u
scaled
by
the
probability
of
u
and
summed
over
all
user
profiles
the
conditional
probability
p
t
u
is
given
by
the
distribution
associated
with
each
user
profile
as
described
above
values
for
p
u
can
be
estimated
from
the
coverage
and
frequency
of
a
profile
at
the
time
the
profile
is
assigned
to
a
user
in
practice
we
can
imagine
individual
users
being
asked
questions
about
their
cell
phone
usage
when
they
are
assigned
to
a
specific
user
profile
how
often
they
will
access
their
cell
phone
s
menu
system
and
the
types
of
functions
they
expect
to
use
the
trade-off
compared
with
direct
sampling
of
p
t
is
between
accuracy
and
resource
demands
all
that
remains
is
to
define
a
specific
cost
function
ch
which
we
can
do
with
our
study
results
for
pragmatic
reasons
we
use
the
easiest
metric
available
to
compute
cost
the
linear
regression
given
in
eq
2
the
goms
or
act-r
model
could
have
been
used
with
comparable
accuracy
but
with
a
significant
increase
in
processing
time
the
factors
that
make
the
linear
regression
less
appropriate
for
modeling
do
not
apply
here
our
choice
for
ch
means
that
ec
h
produces
the
expected
duration
of
choosing
an
arbitrary
terminal
menu
item
in
hierarchy
h
this
measure
can
be
used
by
an
automated
search
algorithm
to
identify
alternative
designs
of
the
menu
hierarchy
that
improve
user
performance
a
complication
is
that
the
automated
modification
of
a
menu
hierarchy
cannot
arbitrarily
rearrange
structure
purely
for
efficiency
changes
should
respect
the
semantic
relationships
between
the
items
that
is
the
item
ringer
volume
is
under
the
settings
category
rather
than
vice-versa
for
good
reason
to
avoid
the
difficulties
of
representing
and
reasoning
about
menu
item
semantics
we
leave
this
for
future
work
we
rely
on
two
search
operators
that
produce
only
small
changes
for
a
terminal
item
with
non-zero
probability
these
operators
can
be
applied
--
promote
item
moves
an
item
to
the
beginning
of
its
menu
list
to
reduce
scrolling
time
--
promote
subtree
moves
an
ancestor
of
the
item
up
one
level
in
the
hierarchy
to
reduce
the
number
of
intermediate
items
that
must
be
selected
to
reach
the
terminal
an
item
or
subtree
rooted
at
an
ancestor
may
only
be
promoted
once
even
with
these
constraints
the
search
space
size
is
exponential
in
the
number
of
target
items
with
non-zero
probability
in
any
profile
e
g
if
all
non-zero
items
in
a
user
profile
are
in
one
menu
list
then
all
permutations
of
these
items
will
be
considered
exhaustive
search
is
thus
impractical
for
the
phone
hierarchy
shown
in
table
i
for
just
the
menu
containing
12
items
mentioned
in
section
1
half
a
billion
permutations
are
possible
a
best-first
search
algorithm
however
gives
good
results
after
as
few
as
100
steps
3
1
results
ideally
we
would
be
able
to
validate
the
search
procedure
based
on
real
user
profiles
found
in
the
most
commonly
used
cell
phones
we
have
been
unable
to
acquire
such
data
unfortunately
lacking
real
cell
phone
user
profiles
we
can
only
illustrate
the
search
procedure
in
practice
but
our
results
are
promising
based
on
the
kyocera
menu
hierarchy
we
defined
random
profiles
of
different
sizes
where
size
refers
to
the
number
of
non-zero
probability
menu
items
contained
in
the
profile
the
probabilities
for
each
profile
were
drawn
from
a
uniform
random
distribution
and
normalized
because
these
profiles
were
randomly
generated
we
used
only
a
single
profile
for
the
search
rather
than
composing
arbitrary
probabilities
from
different
random
profiles
these
profiles
approximate
profiles
for
spreadsheet
usage
napier
et
al
1992
and
modeling
languages
nichols
and
ritter
1995
table
vii
shows
the
results
for
user
profiles
of
size
20
30
and
40
terminals
in
each
case
10
different
random
profiles
were
generated
for
each
size
and
a
best-first
search
bounded
at
500
steps
was
applied
to
produce
improvements
the
cost
values
are
means
of
the
time
estimates
produced
by
the
linear
model
the
last
column
gives
the
time
savings
in
traversing
the
reordered
menus
as
a
percentage
of
the
duration
of
the
traversals
in
the
original
menu
hierarchy
because
these
results
are
based
on
random
probabilities
of
accessing
menu
items
rather
than
actual
user
experiences
they
can
only
be
viewed
as
suggestive
anecdotal
evidence
from
industry
contacts
indicates
that
performing
usability
studies
on
menu
hierarchies
is
not
common
practice
we
expect
that
with
improvements
in
data
collection
however
this
approach
may
help
to
make
cell
phones
more
efficient
in
the
future
targets
for
future
research
include
examining
the
plausibility
of
a
uniform
distribution
for
selectable
menu
items
in
user
profiles
more
efficient
search
to
optimize
menu
layouts
application
to
other
types
of
menu
layouts
and
the
inclusion
of
other
factors
e
g
profile
size
in
cost
computations
3
2
discussion
we
have
presented
formulas
and
a
search
algorithm
to
show
how
menu
efficiency
can
be
improved
by
about
a
third
the
modifications
to
the
menu
hierarchy
produced
by
the
search
have
the
effect
of
reducing
the
depth
of
the
hierarchy
and
increasing
the
length
of
individual
menus
this
was
a
simple
change
but
clearly
one
that
could
be
applied
to
at
least
one
commercially
available
phone
it
could
plausibly
be
applied
to
other
systems
with
similar
menu
structures
the
general
approach
laid
out
in
this
section
is
related
to
two
areas
of
hci
other
than
cognitive
modeling
both
of
which
provide
opportunities
for
further
research
the
first
area
is
adaptive
user
interfaces
the
issue
of
finding
the
best
menu
hierarchy
for
a
given
user
profile
is
separate
from
that
of
deciding
when
the
menu
structure
should
be
put
in
place
our
discussion
in
this
section
assumes
that
a
static
hierarchy
is
associated
with
each
user
profile
even
if
new
usage
data
were
to
become
available
over
time
if
such
data
were
recorded
over
time
for
individual
users
then
a
new
search
could
be
carried
out
incrementally
to
find
improved
menu
hierarchies
this
function
should
not
be
performed
lightly
but
one
now
quite
real
possibility
is
to
treat
the
automated
adaptation
of
the
menu
hierarchy
as
a
customization
option
that
users
can
select
at
their
own
discretion
whenever
they
choose
it
should
also
be
possible
to
incorporate
a
theory
of
learning
that
could
predict
when
to
do
this
and
the
costs
involved
in
learning
the
new
menu
structure
the
second
related
area
is
support
for
navigation
a
menu
hierarchy
is
a
small
restricted
information
space
in
comparison
with
other
spaces
such
as
the
world
wide
web
the
modifications
explored
by
the
search
procedure
are
only
a
small
subset
of
possible
transformations
that
might
be
applied
to
an
interface
nevertheless
some
of
the
same
conceptual
issues
apply
to
the
analysis
of
navigation
in
general
for
example
usage
frequency
could
be
used
for
improving
navigation
on
a
web
site
by
promoting
links
upward
toward
the
site
entry
page
and
move
specific
links
closer
to
the
top
of
their
pages
ritter
et
al
2005
in
practice
the
most
effective
approach
to
navigation
redesign
addresses
the
semantics
of
the
information
space
rather
than
focusing
only
on
its
surface
organization
and
presentation
young
1998
for
menu
hierarchy
modification
this
implies
that
greater
potential
benefits
can
be
gained
from
examining
the
semantic
relationships
between
menu
categories
and
menu
items
than
their
ordering
the
most
relevant
research
along
these
lines
is
pirolli
s
work
on
optimalforaging
theory
and
information
scent
pirolli
1997
2003
optimal-foraging
theory
explains
behavior
adaptations
in
terms
of
resource
availability
and
constraints
in
its
application
to
menu
navigation
information
scent
is
a
metaphor
for
visible
semantic
cues
that
lead
users
to
information
they
seek
pirolli
has
developed
an
extension
of
act-r
called
act-if
to
evaluate
a
foraging
model
of
information
navigation
act-if
relies
on
a
spreading
activation
network
to
capture
associations
in
memory
processing
the
models
described
in
our
article
are
based
on
the
assumption
that
associations
between
menu
items
such
as
sounds
and
ringer
volume
can
be
directly
retrieved
from
memory
by
an
expert
user
a
more
general
model
based
on
act-if
might
be
able
to
explain
the
strength
of
these
associations
based
on
measures
of
semantic
distance
with
such
flexibility
in
representation
it
would
be
possible
to
explore
additional
modeling
issues
such
as
how
novice
users
might
traverse
an
unfamiliar
menu
hierarchy
which
paths
through
the
hierarchy
are
more
likely
to
result
in
errors
and
how
renaming
or
recategorizing
menu
items
could
influence
navigation
performance
more
than
just
reordering
4
conclusion
in
this
article
we
have
described
a
set
of
evaluation
concepts
and
tools
to
support
cell
phone
menu
design
the
goms
model
is
able
to
predict
user
performance
very
well
the
act-r
model
performs
almost
as
well
it
took
more
effort
to
create
but
also
provide
more
detailed
predictions
and
could
be
used
for
a
wider
range
of
analyses
although
our
work
has
relied
on
a
simpler
performance
model
both
of
these
models
could
be
used
by
a
simple
efficient
algorithm
to
optimize
the
redesign
of
cell
phone
menus
the
redesign
could
let
users
on
average
perform
their
tasks
about
30
faster
based
on
plausible
assumptions
about
usage
this
menu
redesign
approach
is
simple
we
believe
it
is
simple
enough
to
be
taught
to
and
used
by
designers
this
approach
is
based
on
knowing
users
through
the
models
and
knowing
their
tasks
in
its
simplest
form
the
approach
is
to
reorder
the
menu
items
to
put
the
most
commonly
used
tasks
earlier
and
higher
in
the
hierarchy
where
users
task
frequencies
are
not
known
or
vary
widely
between
users
it
appears
reasonable
to
allow
the
system
to
reorder
itself
upon
a
user
s
request
after
a
sufficient
break-in
period
of
course
the
semantics
of
the
task
and
the
semantics
of
the
task
titles
will
have
a
role
to
play
as
well
which
we
did
not
explore
here
others
are
working
with
act-r
to
create
models
that
start
to
take
account
of
this
aspect
of
interaction
pirolli
2003
these
models
and
the
optimization
algorithm
bring
together
several
interesting
aspects
of
human
behavior
and
show
how
a
simple
ai
algorithm
can
help
in
hci
design
it
also
gives
rise
to
both
theoretical
and
practical
implications
theoretically
novice
user
actions
learning
error
recovery
behavior
performance
under
stress
and
generality
across
different
devices
are
now
areas
ripe
for
further
exploration
having
the
models
in
hand
also
let
us
explore
and
explain
new
regularities
in
user
behavior
such
as
the
variations
in
key
press
time
shown
in
figure
8
from
a
practical
standpoint
developers
have
models
that
are
ready
for
use
--
these
models
are
general
enough
that
they
do
not
require
cognitive
modeling
expertise
or
programming
skill
to
apply
them
to
different
traversal
tasks
in
different
menu
hierarchies
or
on
different
cell
phones
our
longer-term
goals
for
this
research
include
the
application
of
modeling
techniques
to
provide
insights
into
usability
issues
nichols
and
ritter
1995
and
the
development
of
better
cognitive
modeling
tools
for
evaluating
and
designing
more
general
classes
of
user
interfaces
ritter
et
al
2006
st
amant
et
al
2004a
we
believe
that
as
modeling
concepts
and
techniques
become
more
accessible
to
hci
developers
they
will
become
increasingly
significant
in
their
contribution
to
improving
user
interfaces
wide
application
of
the
menu
design
approach
in
this
article
could
for
example
save
significant
amounts
of
time
if
2
billion
users
were
to
use
their
cell
phone
menus
every
day
for
just
three
seconds
our
improvements
could
save
almost
30
years
of
user
time
per
day
an
integrated
approach
for
modeling
learning
patterns
of
students
in
web-based
instruction
a
cognitive
style
perspective
sherry
y
chen
and
xiaohui
liu
brunel
university
web-based
instruction
wbi
programs
which
have
been
increasingly
developed
in
educational
settings
are
used
by
diverse
learners
therefore
individual
differences
are
key
factors
for
the
development
of
wbi
programs
among
various
dimensions
of
individual
differences
the
study
presented
in
this
article
focuses
on
cognitive
styles
more
specifically
this
study
investigates
how
cognitive
styles
affect
students
learning
patterns
in
a
wbi
program
with
an
integrated
approach
utilizing
both
traditional
statistical
and
data-mining
techniques
the
former
are
applied
to
determine
whether
cognitive
styles
significantly
affected
students
learning
patterns
the
latter
use
clustering
and
classification
methods
in
terms
of
clustering
the
k-means
algorithm
has
been
employed
to
produce
groups
of
students
that
share
similar
learning
patterns
and
subsequently
the
corresponding
cognitive
style
for
each
group
is
identified
as
far
as
classification
is
concerned
the
students
learning
patterns
are
analyzed
using
a
decision
tree
with
which
eight
rules
are
produced
for
the
automatic
identification
of
students
cognitive
styles
based
on
their
learning
patterns
the
results
from
these
techniques
appear
to
be
consistent
and
the
overall
findings
suggest
that
cognitive
styles
have
important
effects
on
students
learning
patterns
within
wbi
the
findings
are
applied
to
develop
a
model
that
can
support
the
development
of
wbi
programs
1
introduction
with
the
rapid
development
of
information
technology
the
world
wide
web
web
contains
an
enormous
amount
of
information
liaw
and
huang
2006
in
particular
there
has
been
considerable
growth
in
the
use
of
instructional
materials
over
the
web
yen
and
li
2003
web-based
instruction
wbi
has
become
increasingly
attractive
to
educational
settings
both
for
financial
and
technological
reasons
brotherton
and
abowd
2004
these
include
easy
updating
of
the
material
scarsbrook
et
al
2005
remote
access
from
everywhere
and
at
any
time
anido
et
al
2001
presentation
with
multiple
media
such
as
text
audio
graphics
video
and
animation
masiello
et
al
2005
and
the
realization
of
a
learner-centered
design
approach
jolliffe
et
al
2001
the
learner-centered
design
is
especially
important
because
wbi
programs
are
used
by
a
diverse
population
of
learners
who
have
far
more
heterogeneous
backgrounds
in
terms
of
their
background
skills
and
needs
soloway
and
pryor
1996
chen
and
macredie
2004
this
type
of
design
argues
that
the
development
of
an
instruction
program
should
be
based
on
the
learners
point
of
view
soloway
et
al
1996
and
should
address
the
needs
of
learners
quintana
et
al
2000
paying
attention
to
learner
diversity
has
been
shown
to
increase
student
motivation
to
learn
which
in
turn
may
lead
to
improved
learning
performance
larkin-hein
and
budny
2001
therefore
individual
differences
arguably
become
an
important
consideration
a
number
of
learnercentered
studies
have
shown
that
individual
differences
have
a
strong
impact
on
the
use
of
instruction
technology
marchionini
1995
an
analysis
of
existing
pedagogical
studies
also
confirms
that
the
successful
usage
of
instructional
technology
depends
on
the
technology
itself
and
the
learners
individual
characteristics
chou
and
wang
2000
for
these
reasons
research
into
individual
differences
has
mushroomed
over
the
past
decade
the
examined
differences
include
cognitive
styles
workman
2004
chen
and
macredie
2004
gender
differences
beckwith
et
al
2005
roy
and
chi
2003
and
prior
knowledge
wang
and
dwyer
2004
mitchell
et
al
2005
among
these
differences
cognitive
style
has
been
identified
as
one
of
the
most
pertinent
factors
because
it
refers
to
a
user
s
information
processing
habits
representing
an
individual
user
s
typical
mode
of
perceiving
thinking
remembering
and
solving
problems
messick
1976
it
has
also
been
suggested
that
teachers
should
assess
the
cognitive
styles
of
their
students
in
order
to
design
instructional
strategies
for
optimal
learning
lee
1992
in
this
vein
this
study
investigates
a
specific
research
question
what
are
the
effects
of
students
cognitive
styles
on
their
learning
patterns
within
a
webbased
instruction
program
over
the
past
five
years
this
issue
has
been
investigated
by
a
number
of
studies
e
g
calcaterra
et
al
2005
liegle
and
janicki
2006
while
their
results
are
useful
they
only
represent
the
tip
of
iceberg
of
what
might
be
obtained
by
using
advanced
intelligent
technologies
one
of
which
is
data
mining
data
mining
also
known
as
knowledge
discovery
in
databases
fayyad
and
uthurusamy
1996
is
an
interdisciplinary
area
that
encompasses
techniques
from
a
number
of
fields
including
information
technology
statistical
analyses
and
mathematic
science
bohen
et
al
2003
a
major
function
of
data
mining
is
to
help
analyze
understand
or
even
visualize
the
huge
amount
of
data
stored
in
databases
data
warehouses
or
other
information
repositories
li
and
shue
2004
recent
studies
suggest
that
data
mining
is
a
useful
tool
for
analyzing
web
usage
data
because
it
can
discover
regularities
and
hidden
patterns
in
data
cho
et
al
2003
ozmutlu
et
al
2002
therefore
we
choose
to
use
data
mining
to
analyze
the
learning
patterns
of
different
cognitive
styles
in
our
study
the
article
is
structured
as
follows
in
section
2
we
briefly
highlight
previous
work
on
cognitive
styles
in
wbi
and
present
related
research
on
data
mining
section
3
describes
the
methodology
used
to
conduct
the
experiment
and
the
techniques
applied
to
the
analysis
of
corresponding
data
subsequently
the
experimental
results
are
presented
in
section
4
it
then
progresses
to
section
5
where
we
discuss
the
effects
that
cognitive
styles
have
on
students
learning
patterns
section
6
presents
a
mechanism
to
help
designers
develop
wbi
programs
that
can
accommodate
the
preferences
of
both
field-independent
and
field-dependent
learners
finally
conclusions
are
drawn
and
possibilities
for
future
work
are
identified
in
section
7
2
background
this
section
starts
with
a
summary
of
the
empirical
findings
about
the
effects
of
cognitive
styles
on
wbi
followed
by
an
explanation
of
the
rationale
for
data
mining
and
a
review
of
its
existing
approaches
and
applications
2
1
cognitive
styles
traditional
computer-based
instruction
programs
present
information
in
a
linear
fashion
wbi
programs
employ
hypermedia
techniques
which
have
great
potential
for
education
chen
and
macredie
2004
and
permit
much
more
flexibility
in
the
delivery
of
instruction
yamda
et
al
1995
learners
are
offered
a
rich
exploration
environment
and
have
freedom
for
navigation
it
can
be
argued
that
when
learners
are
given
the
opportunity
to
move
freely
through
a
wbi
program
they
are
able
to
develop
their
own
learning
patterns
whose
features
can
reflect
their
cognitive
styles
chen
and
ford
1998
terrell
2002
in
other
words
differences
in
cognitive
styles
may
lead
to
distinctive
learning
patterns
cognitive
styles
refer
to
how
individuals
prefer
to
organize
and
represent
information
riding
and
rayner
1998
there
are
many
dimensions
to
cognitive
styles
such
as
visualized
versus
verbalized
right-brained
versus
left-brained
global-holistic
versus
focused-detailed
or
field-dependent
versus
field-independent
among
these
dimensions
field
dependence/-independence
has
emerged
as
one
of
the
most
widely
studied
dimensions
with
the
broadest
application
to
problems
of
education
messick
1976
witkin
et
al
1977
because
it
reflects
how
well
a
learner
is
able
to
restructure
information
based
on
the
use
of
salient
cues
and
field
arrangements
weller
et
al
1994
the
key
issue
of
field
dependence
lies
in
the
differences
between
field-dependent
and
field-independent
learners
which
are
presented
here
--field
independence
these
individuals
tend
to
exhibit
more
individualistic
behaviors
since
they
are
not
in
need
of
external
referents
to
aide
in
the
processing
of
information
they
are
more
capable
of
developing
their
own
internal
referents
and
restructuring
their
knowledge
are
better
at
learning
impersonal
abstract
materials
are
not
easily
influenced
by
others
and
are
not
overly
affected
by
the
approval
or
disapproval
of
superiors
--field
dependence
these
individuals
are
considered
to
have
a
more
social
orientation
than
field-independent
persons
since
they
are
more
likely
to
make
use
of
externally
developed
social
frameworks
they
tend
to
seek
out
external
referents
for
processing
and
structuring
their
information
are
better
at
learning
material
with
human
content
are
more
readily
influenced
by
the
opinions
of
others
and
are
affected
by
the
approval
or
disapproval
of
authority
figures
witkin
et
al
1977
a
number
of
studies
investigate
the
relationships
between
the
degree
of
field
dependence
and
students
leaning
patterns
within
wbi
table
i
presents
a
list
of
11
recent
studies
published
from
2000
to
2005
as
shown
in
this
table
it
is
still
inconclusive
whether
field-independent
and
field-dependent
students
have
different
learning
patterns
in
addition
these
studies
mainly
relied
on
traditional
statistics
which
in
turn
might
produce
insufficient
data
analysis
a
lack
of
deep
analysis
might
cause
some
relevant
relationships
to
be
ignored
for
example
what
dominant
learning
patterns
appear
in
wbi
and
whether
cognitive
style
is
a
major
factor
influencing
students
learning
patterns
or
if
other
human
factors
such
as
prior
knowledge
and
gender
differences
to
conduct
such
a
deep
analysis
there
technology
influence
learning
patterns
is
needed
that
combines
statistical
rigor
and
computational
power
to
analyze
summarize
and
interpret
data
effectively
data
mining
is
one
such
technology
2
2
data
mining
data
mining
is
the
process
of
extracting
valuable
information
from
large
amounts
of
data
hand
et
al
2001
the
main
difference
between
statistical
analyses
and
data
mining
lies
in
the
goal
that
is
sought
the
former
is
often
used
to
verify
prior
hypotheses
or
existing
knowledge
in
order
to
prove
a
known
relationship
moss
and
atre
2003
while
the
latter
is
aimed
at
finding
unexpected
relationships
patterns
and
interdependencies
hidden
in
the
data
wang
et
al
2002
as
opposed
to
traditional
experiments
designed
to
verify
a
priori
hypothesis
with
statistical
analyses
data
mining
uses
the
data
itself
to
uncover
relationships
and
patterns
in
doing
so
hidden
relationships
patterns
and
interdependencies
can
be
discovered
predictive
rules
can
be
generated
and
interesting
hypotheses
can
be
found
these
are
the
advantages
of
data
mining
hedberg
1995
gargano
and
ragged
1999
data
mining
can
be
used
to
achieve
different
types
of
tasks
based
on
the
nature
of
the
information
extraction
these
tasks
can
be
broadly
divided
into
three
major
categories
clustering
classification
and
association
rules
chen
acm
transactions
on
computer-human
interaction
vol
15
no
1
article
1
pub
date
may
2008
results
field-independent
students
favored
using
the
index
conversely
field-dependent
students
preferred
to
use
the
map
field-dependent
learners
in
the
breadth-first
version
performed
better
than
those
in
the
depth-first
version
conversely
field-independent
students
in
the
depth-first
version
outperformed
those
in
the
breadth-first
version
field-independent
students
had
better
performance
in
the
internet
treatment
than
in
the
cd-rom
and
text
treatments
field-independent
students
did
not
differ
significantly
from
field-dependent
students
in
their
learning
patterns
field-independent
students
achieved
superior
scores
in
the
long-page
condition
whereas
field-dependent
students
were
superior
in
the
short-page
condition
field-dependent
subjects
hit
more
often
on
teaching
notes
and
other
class
resources
than
field-independent
subjects
field-independent
students
appreciated
the
fact
that
they
could
study
topics
in
any
order
however
field-dependent
students
felt
confused
over
which
options
they
should
choose
field-independent
students
tended
to
have
higher
online
technologies
self-efficacy
but
they
did
not
receive
higher
grades
than
field-dependent
students
field-dependent
students
used
linear
learning
conversely
field-dependent
students
preferred
nonlinear
learning
there
was
no
significant
difference
between
field-independent
and
field-dependent
students
in
their
quantity
of
annotation
cognitive
style
is
a
significant
factor
in
terms
of
instructional
delivery
modes
and
experience
with
online
learning
clustering
a
major
exploratory
data
analysis
method
eda
tukey
1977
is
concerned
with
the
division
of
data
into
groups
of
similar
objects
each
group
called
a
cluster
consists
of
objects
that
are
similar
among
themselves
and
dissimilar
to
objects
of
other
groups
roussinov
and
zhao
2003
this
approach
has
the
advantages
of
uncovering
unanticipated
trends
correlations
or
patterns
and
no
assumptions
are
made
about
the
structure
of
the
data
wang
et
al
2004
have
developed
a
recommendation
system
for
the
cosmetic
business
in
the
system
they
segmented
the
customers
by
using
clustering
algorithms
to
discover
different
behavior
groups
so
that
customers
in
the
same
group
would
have
similar
purchase
behavior
the
goal
of
classification
is
to
construct
a
classification
procedure
from
a
set
of
cases
whose
classes
are
known
so
that
such
a
procedure
can
be
used
to
classify
new
cases
liu
and
kellam
2003
in
other
words
classification
can
be
used
both
to
understand
existing
data
and
to
predict
how
new
cases
will
behave
for
example
ng
and
tan
2003
brown
et
al
2000
and
mateos
et
al
2002
used
classification
to
infer
the
functions
of
genes
association
rules
that
were
first
proposed
by
agrawal
and
srikant
1994
are
mainly
used
to
find
out
the
meaningful
relationships
between
items
or
features
that
occur
synchronously
in
databases
this
approach
is
useful
when
one
has
an
idea
of
the
different
associations
that
are
sought
this
is
because
one
can
find
all
kinds
of
correlations
in
a
large
dataset
cunningham
and
frank
1999
applied
the
association
rules
to
the
task
of
detecting
subject
categories
that
co-occur
in
transaction
records
of
books
borrowed
from
a
university
library
a
number
of
techniques
are
available
to
support
these
tasks
including
decision
trees
the
k-means
algorithm
the
support
vector
machine
and
selforganization
maps
among
these
the
k-means
algorithm
and
decision
trees
have
been
widely
used
to
analyze
web
usage
data
the
k-means
algorithm
is
an
algorithm
that
can
support
tasks
for
clustering
see
section
3
3
2
for
the
details
this
algorithm
was
applied
in
the
study
of
wiwattanacharoenchai
and
srivihok
2003
that
clustered
customer
segments
from
web
logs
of
various
internet
banking
web
sites
consequently
their
results
showed
that
there
was
a
clear
distinction
between
the
segments
in
terms
of
customer
behavior
a
similar
study
was
conducted
by
chaimeun
and
srivihok
2005
who
combined
the
k-means
algorithm
with
self
organization
maps
to
cluster
handicraft
customers
in
addition
baeza-yates
et
al
2004
used
the
k-means
algorithm
to
discover
clusters
that
define
textual
contexts
for
the
images
on
the
web
decision
trees
can
easily
be
used
for
performing
classification
tasks
see
section
3
3
3
for
the
details
this
technique
was
used
in
the
study
of
lee
and
yang
2003
which
developed
a
learning
agent
to
model
a
user
s
interests
for
tv
program
selections
and
to
explore
different
program
features
for
better
recommendations
kim
et
al
2002
also
used
the
decision
tree
to
develop
personalized
recommendation
procedure
for
an
internet
shopping
mall
furthermore
ayan
et
al
2002
used
the
decision
tree
to
develop
a
technique
for
automatically
defining
logical
domains
on
the
basis
of
web
page
metadata
in
addition
to
the
aforementioned
evidence
that
suggests
the
k-means
algorithm
and
decision
trees
can
provide
effective
approaches
to
examining
the
web
usage
data
another
advantage
of
these
two
techniques
is
that
it
is
easy
to
understand
how
they
work
see
section
3
3
and
the
generated
results
are
easily
understood
mertik
et
al
2004
moreover
they
are
supported
by
many
commercial
data
mining
software
packages
and
thus
are
easily
implemented
in
real-world
applications
ahola
and
rinta-runsala
2001
in
this
article
an
integrated
approach
that
combines
traditional
statistics
with
the
k-means
algorithm
and
decision
trees
is
selected
to
analyze
students
learning
patterns
in
an
empirical
study
conducted
in
a
wbi
program
the
purpose
of
using
such
an
integrated
approach
is
to
bring
together
confirmatory
analysis
and
exploratory
analysis
in
doing
so
we
can
obtain
a
more
complete
picture
which
not
only
shows
the
predefined
relationships
between
the
students
cognitive
styles
and
their
learning
patterns
but
can
also
reveal
the
unexpected
but
valuable
patterns
hidden
in
data
hand
1999
3
methodology
to
understand
the
differences
in
learning
patterns
among
different
cognitive
style
groups
we
conducted
an
experiment
in
a
uk
university
this
section
describes
the
research
instruments
the
experimental
design
and
the
methods
of
data
analyses
3
1
research
instruments
research
instruments
work
as
a
guide
in
order
to
make
sure
that
the
same
information
is
obtained
from
different
students
the
research
instruments
used
in
this
study
included
a
wbi
program
to
teach
students
computational
algorithms
and
riding
s
cognitive
style
analysis
to
measure
students
cognitive
styles
3
1
1
web-based
instruction
programs
a
wbi
program
was
created
containing
materials
from
the
computation
and
algorithms
module
the
program
included
about
75
pages
and
the
content
was
divided
into
six
sections
interface
elements
included
a
a
title
bar
located
at
the
top
of
the
screen
showing
the
section
name
being
viewed
b
a
control
panel
with
choices
for
menu
map
index
and
the
other
available
sections
and
c
the
main
body
of
the
tutorial
providing
referenced
links
and
subject
categories
for
selection
figure
1
shows
the
screen
design
of
this
wbi
program
the
design
of
this
wbi
program
was
underpinned
by
considerations
of
free
exploration
of
the
instructional
material
the
wbi
program
provided
the
students
with
rich
links
within
the
text
as
well
as
a
variety
of
navigation
tools
including
a
hierarchical
map
an
alphabetical
index
and
a
main
menu
in
addition
each
topic
was
further
split
into
four
display
options
comprising
a
to
allow
students
to
control
the
selection
of
the
contents
they
wish
to
learn
to
allow
students
to
choose
one
of
the
display
options
that
covers
the
same
concept
overview
b
details
c
examples
and
d
references
there
were
two
types
of
overview
a
general
content
overview
and
an
overview
for
each
specific
topic
in
this
way
the
learners
were
given
control
of
deciding
their
own
learning
paths
and
choosing
their
favorite
navigation
tools
and
preferred
presentation
formats
three
types
of
learner
control
were
available
in
the
programs
as
shown
in
table
ii
3
1
2
cognitive
style
analysis
the
cognitive
style
dimension
investigated
in
this
study
was
the
level
of
field
dependence
a
number
of
instruments
have
been
developed
to
measure
field
dependence
including
the
group
embedded
figures
test
geft
by
witkin
et
al
1971
and
the
cognitive
styles
analysis
csa
by
riding
1991
the
main
advantage
of
csa
over
geft
is
that
fielddependent
competence
is
positively
measured
rather
than
being
inferred
from
poor
field-independent
capability
ford
and
chen
2001
in
addition
the
csa
offers
computerized
administration
and
scoring
and
consequently
has
been
selected
as
the
instrument
for
measuring
field
dependence
in
this
study
the
csa
includes
two
subtests
figure
2
the
first
presents
items
containing
pairs
of
complex
geometrical
figures
that
the
individual
is
required
to
judge
as
either
the
same
or
different
the
second
presents
items
each
comprising
a
simple
geometrical
shape
such
as
a
square
or
a
triangle
and
a
complex
geometrical
figure
as
in
the
geft
the
individual
is
asked
to
indicate
whether
or
not
the
simple
shape
is
contained
in
a
complex
one
by
pressing
one
of
two
marked
response
keys
riding
and
grimley
1999
these
two
subtests
have
different
purposes
the
first
subtest
is
a
task
requiring
the
field-dependent
capacity
conversely
the
second
subtest
requires
the
disembedding
capacity
associated
with
field
independence
the
csa
measures
what
riding
and
sadler-smith
1992
refer
to
as
a
wholist/analytic
wa
dimension
noting
that
this
is
equivalent
to
field
dependence/independence
as
witkin
et
al
1971
argued
a
field-independent
individual
is
capable
of
a
more
analytical
cognitive
function
than
a
field
dependent
individual
who
uses
a
more
global
approach
riding
s
1991
recommendations
are
that
wa
scores
below
1
03
denote
field-dependent
individuals
wa
scores
of
1
36
and
above
denote
field-independent
individuals
and
students
scoring
between
1
03
and
1
35
are
classed
as
intermediate
in
this
study
categorizations
were
based
on
these
recommendations
as
reported
by
peterson
et
al
2003
the
reliability
is
r
0
81
p
0
00
3
2
experiment
design
the
experiment
took
place
in
a
uk
university
a
request
was
issued
to
students
in
lectures
and
further
by
email
making
clear
the
nature
of
the
experiment
and
their
participation
the
students
took
part
in
the
experiment
a
voluntary
basis
the
experiment
consisted
of
two
phases
held
on
different
days
seventy-six
participants
took
part
in
the
first
phase
of
the
experiment
and
they
were
asked
to
take
riding
s
csa
test
to
classify
their
cognitive
styles
once
this
was
done
they
were
requested
to
fill
out
a
questionnaire
which
was
designed
to
identify
their
personal
background
including
their
gender
the
levels
of
their
prior
knowledge
to
subject
content
and
the
levels
of
their
previous
system
experience
in
using
computers
the
internet
and
computer-aided
learning
the
purpose
of
these
activities
was
to
select
the
sample
for
the
next
phase
based
on
the
participants
previous
system
experience
and
their
prior
knowledge
of
the
subject
content
sixty-five
students
who
had
the
basic
computing
and
internet
skills
necessary
to
operate
the
wbi
program
and
were
inexperienced
in
the
subject
content
were
selected
to
participate
in
the
second
phase
details
of
the
distribution
of
the
participants
for
the
second
phase
are
presented
in
table
iii
to
help
them
use
the
wbi
program
the
second
phase
began
by
giving
the
students
an
introduction
to
the
functionality
of
the
wbi
program
subsequently
all
of
the
participants
interacted
with
the
wbi
program
and
their
interactions
with
the
program
were
recorded
in
a
log
file
the
students
were
given
a
task
sheet
containing
various
exercises
related
to
the
content
of
the
wbi
program
the
purpose
of
doing
this
is
to
ensure
that
the
participants
browse
all
contents
within
the
wbi
program
they
were
informed
that
they
could
use
the
wbi
program
for
a
maximum
of
90
minutes
due
to
the
relatively
small
sample
size
a
within-subjects
design
was
selected
for
experiment
design
reips
2000
in
this
way
participants
were
free
to
choose
preferred
navigation
tools
and
display
options
by
themselves
so
that
how
cognitive
styles
influence
their
choices
could
be
examined
3
3
data
analysis
raw
experimental
data
were
in
the
form
of
web
access
logs
and
initially
went
through
a
preprocessing
phase
in
which
all
damaged
links
were
traced
and
removed
from
the
database
in
order
to
discover
the
relationships
between
students
cognitive
styles
and
their
learning
patterns
it
is
necessary
to
extract
the
potentially
useful
information
from
raw
data
the
rationale
of
selecting
useful
information
is
based
on
a
comprehensive
review
by
chen
and
macredie
2002
which
examined
the
effects
of
cognitive
styles
on
student
learning
in
addition
to
their
findings
the
characteristics
of
the
raw
data
and
web
site
structure
were
also
taken
into
account
to
extract
the
attributes
that
reveal
students
learning
patterns
in
wbi
in
total
there
are
eight
attributes
including
the
total
number
of
pages
each
student
browsed
the
total
number
of
visited
pages
respectively
describing
overviews
examples
detailed
descriptions
the
total
number
of
times
each
navigational
tool
was
used
including
main
menu
hierarchical
map
and
alphabetical
index
and
the
number
of
repeated
visits
the
students
made
it
is
worth
noting
that
the
measurement
of
these
eight
attributes
is
based
on
the
students
choices
in
other
words
this
study
emphasizes
on
students
preferred
learning
patterns
instead
of
the
best
learning
patterns
because
cognitive
style
is
not
a
measure
of
intelligence
or
ability
rose
1988
in
order
to
conduct
a
comprehensive
evaluation
the
students
learning
patterns
were
analyzed
using
traditional
statistical
and
data
mining
techniques
the
former
were
applied
to
determine
whether
cognitive
styles
had
significant
effects
on
students
learning
patterns
the
latter
used
both
clustering
and
classification
methods
in
terms
of
clustering
the
k-means
algorithm
was
employed
to
produce
groups
of
students
that
shared
similar
learning
patterns
and
subsequently
the
corresponding
human
factors
for
each
group
were
identified
as
far
as
classification
is
concerned
the
students
learning
patterns
were
analyzed
using
decision
trees
with
which
rules
were
produced
for
the
automatic
identification
of
students
cognitive
styles
based
on
their
learning
patterns
3
3
1
traditional
statistics
to
determine
if
there
are
statistically
significant
differences
among
the
learning
patterns
of
three
cognitive
style
groups
the
data
collected
from
the
log
file
were
also
used
for
analysis
with
the
statistical
package
for
the
social
sciences
spss
for
windows
release
10
0
the
independent
variable
was
the
participants
cognitive
styles
the
dependent
variables
were
the
eight
attributes
described
in
section
3
3
a
series
of
analyses
of
variance
anova
was
used
to
test
for
differences
in
each
of
the
dependent
variables
because
it
is
suitable
to
test
the
significant
differences
of
three
or
more
independent
groups
pearson
s
correlation
which
is
appropriate
to
reveal
the
nature
and
extent
of
associations
between
two
variables
stephen
and
hornby
1997
was
applied
to
find
the
correlations
between
attributes
and
the
correlations
between
wa
scores
and
the
eight
attributes
a
significance
level
of
p
0
05
was
adopted
for
the
study
the
mean
value
and
standard
deviation
of
each
dependent
variable
are
also
presented
in
a
table
3
3
2
clustering
k-means
is
one
of
the
simplest
clustering
algorithms
for
grouping
objects
with
similar
features
in
k-means
the
number
k
of
clusters
is
fixed
before
the
algorithm
runs
the
algorithm
randomly
picks
k
cluster
centers
assigns
each
point
to
the
cluster
whose
mean
is
closest
in
a
euclidean
distance
sense
then
computes
the
mean
vectors
of
the
points
assigned
to
each
cluster
and
uses
these
as
new
centers
in
an
iterative
approach
hand
et
al
2001
in
detail
the
first
step
is
to
define
k
centroids
centers
one
for
each
cluster
another
parameter
called
seed
s
is
used
to
generate
the
random
numbers
for
the
assignment
of
the
initial
centroids
following
that
the
algorithm
takes
each
datapoint
and
associates
it
with
the
closest
centroid
the
next
step
starts
when
all
the
datapoints
are
assigned
to
clusters
it
is
the
recalculation
procedure
of
k
new
centroids
for
these
k
new
centroids
a
new
binding
has
to
be
done
between
the
same
dataset
points
and
the
closest
new
centroid
these
two
steps
are
alternated
until
a
stopping
criterion
is
met
that
is
when
there
is
no
further
change
in
the
assignment
of
the
datapoints
evgenia
et
al
2004
the
outcome
of
the
algorithm
reveals
the
centroid
or
means
vector
of
each
cluster
as
well
as
statistics
on
the
number
and
percentage
of
instances
assigned
to
different
clusters
thus
centroids
can
be
used
to
characterize
the
behavior
of
each
of
the
formed
clusters
one
of
the
challenging
issues
in
using
the
k-means
algorithm
for
data
clustering
is
that
one
needs
to
decide
a
suitable
number
of
clusters
k
in
advance
this
prerequisite
can
be
a
drawback
to
other
experiments
and
domains
but
not
for
this
particular
experiment
since
the
dataset
is
not
large
we
ran
the
algorithm
for
k
2
10
the
other
issue
is
that
k-means
is
sensitive
to
how
clusters
are
initially
assigned
this
can
be
overcome
by
trying
different
values
for
the
seed
number
s
and
evaluate
results
in
order
to
determine
which
combination
fits
the
data
better
bandyopadhyay
and
maulik
2002
another
evaluation
was
needed
to
compare
various
combinations
of
the
attributes
that
characterize
each
user
the
goals
of
these
evaluations
were
to
minimize
variability
within
clusters
and
maximize
variability
between
clusters
in
other
words
the
similarity
rules
will
apply
maximally
to
the
members
of
one
cluster
and
minimally
to
members
belonging
to
the
rest
of
the
clusters
more
specifically
the
evaluations
will
emphasize
how
similar
the
behavior
of
a
particular
user
is
to
user
of
the
same
cluster
and
how
different
the
behavior
of
users
in
that
clusters
from
the
behavior
of
users
of
all
other
clusters
the
results
of
the
evaluations
indicated
that
there
were
no
significant
differences
among
various
attribute
combinations
and
that
the
most
efficient
outcome
was
obtained
with
a
value
of
k
3
according
to
the
mean
values
and
standard
deviations
of
attributes
and
the
percentage
of
instances
of
each
cluster
3
3
3
classification
among
data
mining
techniques
decision
tree
is
one
of
the
most
frequently
used
methods
for
classification
a
decision
tree
is
used
to
discover
rules
and
relationships
by
systematically
subdividing
the
information
contained
in
data
chou
1991
chen
et
al
2003
typically
a
decision
tree
algorithm
begins
with
the
entire
set
of
data
splits
the
data
into
two
or
more
subsets
based
on
the
values
of
one
or
more
attributes
and
then
repeatedly
splits
each
subset
into
finer
subsets
until
the
size
of
each
subset
reaches
an
appropriate
level
the
entire
modeling
process
can
be
represented
in
a
tree
structure
and
the
model
generated
can
be
summarized
as
a
set
of
if
then
rules
li
2005
the
value
of
the
decision
tree
reflects
its
easy
understanding
and
a
simple
top
down
tree
structure
hsu
et
al
2003
in
tree-structured
representations
a
set
of
data
is
represented
by
a
node
and
the
entire
data
set
is
represented
as
a
root
node
when
a
split
is
made
several
child
nodes
which
correspond
to
partitioned
data
subsets
are
formed
if
a
node
is
not
to
be
split
any
further
it
is
called
a
leaf
otherwise
it
is
an
internal
node
in
this
study
we
deal
with
binary
trees
where
each
split
produces
exactly
two
child
nodes
some
popular
algorithms
of
decision
trees
are
classification
regression
trees
c
rt
breiman
et
al
1984
chi-squared
automatic
interaction
detection
chaid
kass
1980
and
c4
5
quinlan
1993
in
this
study
we
have
used
the
c4
5
because
it
is
a
well-known
classification
algorithm
and
can
generate
easily
understandable
rules
ding
et
al
2002
the
main
goal
of
the
algorithm
is
to
discover
relationships
between
a
given
classification
of
objects
and
a
set
of
attributes
the
output
of
the
algorithm
is
a
classification
tree
showing
how
objects
may
be
assigned
to
the
given
classes
on
the
basis
of
the
values
of
these
attributes
andrienko
and
andrienko
1999
tree
production
has
three
phases
in
this
algorithm
in
phase
i
an
initial
and
large
tree
is
created
from
the
sets
of
examples
according
to
attribute
selection
measures
in
phase
ii
an
error-based
pruning
method
is
used
to
simplify
the
tree
by
discarding
one
or
more
sub-trees
and
replacing
them
with
leaves
or
branches
quinlan
1993
in
phase
iii
the
classification
performance
of
the
decision
tree
is
tested
by
analyzing
the
number
of
correctly
and
incorrectly
classified
instances
the
number
of
correctly
classified
instances
determines
whether
the
decision
tree
can
be
applied
to
the
datasets
or
whether
further
preparation
will
be
necessary
4
results
on
the
basis
of
pearson
s
correlation
there
is
no
significant
negative
relationship
between
the
attributes
the
frequency
of
using
the
hierarchical
map
and
the
number
of
pages
browsed
have
a
significant
positive
correlation
r
304
p
0
05
n
65
followed
by
the
frequency
of
using
the
examples
and
the
number
of
repeated
visits
with
a
significant
positive
correlation
r
260
p
0
05
n
65
however
these
correlations
are
less
than
0
5
so
it
seems
unlikely
that
there
is
a
possible
replication
of
measures
among
the
attributes
that
are
positively
correlated
therefore
all
attributes
were
considered
in
the
aforementioned
three-data
analysis
approaches
section
4
1
described
results
obtained
from
traditional
statistical
tests
i
e
anova
and
pearson
s
correlations
the
results
of
clustering
k-means
algorithm
and
classification
decision
tree
are
presented
in
section
4
2
and
section
4
3
respectively
section
4
4
discusses
the
similarities
and
differences
of
the
results
gained
from
these
three
approaches
4
1
traditional
statistics
to
identify
the
statistical
significance
an
anova
was
conducted
to
compare
the
differences
among
three
cognitive
style
groups
for
each
attribute
table
iv
in
addition
pearson
s
correlation
was
used
to
find
whether
there
was
a
significant
relationship
between
the
wa
score
and
each
attribute
table
v
the
results
appeared
to
be
consistent
five
attributes
were
found
to
be
significant
in
both
analyses
the
frequencies
of
looking
at
examples
and
details
and
the
frequencies
of
using
main
menu
hierarchical
maps
and
alphabetical
index
no
significant
differences
were
found
in
these
two
analyses
for
two
attributes
that
is
the
frequency
of
using
overview
and
the
number
of
repeated
visits
however
the
number
of
pages
browsed
was
significant
in
pearson
s
correlations
but
there
was
no
significance
based
on
the
analysis
with
anova
a
possible
explanation
for
this
inconsistency
is
that
three
intermediate
students
had
wa
scores
near
a
border
with
field-dependent
students
with
whom
they
might
share
some
similar
learning
patterns
post-hoc
analysis
of
mean
differences
using
the
tukey
test
showed
significance
differences
for
the
frequencies
of
using
the
hierarchical
map
and
the
main
menu
between
field-dependent
students
and
field-independent
p
0
01
or
intermediate
students
p
0
05
but
not
between
field-independent
and
intermediate
students
p
0
05
tukey
test
analyses
also
revealed
that
fieldindependent
students
significantly
chose
the
alphabetical
index
and
examined
the
details
many
more
times
than
field-dependent
p
0
01
and
intermediate
students
p
0
01
but
there
were
no
significant
differences
between
fielddependent
and
intermediate
students
p
0
05
table
vi
presents
the
mean
and
standard
deviation
of
the
frequency
of
each
attribute
for
each
cognitive
style
group
4
2
clustering
the
purpose
of
clustering
is
to
group
learners
based
on
their
similar
learning
patterns
based
on
this
rationale
three
clusters
were
produced
the
percentage
of
learners
within
each
cluster
is
satisfactory
for
the
total
number
of
65
instances
clusters
can
be
characterized
as
well
balanced
cluster
0
n
22
34
cluster
1
n
23
35
cluster
2
n
20
31
the
mean
value
and
standard
deviation
of
each
attribute
for
each
cluster
are
shown
in
table
vii
where
learners
are
grouped
according
to
the
following
trends
--cluster
0
c0
learners
visited
few
pages
made
few
repeated
visits
and
seldom
accessed
any
navigation
tools
and
subject
categories
--cluster
1
c1
learners
visited
some
pages
made
some
repeated
visits
frequently
accessed
the
alphabetical
index
and
looked
at
the
detailed
descriptions
--cluster
2
c2
learners
visited
many
pages
made
many
repeated
visits
frequently
accessed
the
hierarchical
map
and
looked
at
the
examples
when
analyzing
the
wa
score
of
each
cluster
we
found
that
there
is
a
significant
difference
f
2
62
5
878
p
0
01
cluster
1
has
the
highest
wa
score
mean
1
4
standard
deviation
0
38
while
cluster
2
has
the
lowest
wa
score
mean
1
0
standard
deviation
0
24
in
other
words
the
learners
of
cluster
1
have
a
field-independent
tendency
whereas
the
learners
of
cluster
2
have
a
field-dependent
tendency
these
results
seem
consistent
with
the
distribution
of
cognitive
styles
among
the
three
clusters
figure
3
fieldindependent
learners
n
14
56
mainly
emerge
in
cluster
1
in
which
the
learners
frequently
examined
the
detailed
descriptions
accessed
the
alphabetical
index
many
more
times
and
browsed
fewer
pages
conversely
the
majority
of
field-dependent
learners
n
12
57
appear
in
cluster
2
in
which
learners
often
accessed
the
hierarchical
map
browsed
many
more
pages
and
favored
learning
from
examples
unlike
field-independent
and
field-dependent
learners
intermediate
learners
of
each
cluster
are
almost
balanced
cluster
0
n
7
36
cluster
1
n
6
32
cluster
2
n
6
32
to
identify
whether
cognitive
style
was
the
only
one
human
factor
that
directed
students
learning
patterns
the
significant
differences
of
other
human
factors
among
the
three
clusters
were
also
examined
none
of
them
are
significant
including
their
previous
experience
using
computers
f
3
61
2
008
p
0
05
the
internet
f
3
61
0
126
p
0
05
and
computer-aided
learning
f
4
60
2
685
p
0
05
4
3
classification
figure
4
shows
the
decision
tree
that
was
generated
for
characterizing
cognitive
styles
according
to
this
decision
tree
the
frequencies
of
using
the
examples
and
the
hierarchical
maps
are
the
most
important
and
second
most
important
features
to
classify
the
cognitive
styles
respectively
on
the
third
level
the
frequency
of
using
the
alphabetical
index
and
the
number
of
pages
browsed
are
also
relevant
as
shown
in
this
decision
tree
the
examples
were
mainly
chosen
by
field-dependent
students
and
this
result
is
coherent
with
those
from
clustering
and
traditional
statistics
it
is
worth
noting
that
the
hierarchical
map
and
alphabetical
index
could
have
been
used
by
both
fielddependent
and
field-independent
students
but
they
used
these
navigation
tools
in
different
ways
field-dependent
students
who
either
frequently
used
the
hierarchical
map
or
frequently
used
the
alphabetical
index
browsed
many
more
pages
conversely
field-independent
students
who
frequently
used
the
hierarchical
map
visited
fewer
pages
field-independent
students
who
rarely
used
the
hierarchical
map
and
the
alphabetical
index
looked
at
the
detailed
descriptions
many
more
times
in
other
words
such
detailed
description
may
be
the
major
source
for
their
learning
these
learning
patterns
are
not
derived
from
traditional
statistical
and
clustering
approaches
in
addition
to
illustrating
the
students
learning
patterns
the
decision
tree
can
also
be
converted
to
create
rules
for
the
predictions
of
the
students
cognitive
styles
that
is
identifying
the
cognitive
styles
of
new
students
based
on
their
learning
patterns
in
total
there
are
eight
rules
drawn
from
the
decision
tree
different
rules
lead
to
different
cognitive
styles
the
first
rule
obtained
from
the
right
side
of
the
second
level
of
the
tree
indicates
that
students
who
visit
the
examples
more
than
11
times
are
classified
field-dependent
students
the
second
rule
taken
from
the
right
side
of
the
fourth
level
shows
that
students
who
use
the
map
more
than
10
times
and
browse
less
than
or
equal
to
57
pages
are
classified
as
field-independent
students
the
remaining
rules
were
acquired
from
the
bottom
level
of
the
tree
the
third
to
the
sixth
rules
were
gained
from
the
left
side
while
the
seventh
and
eighth
rules
were
from
the
right
side
in
terms
of
the
former
the
third
rule
is
that
students
who
use
the
map
less
than
or
equal
to
10
times
and
use
the
index
less
than
or
equal
to
9
times
and
visit
the
detailed
descriptions
less
than
or
equal
to
12
times
are
classified
as
intermediate
students
the
fourth
rule
is
that
students
who
use
the
map
less
than
or
equal
to
10
times
and
use
the
index
less
than
or
equal
to
9
times
and
visit
the
detailed
descriptions
more
than
12
times
are
classified
as
field-independent
students
the
fifth
rule
is
that
students
who
use
the
map
less
than
or
equal
to
10
times
and
use
the
index
more
than
9
times
and
browse
less
than
or
equal
to
45
pages
are
classified
as
intermediate
students
the
sixth
rule
is
that
students
who
use
the
map
less
than
or
equal
to
10
times
and
use
the
index
more
than
9
times
and
browse
more
than
45
pages
are
classified
as
field-dependent
students
the
seventh
rule
is
that
students
who
use
the
map
more
than
10
times
and
browse
more
than
57
pages
and
use
the
main
menu
less
than
or
equal
to
6
times
are
classified
as
intermediate
students
the
eighth
rule
is
that
students
who
use
the
map
more
than
10
times
and
browse
more
than
57
pages
and
use
the
menu
more
than
6
times
are
classified
as
field-dependent
students
as
shown
in
these
eight
rules
a
cognitive
style
could
be
reached
by
more
than
one
rule
in
this
case
three
rules
are
connected
with
field-dependent
students
three
are
associated
with
intermediate
students
and
the
other
two
are
related
to
field-independent
students
these
rules
can
be
applied
to
replace
the
csa
or
other
cognitive
style
tests
and
work
as
criteria
for
automatic
identification
of
the
students
cognitive
styles
in
other
words
the
students
cognitive
styles
can
be
automatically
recognized
based
on
their
learning
patterns
which
can
contribute
to
the
development
of
personalized
learning
environments
on
the
web
4
4
summary
regarding
whether
these
attributes
are
related
to
cognitive
styles
the
results
obtained
from
the
four
data
analysis
approaches
are
rather
similar
table
viii
the
common
relevant
attributes
are
the
frequencies
of
looking
at
examples
and
details
and
frequencies
of
using
the
main
menu
hierarchical
maps
and
alphabetical
index
the
common
irrelevant
attributes
are
the
frequencies
of
looking
at
the
overview
and
the
number
of
repeated
visits
the
number
of
pages
browsed
is
recognized
as
a
critical
attribute
based
on
the
results
obtained
from
the
k-means
algorithm
the
decision
tree
and
the
pearson
s
correlations
but
the
results
from
anova
indicate
that
there
is
no
statistical
significance
5
discussions
as
shown
in
the
previous
section
cognitive
style
has
significant
effects
on
student
learning
patterns
in
the
wbi
program
in
particular
they
have
different
preferences
in
the
selection
of
navigation
tools
and
display
options
the
difference
in
their
preferences
may
be
a
reflection
of
differences
in
the
students
cognitive
styles
wildemuth
et
al
1998
for
example
intermediate
learners
of
each
cluster
are
almost
balanced
it
suggests
that
they
are
equally
comfortable
using
each
learning
pattern
one
possible
interpretation
is
that
individuals
possessing
an
intermediate
cognitive
style
combine
the
characteristics
of
both
field
independence
and
field
dependence
and
employ
a
more
versatile
repertoire
of
learning
strategies
versatile
learners
who
have
acquired
the
skill
to
move
back
and
forth
between
different
learning
strategies
are
more
capable
of
adapting
themselves
to
the
learning
systems
ford
2000
in
addition
the
different
learning
patterns
showed
by
field-independent
and
field-dependent
learners
also
echo
their
characteristics
which
are
discussed
in
the
following
5
1
analytical
vs
global
a
major
issue
in
identifying
differences
between
field-independent
and
fielddependent
students
is
their
separate
tendencies
to
adopt
analytical
and
global
approaches
this
difference
may
explain
the
results
of
this
study
that
field-independent
students
tend
to
browse
fewer
pages
than
field-dependent
students
a
possible
interpretation
of
this
finding
is
that
field-independent
students
tend
to
be
more
analytical
ford
et
al
1994
and
they
are
very
task
oriented
witkin
et
al
1977
hence
they
only
pay
attention
to
particular
topics
related
to
their
learning
and
thus
browse
fewer
pages
on
the
contrary
field-dependent
students
perceive
objects
as
a
whole
and
process
information
in
a
global
fashion
goodenough
1976
therefore
they
tend
to
browse
many
more
pages
to
build
an
overall
picture
of
the
subject
content
such
findings
strengthen
the
claim
of
previous
research
that
field-independent
people
are
good
at
analytical
thought
whereas
field-dependent
people
have
global
perceptions
witkin
et
al
1977
5
2
active
vs
passive
in
this
study
field-dependent
and
field-independent
students
show
different
preferences
for
navigation
tools
field-independent
students
often
access
the
alphabetical
index
which
provides
users
with
means
to
locate
particular
information
without
going
through
a
fixed
sequence
chen
and
macredie
2002
this
may
be
due
to
the
fact
that
field-independent
students
actively
segment
information
into
relevant
parts
goodenough
1976
conversely
fielddependent
students
often
use
the
hierarchical
map
which
applies
a
graphical
representation
to
illustrate
the
relationships
among
different
concepts
turns
et
al
2000
and
reflects
the
conceptual
structure
of
the
subject
content
nilsson
and
mayer
2002
this
might
be
caused
by
the
fact
that
fielddependent
students
tend
to
take
a
passive
approach
by
relying
on
salient
cues
anastasi
1988
and
they
have
more
difficulties
in
separating
the
individual
parts
within
a
whole
witkin
et
al
1977
with
the
extra
guidance
provided
by
the
hierarchical
map
field-dependent
students
can
more
easily
access
meaningful
information
the
findings
in
general
support
previous
studies
evans
and
edwards
1999
hsu
and
schwen
2003
5
3
internal
vs
external
the
results
of
our
study
have
shown
that
different
cognitive
style
groups
tend
to
favor
different
display
options
field-dependent
students
often
look
at
examples
while
field-independent
students
frequently
examine
the
detailed
descriptions
examples
are
down-to-earth
visual
material
that
provides
illustrations
with
practical
cases
to
be
helpful
to
students
in
deepening
their
understandings
this
approach
is
beneficial
to
field-dependent
individuals
who
rely
more
on
external
frames
of
reference
and
operate
best
when
analyses
are
already
provided
lyons-lawrence
1994
the
detailed
descriptions
presents
text-based
explanations
of
theoretical
concepts
which
are
very
detailed
but
they
do
not
illustrate
the
concepts
with
practical
cases
therefore
there
is
a
lack
of
concrete
guidance
for
students
students
need
to
transfer
their
knowledge
into
an
activity
by
themselves
this
may
not
interfere
with
field-independent
students
who
use
an
internal
frame
of
reference
to
organize
information
reiff
1996
and
structure
problems
davis
and
cochran
1989
therefore
they
are
more
able
to
reorganize
information
from
the
detailed
descriptions
and
create
useful
cues
on
their
own
this
finding
is
in
accordance
with
that
of
previous
work
that
field-independent
individuals
have
more
theoretical
and
abstract
interests
miller
and
escolme
1990
6
implications
for
system
design
the
previous
discussion
suggests
that
student
learning
patterns
are
significantly
influenced
by
their
cognitive
styles
field-independent
and
fielddependent
learners
have
different
characteristics
that
affect
their
learning
patterns
this
implies
that
wbi
programs
should
be
developed
to
support
the
unique
needs
of
each
cognitive
style
group
more
specifically
the
wbi
programs
should
be
flexible
enough
to
offer
multiple
options
tailored
to
the
distinctive
cognitive
styles
to
this
end
a
model
figure
5
which
was
developed
using
the
results
presented
in
section
4
can
be
considered
as
a
mechanism
to
help
designers
develop
wbi
programs
that
can
accommodate
the
preferences
of
both
field-independent
and
field-dependent
learners
the
core
of
the
model
consists
of
three
key
design
elements
of
wbi
programs
navigation
tools
display
options
and
content
scope
6
1
navigation
tools
in
this
study
field-independent
students
tend
to
actively
group
relevant
concepts
from
the
alphabetical
index
by
themselves
while
field-dependent
students
tend
to
be
passive
and
rely
on
the
hierarchical
map
to
build
the
relationships
among
different
concepts
one
of
the
solutions
to
accommodate
their
different
preferences
is
to
allow
the
learners
to
see
both
navigation
tools
at
the
same
time
by
using
frames
frame
uses
an
underlying
grid
layout
which
allows
multiple
documents
to
be
shown
in
a
window
at
once
milunovich
2002
navigation
control
is
one
of
the
benefits
of
using
frames
one
frame
can
always
stay
visible
even
if
the
contents
of
the
other
frame
change
hobbs
1999
in
this
case
there
are
two
frames
the
hierarchical
map
frame
and
the
alphabetical
index
frame
the
hierarchical
map
frame
is
always
visible
and
it
corresponds
to
information
selected
from
the
alphabetical
index
frame
in
other
words
the
selected
topic
in
the
alphabetical
index
frame
would
be
highlighted
in
the
hierarchical
map
frame
in
doing
so
field-independent
students
can
use
the
alphabetical
index
frame
to
select
a
relevant
topic
and
field-dependent
students
can
identify
the
relationships
between
this
topic
and
other
topics
from
the
hierarchical
map
frame
6
2
display
options
the
results
of
our
study
have
shown
that
field-independent
students
are
capable
of
extracting
relevant
information
from
the
detailed
description
which
lacks
practical
instruction
because
they
have
a
tendency
to
use
their
own
internal
references
on
the
other
hand
field-dependent
students
rely
more
heavily
on
external
cues
thus
they
prefer
to
get
concrete
guidance
from
examples
one
of
the
possible
ways
to
address
their
different
needs
is
to
show
both
of
the
display
options
detailed
description
and
concrete
examples
within
a
table
by
using
a
table
all
of
the
relevant
information
about
a
particular
case
can
gathered
together
in
one
place
for
example
one
column
can
be
used
to
present
the
detailed
descriptions
of
a
particular
topic
while
the
other
column
provides
the
illustration
with
examples
for
that
topic
this
approach
not
only
lets
field-independent
students
have
a
look
at
detailed
descriptions
but
also
provides
field-dependent
students
with
concrete
examples
in
other
words
the
information
needs
of
both
cognitive
style
groups
can
be
met
simultaneously
6
3
content
scope
field-independent
students
approach
an
environment
in
an
analytical
fashion
so
they
tend
to
focus
on
information
that
is
relevant
to
their
tasks
hence
they
browse
fewer
pages
to
directly
get
to
relevant
topics
for
completing
their
tasks
conversely
field-dependent
students
use
a
global
approach
to
process
information
so
they
tend
to
build
an
overall
picture
by
browsing
more
pages
one
of
the
potential
solutions
to
deal
with
their
different
requirements
is
to
use
a
pop-up
window
which
is
a
secondary
window
to
provide
additional
information
about
selected
objects
by
clicking
a
hypertext
link
bell
et
al
2002
in
this
case
a
pop-up
window
can
be
used
to
show
additional
topics
for
field-dependent
students
who
would
like
to
get
a
global
picture
of
the
subject
content
however
these
windows
can
be
switched
off
lest
they
irritate
field-independent
students
who
have
a
tendency
to
focus
on
specific
relevant
topics
in
other
words
information
that
is
related
to
tasks
is
put
in
the
main
window
for
field-independent
students
while
unrelated
information
is
displayed
with
a
pop-up
window
for
field-dependent
students
7
conclusions
wbi
creates
learning
opportunities
for
everyone
if
suitable
considerations
are
made
in
the
design
process
otherwise
they
can
impose
needless
barriers
to
equal
participation
in
educational
settings
the
experimental
results
obtained
in
this
study
suggest
that
cognitive
style
plays
an
influential
role
in
student
learning
patterns
within
wbi
field-independent
and
field-dependent
learners
have
different
preferences
for
locating
information
especially
for
the
selection
of
navigation
tools
and
display
options
thus
there
is
a
need
to
be
aware
of
cognitive
styles
when
planning
to
improve
the
usability
and
functionality
of
wbi
programs
the
contribution
of
this
study
includes
three
aspects
theory
methodology
and
applications
in
terms
of
theory
this
study
deepens
the
understanding
of
the
importance
of
cognitive
styles
in
the
development
of
wbi
programs
by
providing
empirical
evidence
cognitive
styles
gender
differences
and
system
experience
are
factors
that
are
frequently
considered
in
the
literature
of
individual
differences
but
it
is
inconclusive
as
to
their
relative
importance
the
findings
of
this
study
indicated
that
cognitive
style
is
a
major
factor
that
influences
student
learning
patterns
however
it
was
only
one
relatively
small
study
further
work
needs
to
be
undertaken
with
a
larger
sample
to
provide
additional
evidence
with
regard
to
methodology
this
study
analyzed
the
experimental
data
with
a
data
mining
approach
which
used
both
clustering
and
classification
techniques
these
two
techniques
are
complementary
in
that
they
integrate
the
analysis
of
macro
and
micro
levels
the
results
from
clustering
present
an
overall
picture
of
the
students
learning
patterns
whereas
those
from
classification
provide
the
detailed
rules
for
the
automatic
identification
of
students
cognitive
styles
based
on
their
learning
patterns
however
this
study
only
used
two
methods
that
is
k-means
clustering
and
decision
trees
classification
given
any
dataset
there
are
often
no
strict
rules
that
impose
the
use
of
a
specific
method
over
another
in
its
analysis
therefore
it
is
necessary
to
conduct
further
work
to
analyze
student
learning
patterns
using
other
clustering
or
classification
methods
for
examples
self-organizing
maps
and
support
vector
machines
it
would
be
interesting
to
see
whether
similar
results
can
be
found
by
using
these
methods
as
far
as
the
application
is
concerned
this
study
recognized
the
importance
of
versatility
in
the
development
of
wbi
programs
and
developed
a
model
to
illustrate
the
needs
of
different
cognitive
styles
in
addition
several
design
approaches
were
proposed
to
accommodate
the
preferences
of
both
fieldindependent
and
field-dependent
learners
in
the
future
the
rationale
of
the
model
and
the
design
approaches
can
be
used
to
improve
the
development
of
existing
wbi
programs
and
other
web-based
applications
such
as
digital
libraries
search
engines
and
electronic
journals
finally
it
would
be
valuable
to
see
whether
such
wbi
programs
and
the
web-based
applications
can
promote
learners
performance
and
increase
their
satisfaction
repeatable
evaluation
of
search
services
in
dynamic
environments
eric
c
jensen
summize
inc
steven
m
beitzel
illinois
institute
of
technology
abdur
chowdhury
summize
inc
and
ophir
frieder
illinois
institute
of
technology
and
georgetown
university
in
dynamic
environments
such
as
the
world
wide
web
a
changing
document
collection
query
population
and
set
of
search
services
demands
frequent
repetition
of
search
effectiveness
relevance
evaluations
reconstructing
static
test
collections
such
as
in
trec
requires
considerable
human
effort
as
large
collection
sizes
demand
judgments
deep
into
retrieved
pools
in
practice
it
is
common
to
perform
shallow
evaluations
over
small
numbers
of
live
engines
often
pairwise
engine
a
vs
engine
b
without
system
pooling
although
these
evaluations
are
not
intended
to
construct
reusable
test
collections
their
utility
depends
on
conclusions
generalizing
to
the
query
population
as
a
whole
we
leverage
the
bootstrap
estimate
of
the
reproducibility
probability
of
hypothesis
tests
in
determining
the
query
sample
sizes
required
to
ensure
this
finding
they
are
much
larger
than
those
required
for
static
collections
we
propose
a
semiautomatic
evaluation
framework
to
reduce
this
effort
we
validate
this
framework
against
a
manual
evaluation
of
the
top
ten
results
of
ten
web
search
engines
across
896
queries
in
navigational
and
informational
tasks
augmenting
manual
judgments
with
pseudo-relevance
judgments
mined
from
web
taxonomies
reduces
both
the
chances
of
missing
a
correct
pairwise
conclusion
and
those
of
finding
an
errant
conclusion
by
approximately
50
1
introduction
evaluating
the
effectiveness
of
information
retrieval
systems
in
terms
of
relevance
requires
a
large
amount
of
human
effort
many
environments
such
as
the
world
wide
web
grow
and
change
too
rapidly
for
a
single
evaluation
to
carry
meaning
for
any
extended
period
changes
in
their
document
collection
query
population
and
set
of
search
services
demand
the
repetition
of
evaluations
over
time
in
these
environments
static
test
collections
become
outdated
too
quickly
and
require
too
much
effort
to
reconstruct
rather
practitioners
often
compare
a
small
number
of
live
engines
by
judging
every
result
retrieved
at
a
shallow
depth--without
system
pooling
the
number
of
queries
necessary
for
such
an
evaluation
to
be
reliable1
must
be
determined
however
we
hypothesize
that
combining
automatic
evaluation
techniques
with
a
smaller
set
of
manual
relevance
judgments
can
provide
more
reliable
pairwise
conclusions
engine
a
outperforms
engine
b
than
the
manual
set
alone
we
propose
a
semiautomatic
framework
for
combining
manually
judged
queries
with
automatically
evaluated
ones
our
ultimate
goal
being
to
reduce
manual
evaluation
effort
by
finding
reliable
conclusions
using
less
manually
judged
queries
to
test
our
hypothesis
we
adopt
the
reproducibility
probability
probability
of
observing
a
significant
clinical
result
from
a
future
trial
shao
and
chow
2002
as
our
estimate
of
reliability
we
then
compare
conclusions
drawn
with
high
reproducibility
probability
from
semiautomatic
evaluations
against
those
from
a
manual
evaluation
of
the
top
ten
results
of
ten
web
search
engines
over
896
queries
2
the
available
content
on
the
web
changes
8
every
week
along
with
dramatic
changes
in
the
number
of
servers
and
pages
cho
et
al
2000
ntoulas
et
al
2004
in
our
experimentation
we
found
that
only
61
of
web
search
engines
top
ten
results
remained
the
same
three
months
later
on
average
and
only
38
for
the
most
changed
engine
jensen
2006
searchers
interests
and
the
popular
queries
they
use
to
express
them
are
also
in
a
constant
state
of
flux
with
20
of
even
the
30
000
most
popular
queries
changing
from
one
week
to
the
next
and
less
than
half
remaining
the
same
after
six
months
pass
et
al
2006
even
the
topical
categories
these
queries
fall
into
have
changing
relative
popularities
within
days
weeks
months
and
years
beitzel
et
al
2004b
2006
jansen
and
spink
2005
not
only
is
the
query
population
rapidly
changing
but
its
size
and
diversity
also
indicate
that
a
large
number
of
queries
are
required
to
construct
a
representative
random
sample
pass
et
al
2006
popular
queries
and
even
popular
query
terms
make
up
only
a
small
portion
of
the
total
query
stream
with
approximately
half
of
all
queries
being
repeated
ten
or
fewer
times
over
a
week
beitzel
et
al
2006
jansen
et
al
2005
developing
new
algorithms
or
even
tuning
traditional
retrieval
strategies
for
emerging
applications
image
search
blog
search
etc
requires
reliable
repeatable3
evaluations
on
their
respective
dynamic
environments
static
test
collections
such
as
those
constructed
for
the
text
retrieval
conference
trec
become
outdated
too
quickly
to
address
these
changes
in
popular
queries
and
their
associated
relevant
results
with
typical
trec
evaluations
requiring
well
over
500
assessor-hours
see
section
2
these
sorts
of
collections
are
too
expensive
to
reconstruct
when
changes
in
effectiveness
over
time
must
be
measured
this
effort
is
exacerbated
by
rapidly
growing
collection
sizes
as
the
reusability
of
such
collections
depends
on
the
depth
of
their
pooled
evaluations
also
detailed
in
section
2
therefore
practitioners
in
dynamic
environments
often
dispense
with
efforts
to
build
reusable
test
collections
in
favor
of
reevaluating
each
engine
as
decisions
are
required
however
based
on
analysis
of
our
manual
evaluation
we
find
that
such
shallow
judgments
demand
a
large
number
of
queries
to
provide
reliable
conclusions
as
many
as
650
in
our
environment
a
method
of
reducing
the
effort
needed
to
draw
reliable
conclusions
in
such
an
environment
is
needed
to
make
the
repetition
of
such
large
evaluations
over
time
feasible
we
propose
a
semiautomatic
framework
that
incorporates
automatically
evaluated
queries
using
pseudo-relevance
judgments
with
manually
judged
ones
this
provides
insight
into
conclusions
earlier
in
the
evaluation
process
so
that
poorly
performing
engines
can
be
eliminated
before
judging
every
result
from
every
engine
over
a
large
query
sample
we
identify
two
methods
for
integrating
automatic
judgments
each
provides
a
different
form
of
guidance
for
evaluators
to
reach
reliable
conclusions
with
less
effort
than
manual
judgments
alone
semiautomatic
filtering
verify
conclusions
drawn
from
a
smaller
number
of
manual
judgments
based
on
their
agreement
with
automatic
techniques
semiautomatic
prediction
directly
combine
automatically
judged
queries
with
manual
ones
to
yield
samples
of
larger
sizes
whose
conclusions
can
be
used
as
an
estimate
of
those
that
might
be
found
with
that
many
manual
judgments
to
test
our
hypothesis
that
this
semiautomatic
framework
yields
more
reliable
conclusions
than
those
available
from
the
manually
judged
sample
alone
we
must
adopt
a
specific
method
of
estimating
reliability
we
use
reproducibility
probability
how
likely
a
pairwise
conclusion
is
to
hold
across
any
query
sample
of
a
given
size
as
our
estimate
of
reliability
for
two
reasons
first
measuring
changes
in
performance
over
time
in
a
dynamic
environment
demands
conclusions
that
generalize
to
the
query
population
as
a
whole
at
the
time
of
evaluation
if
applying
an
identical
evaluation
methodology
to
different
query
samples
from
the
same
time
period
yields
inconsistent
conclusions
nothing
can
be
concluded
about
changes
in
engine
performance
over
time
comparing
the
conclusions
from
any
two
evaluations
that
use
different
query
samples
would
be
impossible
implicit
in
this
assertion
is
our
view
of
the
query
stream
at
a
given
point
in
time
as
a
hypothetical
infinite
population
in
following
with
the
frequentist
approach
we
adopt
well
reviewed
recently
for
information
retrieval
in
cormack
and
lynam
2006
second
we
seek
to
reduce
manual
evaluation
effort
by
exploiting
the
fact
that
larger
differences
in
evaluation
scores
are
detectable
with
smaller
query
sample
sizes
possibly
available
from
an
evaluation
in
progress
information
retrieval
traditionally
uses
a
priori
heuristics
for
determining
the
necessary
query
sample
size
to
yield
a
desired
level
of
reliability
such
as
trec
rules
of
thumb
about
the
minimum
absolute
difference
between
scores
often
derived
from
empirical
meta-evaluation
see
section
2
2
3
however
these
do
not
address
the
problem
of
detecting
reliable
conclusions
from
an
evaluation
in
progress
we
leverage
the
pointwise
bootstrap
estimate
of
reproducibility
probability
of
hypothesis
tests
that
quantifies
the
reliability
of
conclusions
from
any
pairwise
evaluation
without
the
prerequisite
of
a
sufficient
query
sample
size
to
estimate
parameters
such
as
the
mean
score
difference
or
a
context
of
meta-evaluation
over
a
diverse
set
of
engine
pairs
the
ability
to
develop
intelligent
evaluation
strategies
such
as
discarding
results
from
an
engine
that
is
clearly
inferior
based
on
a
small
number
of
judgments
is
largely
unexplored
because
the
running
averages
available
from
evaluations
over
small
query
sample
sizes
have
been
shown
to
be
unreliable
when
viewed
as
whole
voorhees
and
buckley
2002
quantifying
the
utility
of
intelligent
evaluation
strategies
is
also
difficult
using
existing
methods
of
comparing
evaluations
meta-evaluation
for
example
prior
automatic
evaluation
and
implicit
preference
research
reviewed
in
section
2
3
focuses
on
optimizing
the
correlation
of
engine
rankings
from
a
purely
automatic
evaluation
to
a
manual
one
however
critical
decisions
such
as
which
search
service
to
employ
and
so
forth
demand
a
more
rigorous
comparison
of
conclusions
drawn
by
these
methods
with
those
from
manual
judgment
by
leveraging
reproducibility
probability
we
ensure
only
conclusions
with
high
reproducibility
probability
are
compared
those
that
would
not
generalize
to
other
query
samples
using
the
same
evaluation
technique
are
considered
ties
next
we
review
related
work
in
information
retrieval
evaluation
and
reliability
estimation
in
section
3
we
show
that
our
manual
web
search
evaluation
is
reliable
and
we
validate
reproducibility
probability
estimation
techniques
on
it
having
established
that
prerequisite
we
propose
and
validate
our
semiautomatic
framework
in
section
4
using
two
simple
automatic
evaluation
techniques
even
with
these
na
ve
techniques
errors
are
often
reduced
by
half
i
compared
to
using
small
sets
of
manual
judgments
alone
more
importantly
metrics
for
comparing
evaluations
and
measuring
the
utility
of
semiautomatic
methods
are
developed
2
related
work
first
we
review
evaluation
of
information
retrieval
systems
on
the
web
we
then
examine
four
methods
for
estimating
the
reliability
of
evaluations
hypothesis
testing
confidence
intervals
empirical
meta-evaluation
and
reproducibility
probability
estimation
finally
we
review
prior
work
in
automatic
evaluation
techniques
2
1
web
search
evaluation
evaluating
the
effectiveness
relevance
of
live
web
search
engines
provides
many
unique
challenges
because
they
operate
on
data
that
are
continually
changing
hawking
et
al
1999
savoy
and
picard
2001
the
set
of
popular
web
queries
and
the
relevant
documents
for
those
queries
changes
dramatically
over
time
pass
et
al
2006
previous
studies
concluded
that
overlap
among
results
from
different
web
search
engines
was
too
high
for
them
to
be
deemed
significantly
different
ding
and
marchionini
1996
however
when
a
decision
must
be
made
some
form
of
reliable
evaluation
is
necessary
most
of
the
work
in
evaluating
search
effectiveness
follows
the
text
retrieval
conference
trec
methodology
for
constructing
reusable
test
collections
trec
holds
constant
the
document
collection
and
query
set
pooling
the
top
ranked
results
up
to
a
given
depth
typically
100
from
each
engine
and
manually
judging
each
document
in
this
pool
as
relevant
or
not
relevant
if
this
judgment
depth
is
large
enough
these
collections
are
reusable
in
that
the
relative
effectiveness
of
runs
from
new
engines
over
the
same
documents
and
queries
can
be
evaluated
simply
by
applying
the
existing
judgments
and
assuming
documents
that
are
not
judged
are
not
relevant
zobel
1998
studies
of
evaluation
in
trec
meta-evaluations
have
shown
that
although
relevance
is
an
ambiguous
concept
borlund
2003
variations
in
relevance
judgments
due
to
assessor
disagreement
do
not
destabilize
evaluation
voorhees
1998
the
trec
web
track
applies
this
methodology
to
static
web
document
collections
the
recognition
that
web
search
users
perform
tasks
other
than
the
trec
standard
informational
task
searching
for
many
relevant
documents
topically
related
to
the
query
has
led
to
the
incorporation
of
navigational
homepage
or
namedpage-finding
evaluations
that
assume
there
is
a
single
best-known
item
sans
duplications
the
searcher
wants
to
find
most
recently
trec
has
begun
to
address
the
question
of
whether
building
reusable
collections
through
pooled
evaluation
is
scalable
to
terabyte-sized
collections
clarke
et
al
2005
recent
work
by
sanderson
and
zobel
2005
shows
that
judging
only
the
top
ten
results
of
each
engine
provides
reliable
evaluation
for
less
effort
than
system
pooling
evaluations
are
very
labor
intensive
our
own
precision
oriented
evaluation
of
the
top
ten
results
of
ten
web
search
engines
over
896
queries
required
225
assessor-hours
to
complete
jensen
et
al
2005
jensen
2006
this
is
approximately
15
minutes
per
query
to
assign
binary
relevance
and
choose
the
best
result
from
an
average
of
43
distinct
results
a
previous
navigational
evaluation
we
performed
selecting
only
the
best
page
and
its
duplicates
from
a
pool
of
six
web
search
engines
results
about
25
on
average
over
418
queries
required
87
hours
or
approximately
12
minutes
per
query
on
average
beitzel
et
al
2003b
creating
reusable
test
collections
such
as
those
developed
in
trec
requires
a
larger
amount
of
effort
recent
trec
efforts
have
employed
six
assessors
generally
working
20
hour
weeks
for
over
a
month
soboroff
2006
the
trec
2001
web
ad
hoc
search
task
required
761
25
assessor-hours
to
perform
judgments
over
50
topics
and
an
additional
283
hours
to
develop
those
topics
the
2004
terabyte
track
ad
hoc
task
required
1037
5
hours
total
with
over
half
spent
performing
judgments
over
50
topics
even
in
the
2003
homepage/named
page
task
where
the
query
was
developed
for
a
prechosen
best
document
the
process
of
simply
checking
shallow
retrieved
pools
for
duplicates
over
the
300
queries
required
as
many
as
100
120
hours
2
2
estimating
evaluation
reliability
the
ultimate
goal
of
evaluation
is
to
facilitate
the
construction
of
engines
that
are
a
meaningful
improvement
over
the
state
of
the
art
however
this
improvement
often
characterized
as
a
level
of
difference
discernable
to
users
may
be
achieved
through
several
iterations
of
reliable
improvements
we
specialize
on
tague-sutcliffe
s
1996
definition
of
reliability
for
the
case
of
a
pairwise
conclusion
from
an
information
retrieval
evaluation
as
its
reproducibility
probability
across
any
random
query
sample
of
equivalent
size
we
focus
only
on
the
reliability
of
conclusions
as
minimum
levels
of
difference
could
easily
be
incorporated
into
such
analysis
by
selecting
a
different
null
hypothesis
and
would
only
increase
the
required
sample
sizes
next
we
review
several
methods
of
estimating
reliability
2
2
1
hypothesis
tests
applying
statistical
hypothesis
tests
to
information
retrieval
evaluations
has
a
history
of
controversy
as
most
tests
rely
on
observations
conforming
to
continuous
often
particular
distributions
but
typical
information
retrieval
evaluation
metrics
are
bounded
discrete
and
often
non-normal
in
nature
van-rijsbergen
1979
bootstrap
hypothesis
tests
such
as
those
applied
to
information
retrieval
evaluation
by
savoy
1997
or
sakai
2006
do
not
require
these
assumptions
because
they
estimate
the
empirical
distribution
by
resampling
thousands
of
times
when
performing
a
handful
of
these
tests
this
computational
cost
is
not
of
consequence
but
estimating
their
power
is
computationally
and
theoretically
challenging
davidson
and
mackinnon
2006
therefore
we
choose
the
wilcoxon
signed
rank
test
with
standard
corrections
for
noncontinuity
in
our
experimentation
because
its
nonparametric
nature
does
not
require
assuming
a
particular
distribution
but
it
is
easily
calculable
and
maintains
higher
power
than
very
simple
tests
such
as
the
sign
test
hollander
and
wolfe
1973
although
the
reproducibility
probability
of
any
test
could
be
estimated
with
the
nonparametric
bootstrap
we
leverage
below
the
distribution
of
scores
in
our
evaluation
motivated
this
decision
they
failed
a
shapiro-wilk
test
for
normality
but
did
appear
to
be
symmetric
as
required
for
the
wilcoxon
test
jensen
2006
our
own
and
others
experimentation
with
the
t-test
the
sign
test
with
and
without
the
zero
fudge
wilcoxon
test
with
both
the
continuity
correction
and
normal
approximation
and
also
an
exact
version
of
the
wilcoxon
test
that
computes
every
permutation
in
the
case
of
tied
ranks
found
none
that
resulted
in
substantially
more
reliable
reproducibility
probability
estimates
than
others
jensen
2006
sanderson
and
zobel
2005
our
same
prior
investigation
showed
that
reliable
reproducibility
probability
estimates
with
a
95
confidence
level
would
have
required
more
queries
so
we
chose
0
10
the
fundamental
problem
with
relying
on
the
p-value
from
a
single
hypothesis
test
is
that
it
does
not
address
the
problem
of
adequately
representing
the
query
population
to
ensure
reproducibility
goodman
1992
because
of
this
it
is
possible
to
find
statistically
significant
differences
over
a
particular
sample
of
queries
that
may
not
generalize
to
the
query
population
another
factor
that
must
be
considered
in
applying
hypothesis
tests
to
information
retrieval
evaluation
is
that
performing
multiple
tests
with
the
same
null
hypothesis
requires
simultaneous
testing
procedures
such
as
the
commonly
used
bonferroni
correction
to
account
for
the
overall
larger
probability
of
finding
a
significant
result
by
random
chance
miller
distinguishes
between
experiments
designed
for
uncovering
leads
that
can
be
pursued
further
to
determine
their
relevance
to
the
problem
versus
those
that
report
final
conclusions
suggesting
that
multiple
test
procedures
are
more
important
in
the
latter
case
miller
1981
our
primary
endpoint
is
comparing
conclusions
that
result
from
pairwise
hypothesis
tests
of
semiautomatic
evaluations
versus
those
of
benchmark
manual
ones
because
each
comparison
between
a
pair
of
engines
has
its
own
hypothesis
differing
from
others
multiple
testing
procedures
are
not
required
in
our
analysis
however
if
the
primary
endpoint
is
to
find
the
best
engine
or
to
rank
the
engines
multiple
testing
procedures
for
step-wise
and
pairwise
comparisons
should
be
considered
to
ensure
conservative
estimates
munzel
2001
2
2
2
confidence
intervals
many
advocate
reporting
confidence
intervals
for
the
parameter
of
interest
which
in
evaluation
is
typically
the
score
difference
rather
than
hypothesis
testing
because
they
are
easier
to
interpret
correctly
cormack
and
lynam
2006
construct
confidence
intervals
of
average
precision
over
varying
document
collections
in
the
trec
informational
task
using
the
bootstrap
if
we
use
a
confidence
interval
to
decide
whether
or
not
one
engine
significantly
outperforms
another
by
checking
whether
the
null
hypothesis
typically
zero
difference
in
scores
lies
outside
the
interval
we
are
performing
exactly
the
same
analysis
as
the
equivalent
hypothesis
test
again
this
does
not
address
the
problem
of
adequately
representing
the
query
population
or
quantifying
reproducibility
2
2
3
empirical
meta-evaluation
empirical
meta-evaluation
studying
the
results
of
an
evaluation
over
a
large
number
of
engines
focuses
on
estimating
the
reliability
of
an
evaluation
as
a
whole
this
sort
of
analysis
is
a
key
component
of
trec
where
it
is
almost
exclusively
applied
due
to
the
lack
of
such
a
large
diverse
set
of
engines
in
proprietary
environments
in
empirical
meta-evaluation
reliability
is
defined
as
the
stability
consistency
of
the
ranked
list
of
engines
across
query
sets
kendall
s
tau
or
spearman
s
rank
correlation
measures
are
often
used
to
compare
evaluations
based
on
their
ranking
of
engines
however
the
most
relevant
metric
to
our
work
is
the
error
rate
probability
of
a
pair
of
engines
flipping
positions
relative
to
one
another
in
the
ranked
list
of
engines
when
using
a
different
query
sample
as
defined
in
buckley
and
voorhees
2000
it
is
estimated
post
hoc
by
counting
the
number
of
pairwise
flips
in
the
rankings
of
a
large
number
of
engines
across
varying
query
samples
by
resampling
the
pilot
query
set
total
available
judged
queries
into
smaller
samples
in
voorhees
and
buckley
2002
they
focused
on
performing
this
calculation
for
several
different
query
sample
sizes
up
to
half
the
size
of
the
pilot
sample
and
then
extrapolating
to
estimate
the
error
rate
at
the
total
pilot
set
s
size
by
also
calculating
the
error
rate
for
several
different
fuzziness
minimum
difference
in
average
scores
to
not
be
considered
a
tie
values
and
leveraging
the
extrapolations
to
the
pilot
set
size
these
estimates
can
be
used
to
devise
a
priori
heuristics
sometimes
cited
when
planning
or
analyzing
experiments
at
trec
typically
these
take
the
form
of
an
x
difference
in
mean
average
precision
is
needed
to
ensure
an
error
rate
of
less
than
5
with
50
queries
however
these
heuristics
do
not
account
for
the
differences
in
distribution
of
a
particular
pair
of
engines
scores
their
variance
for
example
while
error
rate
is
useful
for
post
hoc
comparison
these
general
heuristics
derived
from
it
are
only
applicable
to
a
completed
evaluation
applying
these
heuristics
to
an
evaluation
in
progress
is
questionable
as
preliminary
differences
in
average
scores
are
subject
to
influence
from
outlying
scores
such
as
zero
and
one
recent
work
builds
on
error
rate
with
improved
theoretical
foundations
sanderson
and
zobel
2005
mitigate
distributional
issues
by
requiring
both
that
a
pair
of
engines
pass
a
hypothesis
test
and
have
a
difference
in
average
scores
large
enough
to
correspond
with
a
low
error
rate
lin
and
hauptmann
2005
derive
error
rate
from
statistical
principles
showing
that
variance
in
engines
scores
dramatically
impacts
the
reliability
of
evaluations
sakai
2006
uses
bootstrapping
to
find
the
score
difference
required
to
achieve
a
given
significance
level
in
bootstrap
hypothesis
testing
each
of
these
methods
has
in
common
the
use
of
a
large
number
of
diverse
runs
to
provide
a
general
rule
for
the
difference
in
average
scores
required
they
do
not
address
the
problem
of
reducing
the
effort
needed
to
answer
specific
questions
without
such
a
context
such
as
does
engine
a
outperform
engine
b
from
an
evaluation
in
progress
2
2
4
reproducibility
probability
although
we
are
unaware
of
its
application
to
information
retrieval
evaluation
we
adopt
reproducibility
probability
because
it
directly
measures
the
likelihood
that
a
particular
pairwise
conclusion
generalizes
to
the
query
population
as
a
whole
while
its
generality
enables
its
application
to
evaluations
in
progress
and
does
not
require
a
large
number
of
diverse
engines
as
a
context
shao
and
chow
2002
analyze
several
methods
of
estimating
reproducibility
probability
we
follow
their
first
in
which
the
reproducibility
probability
can
be
defined
as
an
estimated
power
of
the
future
trial
using
the
data
from
the
previous
trial
s
for
clarity
we
briefly
diverge
to
differentiate
between
the
true
power
typically
referred
to
simply
as
the
power
or
probability
of
rejecting
a
legitimately
false
null
hypothesis
and
this
estimated
power
described
by
shao
and
chow
the
true
power
defined
as
an
expectation
in
equation
1
borrowing
notation
from
lehmann
1986
is
commonly
used
a
priori
in
experimental
design
to
determine
what
sample
size
will
be
large
enough
to
detect
a
significance
difference
if
one
exists
however
calculating
the
true
power
depends
on
specifying
a
particular
distribution
f
that
satisfies
the
alternative
hypothesis
this
can
be
inaccurate
when
little
is
known
about
the
actual
distribution
of
observations
bacchetti
2002
when
f
is
unknown
the
true
power
of
nonparametric
tests
is
most
accurately
estimated
using
the
bootstrap
by
creating
artificial
subsamples
of
a
pilot
sample
in
which
the
alternative
hypothesis
is
enforced
troendle
1999
we
are
primarily
concerned
with
comparing
evaluations
based
only
on
their
conclusions
with
high
reproducibility
probability
not
the
true
power
of
hypothesis
tests
which
determines
the
likelihood
of
detecting
a
significant
difference
where
one
exists
true
power
of
a
hypothesis
test
the
estimated
power
method
of
estimating
reproducibility
probability
described
by
shao
and
chow
differs
from
this
true
power
in
that
it
comes
from
observed
experimental
data
where
the
truth-value
of
the
null
hypothesis
is
unknown
for
this
reason
it
is
also
known
as
the
observed
power
like
shao
and
chow
however
we
prefer
the
term
reproducibility
probability
to
avoid
any
implication
about
the
truth
of
the
null
hypothesis
inference
post
hoc
power
analysis
has
drawn
criticism
for
the
way
it
has
been
misinterpreted
as
evidence
against
the
null
hypothesis
for
tests
that
do
not
reject
the
null
hypothesis
since
the
observed
power
is
greater
than
zero
we
must
simply
not
have
a
large
enough
sample
to
support
our
conclusions
hoenig
and
heisey
2001
this
is
the
trap
of
the
large
sample
that
any
two
nonidentical
engines
are
significantly
different
with
a
large
enough
sample
we
focus
only
on
tests
that
do
reject
the
null
hypothesis
and
have
high
reproducibility
probability
at
sample
sizes
just
large
enough
to
reliably
estimate
reproducibility
probability
as
analyzed
in
section
3
3
although
their
definition
is
general
shao
and
chow
2002
only
apply
the
estimated
power
approach
to
reproducibility
probability
for
the
parametric
t-test
however
it
can
be
applied
in
the
general
case
to
include
nonparametrics
using
the
point-wise
bootstrap
estimate
i
e
as
done
by
de
martini
2006
this
pointwise
estimate
equation
2
is
based
on
efron
and
tibshirani
s
1993
nonparametric
bootstrap
a
preliminary
data
set
datan
is
used
to
estimate
a
probability
distribution
in
this
case
f
then
the
desired
power
or
sample
size
calculations
are
carried
out
as
if
f
were
the
true
distribution
as
discussed
in
their
example
of
estimating
the
true
power
of
a
bioequivalence
test
we
provide
an
algorithm
implementing
equation
2
specifically
for
information
retrieval
in
section
3
2
note
that
this
is
in
the
same
spirit
as
the
error
rate
heuristic
discussed
in
section
2
2
3
but
formalizes
reproducibility
probability
of
a
particular
pairwise
conclusion
without
requiring
a
completed
evaluation
over
a
large
variety
of
engines
point-wise
nonparametric
bootstrap
estimate
of
reproducibility
probability
however
these
reproducibility
probability
estimates
are
just
that
estimates
that
are
influenced
by
the
variability
in
the
observed
data
pilot
sample
the
necessary
pilot
sample
size
required
to
reliably
reproducibly
across
pilot
samples
estimate
them
must
be
established
we
leverage
graphical
methods
for
comparing
true
power
to
estimates
that
have
been
developed
for
just
this
purpose
collings
and
hamilton
1988
aggregated
numerical
methods
have
also
been
introduced
but
they
are
targeted
at
relative
comparison
of
power
estimation
techniques
rather
than
our
focus
of
determining
necessary
sample
sizes
de
martini
and
rapallo
2003
one
method
of
making
more
conservative
bootstrap
estimates
is
to
perform
a
double
bootstrap
essentially
performing
secondary
bootstrap
replications
of
each
of
the
bootstrap
samples
de
martini
2006
hall
and
martin
1988
when
performing
a
hypothesis
test
for
each
bootstrap
sample
of
reasonable
size
this
is
computationally
prohibitive
our
validation
of
the
reliability
of
reproducibility
probability
estimates
in
section
3
3
follows
a
limited
version
of
this
procedure
visualizing
differences
in
estimates
over
several
pilot
samples
2
3
reducing
evaluation
effort
two
aspects
of
reducing
evaluation
effort
have
been
studied
in
prior
work
evaluation
strategies
that
reduce
the
number
of
judgments
needed
in
a
manual
evaluation
and
automatic
evaluation
techniques
that
heuristically
infer
pseudo-relevance
judgments
studies
in
each
of
these
areas
suffer
from
a
difficulty
in
comparing
conclusions
drawn
from
one
evaluation
to
another
to
ensure
lower-effort
techniques
provide
correct
conclusions
with
respect
to
more
thorough
methods
simply
knowing
that
the
engine
rankings
of
one
evaluation
correlate
with
another
does
not
address
differing
levels
of
confidence
in
conclusions
and
the
associated
issue
of
whether
too
many
errant
conclusions
are
being
drawn
or
too
few
correct
conclusions
too
many
ties
are
found
several
evaluation
strategies
are
proposed
as
extensions
or
alternatives
to
the
trec
pooling
methodology
to
reduce
manual
effort
soboroff
2006
focused
on
the
problem
of
changes
in
the
document
collection
proposing
to
maintain
existing
trec
collections
to
limit
the
impact
of
these
changes
over
time
recent
work
dramatically
improves
on
the
evaluation
effort
required
in
trec
by
intelligently
selecting
results
to
be
evaluated
aslam
et
al
2006
carterette
et
al
2006
cormack
et
al
1998
proposed
interactive
searching
and
judging
in
which
no
system
pooling
is
used
evaluators
simply
perform
various
queries
for
a
topic
marking
relevant
documents
as
they
proceed
sanderson
and
joho
2004
analyze
methods
of
producing
test
collections
without
any
system
pooling
and
find
that
their
quality
correlates
with
that
of
trec
collections
sanderson
and
zobel
2005
quantified
the
relative
advantage
of
not
pooling
in
terms
of
the
evaluation
effort
required
to
achieve
a
desired
error
rate
fully
automatic
evaluation
techniques
are
widely
employed
in
domains
where
manual
evaluation
would
require
a
prohibitive
amount
of
effort
goldstein
et
al
2005
two
categories
of
automatic
evaluation
techniques
proposed
for
information
retrieval
are
inferring
pseudo-relevance
judgments
from
the
retrieved
documents
themselves
and
using
external
resources
to
aid
in
this
inference
several
approaches
randomly
sample
the
documents
from
the
retrieved
pools
based
on
known
statistics
about
the
typical
distribution
of
relevant
documents
as
pseudo-relevant
documents
but
find
that
the
effectiveness
of
only
typical
engines
but
not
the
best
engines
can
be
predicted
aslam
et
al
2003
nuray
and
can
2006
soboroff
et
al
2001
wu
and
crestani
2003
others
use
similarity
functions
between
documents
and
the
query
to
automatically
estimate
relevance
shang
and
li
2002
several
methods
of
leveraging
external
resources
to
infer
pseudo-relevance
judgments
have
been
proposed
some
advocate
the
use
of
click-through
data
tuples
consisting
of
a
query
and
a
user-clicked
result
for
automatic
assessment
however
there
is
a
well-known
presentation
bias
inherent
in
these
data
users
are
more
likely
to
click
on
highly
ranked
documents
regardless
of
their
quality
boyan
et
al
1996
joachims
et
al
2005
find
that
clickthrough
data
can
however
be
used
to
infer
relative
preferences
between
documents
others
have
made
use
of
taxonomies
to
fuel
automatic
evaluation
such
as
the
open
directory
project
referred
to
as
dmoz
or
odp
yahoo
s
directory
and
looksmart
haveliwala
et
al
2002
srinivasan
et
al
2005
these
taxonomies
divide
the
web
into
a
hierarchy
of
categories
with
some
pages
placed
in
multiple
categories
each
category
has
a
title
and
a
path
that
represents
its
placement
in
the
hierarchy
they
also
typically
have
editor-entered
page
titles
that
do
not
necessarily
correspond
to
the
titles
of
the
pages
themselves
3
reliable
manual
evaluation
evaluating
our
semiautomatic
framework
requires
a
reliable
manual
evaluation
for
comparison
we
are
unaware
of
currently
available
large
manual
evaluation
in
a
dynamic
environment
such
as
the
web
therefore
we
performed
our
own
evaluation
of
ten
web
search
engines
over
896
queries
based
on
the
assessor
time
we
allocated
with
no
preference
for
this
particular
number
we
briefly
review
this
experimental
environment
with
more
details
available
in
jensen
2006
we
then
examine
the
question
of
reliability
of
conclusions
drawn
from
such
an
evaluation
with
prior
techniques
for
estimating
reliability
inapplicable
we
review
reproducibility
probability
and
specifically
the
pointwise
bootstrap
estimate
that
we
leverage
as
with
any
reliability
estimate
the
conditions
for
the
estimate
itself
to
be
reliable
must
be
verified
we
therefore
continue
by
validating
the
reliability
of
these
reproducibility
probability
estimates
themselves
finding
the
minimum
query
sample
size
necessary
in
our
environment
to
ensure
that
high
reproducibility
probability
estimates
from
a
sample
correspond
to
similarly
high
levels
on
larger
samples
3
1
experimental
environment
we
manually
evaluated
the
top
ten
results
of
ten
web
search
engines
over
896
queries
without
system
pooling
the
engines
evaluated
altavista
alltheweb
gigablast
google
lycos
msn
msn
tech
preview
now
their
main
engine
teoma
wisenut
and
yahoo
are
anonymized
in
no
particular
order
as
e1
e2
e10
we
randomly
sampled
896
distinct
queries
from
an
aol
search
query
log
consisting
of
the
entire
search
traffic
hundreds
of
millions
of
queries
for
the
two
days
9/17
and
9/18
2004
queries
in
the
log
are
lowercased
and
stripped
of
most
punctuation
we
were
careful
to
randomly
select
from
the
true
distribution
of
queries
creating
a
sample
that
approximates
the
frequency
distribution
of
the
query
population
beitzel
et
al
2004b
2006
results
from
all
ten
engines
were
pooled
in
a
uniform
interface
based
on
canonicalized
url
including
the
surrogate
document
representations
consisting
of
title
snippet
and
the
link
to
the
page
that
assessors
could
optionally
click
through
for
each
query
a
group
of
aol
editors
undergraduate
and
graduate
computer
science
student
assessors
manually
assigned
each
result
as
relevant
or
not
relevant
and
selected
a
single
best
result
from
the
entire
pool
assessors
were
instructed
to
imagine
they
had
posed
the
query
to
determine
the
most
likely
information
need
based
on
only
the
typically
short
query
from
the
log
of
course
this
environment
may
suffer
from
problems
of
assigning
navigational
and
informational
interpretations
to
each
query
shifting
definitions
of
relevance
or
differing
perceptions
of
relevance
based
on
the
quality
of
surrogates
our
focus
on
finding
conclusions
that
generalize
to
the
query
population
as
a
whole
motivated
us
to
use
the
limited
number
of
assessor
hours
available
to
us
to
judge
more
queries
rather
than
attempt
to
reduce
such
sources
of
random
error
a
more
controlled
evaluation
environment
would
likely
reduce
the
number
of
queries
required
but
at
a
cost
of
higher
effort
per
query
detailed
statistics
about
this
evaluation
including
score
distributions
and
so
on
are
available
in
jensen
2006
for
each
engine
over
each
query
we
calculated
three
evaluation
metrics
average
precision
at
ten
precision
averaged
at
each
retrieved
relevant
document
limiting
the
denominator
to
the
maximum
number
of
retrieved
results
ten
denoted
as
avgp
precision
at
ten
denoted
as
p
10
and
reciprocal
rank
of
the
best
page
denoted
as
mrr
for
familiarity
despite
our
point-wise
use
of
it
see
table
i
for
mean
and
median
scores
ranked
by
mean
next
we
performed
pairwise
hypothesis
testing
for
significant
differences
in
median
score
using
the
wilcoxon
test
as
motivated
by
section
2
2
1
while
overall
median
is
not
terribly
descriptive
in
table
i
due
to
the
discretized
nature
of
a
top-ten
evaluation
simply
the
discrepancies
between
means
and
medians
are
indicative
of
non-normal
distributions
we
visualize
the
significant
differences
found
as
a
hierarchy
where
any
path
to
a
lower
node
represents
that
the
higher
node
significantly
outperforms
the
lower
one
figure
1
these
hierarchies
are
simply
a
visualization
conveying
the
same
information
as
more
common
textual
approaches
to
represent
groups
such
as
those
provided
in
trec
using
ir-stat-pak
blustein
and
tague-sutcliffe
1995
we
believe
they
are
more
readable
than
purely
textual
approaches
when
engines
are
not
strictly
ranked
by
their
average
scores
nodes
are
collapsed
together
when
they
have
an
equivalent
set
of
relationships
for
example
e1
significantly
outperforms
every
other
engine
under
the
mrr
evaluation
metric
because
there
is
a
path
from
e1
to
e2
to
e3
and
e10
and
so
on
e6
and
e4
under
mrr
by
contrast
neither
outperform
nor
are
outperformed
by
e8
but
they
both
significantly
outperform
e9
we
make
our
best
effort
to
place
engines
with
larger
scores
higher
but
favor
readability
over
enforcing
this
strictly
as
one
would
hope
for
any
measure
of
reliability
all
of
our
results
produce
figures
that
are
associative
never
requiring
more
than
one
node
to
represent
an
engine
precision
at
ten
is
not
shown
as
it
is
nearly
identical
to
average
precision
over
the
top
ten
results
with
only
two
differences
in
significant
conclusions
e6
e8
with
avgp
read
engine
six
significantly
outperforms
engine
eight
and
e9
e6
with
p
10
while
average
precision
is
not
typically
used
for
retrieved
sets
of
ten
it
does
help
to
reduce
the
number
of
tied
scores
across
engines
compared
to
the
more
discretized
p
10
see
jensen
2006
which
we
hypothesized
would
increase
reliability
however
we
do
not
find
any
meaningful
differences
in
either
the
reliability
or
conclusions
of
avgp
versus
p
10
see
section
3
3
so
for
the
remainder
of
this
article
we
simply
choose
avgp
3
2
bootstrapping
reproducibility
probability
the
algorithm
we
employ
for
bootstrap
estimates
of
reproducibility
probability
in
pairwise
information
retrieval
evaluations
is
detailed
in
figure
2
this
is
a
specialization
of
the
nonparametric
bootstrap
from
prior
work
described
in
section
2
2
4
particularly
an
implementation
of
equation
2
for
the
pairwise
information
retrieval
evaluation
problem
we
first
analyzed
point-wise
estimates
such
as
this
in
a
preliminary
investigation
jensen
et
al
2005
for
generality
we
leave
the
hypotheses
stated
as
e
a
e
b
engine
a
significantly
outperforms
engine
b
and
the
converse
because
the
specific
hypotheses
depend
on
the
test
chosen
the
null
hypothesis
for
both
tests
is
that
there
is
no
difference
between
the
two
engines
we
favor
one-sided
tests
because
the
conclusions
we
are
ultimately
interested
in
are
whether
one
engine
outperforms
another
not
simply
whether
they
differ
implicit
in
deciding
the
direction
of
differences
is
the
risk
of
type
iii
error
actually
drawing
firm
but
incorrect
conclusions
but
for
even
minimal
differences
in
engines
this
risk
is
small
spiegelhalter
and
freedman
1986
for
the
conclusions
included
in
our
comparisons
calculating
reproducibility
probability
for
each
direction
makes
this
choice
abundantly
clear
we
compare
only
conclusions
with
at
least
90
reproducibility
probability
in
which
case
the
converse
conclusions
typically
have
reproducibility
probability
less
than
1
performing
this
procedure
for
every
pair
of
k
10
engines
results
in
k
k
-
1
90
reproducibility
probability
estimates
from
which
we
simply
discard
the
weakest
estimate
of
each
pair
e
a
e
b
or
e
b
e
a
leaving
k
k
-1
/2
45
estimates
in
our
analyses
throughout
our
experimentation
we
set
the
number
of
bootstrap
iterations
b
2
401
where
b
has
no
relation
to
engine
b
which
we
always
represent
as
e
b
we
have
no
preference
for
such
an
odd
number
except
that
it
is
larger
than
the
recommended
minimums
for
bootstrap
calculations
including
those
for
bootstrapped
hypothesis
tests
davidson
and
mackinnon
2000
preliminary
experimentation
also
confirmed
this
was
more
than
sufficient
3
3
reliability
of
point-wise
bootstrap
power
estimates
the
margin
of
error
for
reliability
estimates
due
to
variability
in
their
pilot
samples
is
rarely
studied
since
we
cannot
evaluate
the
entire
query
population
any
estimate
of
reliability
is
biased
by
the
pilot
query
sample
used
to
calculate
it
we
focus
on
determining
the
sample
size
required
to
ensure
that
high
reproducibility
probability
estimates
from
any
pilot
sample
correspond
to
similarly
high
reproducibility
probability
estimates
for
the
same
engine
pair
from
our
entire
sample
of
896
queries
although
this
analysis
must
be
performed
separately
in
each
evaluation
environment
it
serves
as
a
simple
method
for
establishing
that
high
reproducibility
probability
estimates
converge
in
that
they
remain
high
across
pilot
samples
at
a
particular
pilot
sample
size
requiring
a
certain
number
of
pilot
queries
simply
to
estimate
reproducibility
probability
would
seem
to
dissolve
all
hope
of
reducing
evaluation
effort
but
as
we
demonstrate
in
section
5
incorporating
automatic
judgments
allows
us
to
meet
this
minimum
sample
size
without
manually
evaluating
each
query
in
figure
3
we
provide
an
example
of
the
growth
of
reproducibility
probability
for
two
example
engine
pairs
with
increasing
bootstrap
sample
size
m
and
corresponding
size
of
pilot
samples
n
hereafter
the
wilcoxon
test
with
0
10
is
assumed
the
scores
for
these
three
engines
and
their
associated
rankings
are
detailed
in
section
3
1
the
points
on
the
lines
of
figure
3
provide
relatively
smooth
curves
because
they
are
estimates
from
the
same
pilot
sample
q
of
all
896
queries
the
error
bars
however
represent
the
range
of
reproducibility
probability
estimates
calculated
using
several
other
pilot
samples
q
created
by
randomly
sampling
m
50
queries
from
q
throughout
we
use
a
bootstrap
sample
size
of
50
less
than
the
pilot
sample
to
dampen
the
issue
of
tied
score
differences
due
to
duplicated
queries
created
by
sampling
with
repetition
equivalent
score
differences
result
in
tied
ranks
in
the
wilcoxon
test
that
reduce
its
accuracy
error
bars
are
not
shown
for
m
850
because
creating
pilot
samples
that
vary
substantially
out
of
the
896
queries
available
is
not
possible
with
over
600
queries
we
are
able
to
conclude
that
e2
reliably
outperforms
e3
their
median
avgp
scores
are
676
and
646
respectively
the
candidate
conclusion
e5
e3
however
clearly
lacks
the
reproducibility
probability
to
support
it
with
these
sample
sizes
with
a
very
large
number
of
queries
we
might
expect
to
be
able
to
distinguish
between
e3
and
e5
reliably
as
discussed
in
section
2
2
4
increasing
the
sample
size
until
significant
differences
are
found
is
a
dangerous
and
inefficient
method
of
comparing
engines
any
nonidentical
engines
can
be
declared
significantly
different
with
a
large
enough
sample
size
as
our
goal
is
to
compare
evaluations
that
use
differing
query
samples
the
sample
sizes
used
in
our
analysis
are
determined
by
the
reliability
of
reproducibility
probability
estimates
for
any
engine
pair
not
the
significance
or
reproducibility
of
particular
conclusions
how
can
we
use
reproducibility
probability
to
determine
the
sample
size
necessary
to
ensure
reliable
conclusions
one
option
would
be
to
extrapolate
reproducibility
probability
estimates
from
smaller
sample
sizes
to
project
the
sample
size
at
which
a
conclusion
will
be
reliable
as
is
often
done
for
error
rate
however
we
can
see
from
the
error
bars
in
figure
3
that
estimates
based
on
small
pilot
samples
vary
wildly
having
only
evaluated
450
queries
for
example
we
might
extrapolate
that
with
650
we
would
find
a
reliable
difference
between
e5
and
e3
instead
we
favor
a
conservative
approach
of
evaluating
enough
queries
to
make
it
clear
that
reproducibility
probability
estimates
are
converging
to
similar
values
across
varying
pilot
samples
for
all
pairs
of
engines
the
discrepancies
between
reproducibility
probability
estimates
from
one
pilot
sample
to
another
can
be
dramatic
even
with
substantial
numbers
of
evaluated
queries
for
example
in
figure
4
we
plot
reproducibility
probability
estimates
over
all
45
pairs
candidate
conclusions
e
a
e
b
at
bootstrap
sample
size
450
from
varying
pilot
samples
of
500
queries
created
as
described
for
figure
3
versus
identically
sized
estimates
using
all
896
queries
as
the
pilot
sample
just
as
varying
pilot
samples
produced
large
error
margins
in
figure
3
here
we
see
that
reproducibility
probability
estimates
above
0
9
from
a
pilot
of
500
queries
might
correspond
to
estimates
as
low
as
0
4
for
the
same
conclusion
when
using
all
896
queries
to
determine
the
minimum
query
sample
size
necessary
to
ensure
that
highly
reproducible
probability
estimates
from
a
given
pilot
sample
will
correspond
to
similarly
high
estimates
from
other
samples
we
employ
a
simple
metric
the
minimum
reproducibility
probability
estimate
from
a
pilot
sample
to
ensure
a
reproducibility
probability
of
at
least
90
using
our
entire
sample
of
896
queries
as
the
pilot
in
figure
4
for
example
we
would
judge
that
500
queries
are
insufficient
because
only
sample
estimates
very
near
1
0
meet
this
criterion
the
corresponding
y-axis
estimates
from
all
896
queries
are
below
0
9
for
even
high
sample
estimates
as
our
metric
decreases
with
larger
sample
sizes
the
entire
discrepancy
graph
continues
to
grow
tighter
to
the
diagonal
this
analysis
is
a
limited
version
of
the
conservative
double
bootstrap
method
proposed
by
de
martini
2006
which
is
computationally
infeasible
for
our
sample
sizes
while
such
analysis
could
be
performed
on
each
engine
pair
individually
or
by
bucketing
pairs
by
levels
of
difference
this
creates
the
same
dependencies
that
make
error
rate
difficult
to
apply
in
new
environments
defining
the
level
of
difference
from
unreliable
preliminary
values
and
an
exaggerated
dependence
on
the
diversity
of
engines
evaluated
ensuring
that
none
of
the
pairs
of
engines
especially
those
with
small
differences
yields
a
falsely
high
reproducibility
probability
estimate
removes
the
dependence
on
determining
levels
of
differences
from
small
query
sets
rather
than
generating
synthetic
differences
or
engines
this
analysis
provides
a
minimum
sample
size
that
makes
false
positive
errors
unlikely
for
any
new
engine
with
similar
score
distribution
in
the
given
environment
in
table
ii
we
detail
this
analysis
for
our
web
search
evaluation
presenting
the
minimum
pm
n
-50
0
10
from
20
varying
pilot
samples
of
size
n
to
ensure
pm
n
-50
0
10
0
90
using
all
896
as
the
pilot
sample
for
p
10
with
samples
of
300
queries
and
mrr
with
450
even
an
estimate
of
1
0
does
not
guarantee
the
estimate
from
all
896
is
above
0
90
for
the
same
candidate
conclusion
because
of
the
margin
of
error
for
bootstrap
estimates
from
a
single
pilot
sample
which
depends
on
b
minimums
of
0
99
and
above
are
difficult
to
enforce
with
pilot
samples
of
size
650
however
estimates
begin
to
converge
to
ensure
that
high
reproducibility
probability
from
a
sample
corresponds
to
a
high
reproducibility
probability
estimate
using
all
896
therefore
we
conclude
that
650
queries
are
necessary
to
estimate
reproducibility
probability
reliably
in
our
environment
because
this
convergence
takes
place
nearly
250
queries
below
the
size
of
our
entire
sample
of
queries
we
conclude
that
it
is
not
an
artifact
of
pilot
sample
size
approaching
that
of
our
entire
sample
this
convergence
takes
place
near
the
same
size
for
each
evaluation
metric
leading
us
to
hypothesize
that
the
size
of
the
pilot
samples
has
more
impact
than
the
distributions
under
evaluation
and
providing
further
evidence
that
even
different
engines
would
likely
have
reliable
reproducibility
probability
estimates
with
this
number
of
queries
we
performed
this
same
analysis
on
several
trec
collections
in
jensen
2006
finding
conclusions
difficult
to
generalize
here
as
such
collections
are
not
intended
to
represent
a
query
population
3
4
conclusions
from
manual
web
search
evaluation
having
established
that
the
point-wise
bootstrap
estimate
of
reproducibility
probability
is
reliable
for
high
reproducibility
probability
estimates
on
large
enough
sample
sizes
we
conclude
by
applying
it
to
our
manual
evaluation
the
metric
we
chose
for
measuring
reliability
is
also
convenient
for
providing
an
ad
hoc
correction
to
our
reproducibility
probability
estimates
while
we
are
interested
in
conclusions
with
at
least
90
reproducibility
probability
we
saw
that
estimates
from
a
pilot
sample
of
even
800
queries
must
be
above
98
to
ensure
this
is
valid
for
the
population
to
ensure
we
only
examine
reliable
conclusions
therefore
we
only
include
those
with
reproducibility
probability
of
at
least
99
for
the
remainder
of
our
investigation
these
benchmark
high
reproducibility
probability
conclusions
from
wilcoxon
tests
using
0
10
are
shown
in
figure
5
while
many
conclusions
are
significant
based
on
a
wilcoxon
test
see
figure
1
approximately
half
of
these
have
high
reproducibility
probability
4
semiautomatic
evaluation
we
have
shown
that
evaluations
in
dynamic
environments
are
capable
of
yielding
conclusions
that
are
reproducible
across
query
samples
however
the
sample
sizes
necessary
to
ensure
this
are
large
demanding
substantial
effort
to
evaluate
each
query
manually
to
reduce
the
required
manual
judgment
effort
so
that
evaluations
can
feasibly
be
repeated
as
the
environment
changes
we
propose
a
semiautomatic
evaluation
framework
for
integrating
automatic
judgments
with
manual
ones
whereas
small
numbers
of
manually
evaluated
queries
are
of
little
use
on
their
own
due
to
the
large
number
of
false
positives
we
saw
in
section
3
combining
them
with
automatic
evaluation
provides
insight
into
conclusions
although
any
automatic
evaluation
technique
using
implicit
preferences
such
as
clickthrough
data
fusion
or
metasearch
based
approaches
and
so
on
could
be
applied
in
this
framework
we
leverage
the
resource-based
approach
we
developed
in
prior
work
mining
pseudo-relevance
judgments
from
taxonomies
such
as
the
open
directory
project
referred
to
as
dmoz
chowdhury
2005
this
serves
as
both
an
analysis
of
the
utility
of
our
resource-based
automatic
evaluation
technique
and
more
importantly
a
vehicle
for
developing
our
semiautomatic
framework
and
demonstrating
how
to
apply
and
validate
it
first
we
provide
an
overview
of
mining
pseudo-relevance
judgments
from
taxonomies
and
give
conclusions
derived
from
its
automatic
judgments
alone
next
we
present
the
two
basic
ways
in
which
automatic
techniques
can
augment
manual
ones
by
predicting
conclusions
that
are
likely
to
be
found
with
larger
query
sets
by
using
a
combination
of
a
smaller
number
of
manual
judgments
with
automatic
ones
and
by
filtering
conclusions
from
small
manual
evaluations
to
improve
their
reliability
finally
we
present
simple
methods
for
leveraging
each
of
these
two
aspects
we
compare
the
reliable
pairwise
engine
a
vs
engine
b
conclusions
they
provide
with
those
drawn
from
our
manual
evaluation
our
analysis
serves
as
an
example
of
that
which
would
be
required
using
any
automatic
evaluation
technique
in
a
given
environment
thus
illustrating
our
framework
and
corresponding
metrics
for
analyzing
the
utility
of
semiautomatic
methods
4
1
mining
automatic
relevance
judgments
to
validate
our
semiautomatic
framework
we
employ
automatic
evaluation
techniques
developed
in
our
previous
work
that
address
both
the
informational
and
navigational
tasks
beitzel
et
al
2003a
chowdhury
2005
jensen
2006
these
automatic
techniques
leverage
two
types
of
resources
that
are
likely
to
be
available
in
most
dynamic
search
environments
a
log
sufficiently
representing
the
population
of
queries
and
a
human-edited
taxonomy
of
documents
in
the
collection
that
is
large
enough
to
include
a
representative
sample
of
the
collection
this
could
be
any
form
of
taxonomy
such
as
a
corporate
intranet
directory
web
taxonomy
or
large
collection
of
categorized
bookmarks
but
it
must
represent
human
matches
of
topics
to
documents
and
not
be
biased
towards
particular
search
services
our
initial
investigations
into
automatic
evaluation
used
the
dmoz
and
looksmart
taxonomies
to
show
that
on
the
web
these
techniques
are
not
biased
towards
particular
engines
by
the
choice
of
taxonomy
to
mine
judgments
from
finding
a
0
931
pearson
correlation
between
mrr1
scores
the
reciprocal
rank
of
the
first
relevant
result
in
the
retrieved
list
of
automatic
evaluations
using
each
beitzel
et
al
2003b
chowdhury
and
soboroff
2002
these
purely
automatic
techniques
have
correlations
in
the
0
7
range
with
manual
evaluation
scores
beitzel
et
al
2003a
chowdhury
2005
for
the
following
experimentation
we
repeated
our
automatic
evaluations
on
the
web
using
more
recent
dmoz
data
downloaded
on
12/8/2004
applying
their
judgments
to
queries
from
the
same
log
and
results
from
the
same
set
of
ten
web
search
engines
as
in
our
manual
evaluation
details
of
this
process
are
provided
in
appendix
a
1
an
example
of
each
technique
is
provided
in
figure
6
for
the
navigational
homepage/named
page-finding
task
we
mine
pseudo-relevance
judgments
using
a
technique
we
term
title-match
it
collects
documents
from
the
taxonomy
whose
editor-supplied
titles
exactly
match
a
given
query
these
documents
are
treated
as
the
best
or
most
relevant
documents
for
that
query
for
the
informational
topical
search
task
we
use
a
technique
termed
category-match
if
the
most
specific
component
of
a
category
name
exactly
matches
a
given
query
all
documents
from
that
category
are
used
as
the
pseudo-relevant
set
scores
for
the
ten
engines
using
these
automatic
techniques
are
available
in
appendix
a
1
as
with
manual
evaluations
ranking
engines
by
their
average
score
and
comparing
rankings
using
correlations
is
insufficient
to
compare
only
the
reliable
conclusions
drawn
from
automatic
evaluations
with
those
from
manual
ones
we
apply
the
same
reproducibility
probability
analysis
using
the
randomly
selected
title-matched
queries
as
the
pilot
sample
and
setting
the
bootstrap
sample
size
equivalent
to
that
of
our
manual
evaluation
so
that
we
would
detect
differences
of
comparable
magnitude
we
found
those
diagrammed
in
figure
7
comparing
these
conclusions
with
those
of
our
manual
evaluation
in
figure
5
duplicated
for
convenience
the
automatic
technique
ranks
e10
and
e6
relatively
lower
while
it
ranks
e4
and
e5
higher
category-match
has
a
similar
correlation
see
appendix
a
1
although
our
focus
is
on
demonstrating
our
framework
we
investigated
several
methods
of
improving
this
correlation
including
correcting
for
query
popularity
distribution
topical
category
distribution
and
number
of
relevant
results
none
of
these
preliminary
investigations
substantially
improved
correlation
jensen
2006
4
2
integrating
manual
and
automatic
judgments
although
they
are
useful
in
examining
evaluation
characteristics
over
query
sample
sizes
difficult
to
evaluate
manually
we
have
seen
that
these
purely
automatic
techniques
are
often
inaccurate
we
have
also
shown
in
section
3
3
that
evaluation
of
search
engines
in
dynamic
environments
demands
a
large
query
sample
size
even
to
estimate
reproducibility
probability
incorporating
automatic
techniques
with
smaller
numbers
of
manual
judgments
provides
a
sort
of
evaluation
roadmap
where
there
would
otherwise
have
been
little
information
about
engines
relative
performances
we
focus
on
providing
guidance
for
developing
an
intelligent
evaluation
strategy
without
having
to
manually
evaluate
the
requisite
number
of
queries
for
a
reliable
evaluation
over
every
engine
we
examine
the
two
basic
advantages
semiautomatic
methods
can
offer
towards
this
goal
expanding
the
set
of
conclusions
by
predicting
which
will
have
high
reproducibility
probability
with
more
manual
evaluation
and
pruning
the
set
of
conclusions
from
a
manually
judged
query
sample
by
removing
those
that
do
not
seem
to
be
reproducible
across
samples
of
this
size
4
2
1
semiautomatic
prediction
to
aid
evaluators
in
focusing
on
conclusions
that
are
likely
to
be
reliable
with
further
manual
evaluation
we
propose
the
technique
detailed
in
figure
8
although
automatic
and
manual
judgments
could
also
be
combined
per-result
rather
than
on
a
query-by-query
basis
we
hypothesized
that
evaluating
only
some
of
the
results
from
a
query
is
not
dramatically
less
effort
than
evaluating
all
of
a
query
s
results
we
employ
this
probabilistic
sampling
rather
than
simply
using
the
same
entire
q
man
sample
in
each
bootstrap
replication
to
reduce
false
positives
by
increasing
the
diversity
of
the
samples
we
assume
the
number
of
queries
with
automatic
judgments
is
much
larger
than
that
used
in
each
bootstrap
replication
to
prevent
a
large
number
of
tied
scores
the
primary
goal
of
the
following
experimentation
is
to
determine
the
range
of
rman
and
nman
parameters
at
which
the
semiautomatic
method
predicts
more
of
the
correct
conclusions
than
simply
using
q
man
alone
while
maintaining
a
relatively
low
probability
of
finding
errant
false
positive
conclusions
4
2
2
semiautomatic
filtering
to
finalize
conclusions
from
manually
evaluated
query
samples
too
small
to
provide
reliable
conclusions
on
their
own
removing
the
need
for
further
judgments
of
the
associated
engines
we
propose
the
technique
detailed
in
figure
9
this
technique
leverages
the
large
sample
sizes
possible
using
automatic
techniques
to
reduce
the
likelihood
that
initial
conclusions
are
simply
artifacts
of
the
insufficient
manual
sample
size
for
sizes
nman
too
small
to
yield
reliable
conclusions
on
their
own
as
discussed
in
section
3
3
we
hypothesize
that
filtering
their
conclusions
with
those
from
an
automatic
evaluation
can
reduce
false
positive
errors
enough
to
allow
them
to
be
accepted
the
primary
goal
of
our
experimentation
with
this
technique
is
to
determine
the
range
of
sizes
nman
for
which
this
effect
is
achieved
while
not
discarding
too
many
of
the
conclusions
from
the
purely
manual
evaluation
that
are
actually
correct
4
3
utility
of
semiautomatic
evaluation
the
primary
goal
of
these
semiautomatic
methods
is
to
make
repeating
evaluations
feasible
in
large
dynamic
environments
they
address
this
by
providing
insight
into
conclusions
before
completing
an
evaluation
of
every
engine
s
results
over
the
entire
query
sample
size
required
to
ensure
reliability
this
enables
the
development
of
intelligent
evaluation
strategies
that
reduce
manual
effort
by
removing
engines
from
an
evaluation
in
progress
however
acceptable
levels
of
error
for
making
decisions
such
as
discarding
an
engine
depend
on
factors
specific
to
evaluation
goals
making
conclusions
about
total
effort
difficult
to
generalize
the
level
of
investigation
are
we
trying
to
divide
the
best
engines
from
the
worst
or
determine
whether
one
of
the
top
two
is
truly
better
than
the
other
or
even
the
relative
efficiency
monetary
cost
and
so
on
of
the
engines
considered
to
be
likely
determines
whether
we
are
willing
to
tolerate
some
false
alarms
or
missed
conclusions
this
is
outside
the
scope
of
comparing
the
relative
utility
of
various
semiautomatic
techniques
therefore
we
focus
only
on
the
general
utility
of
these
semiautomatic
techniques
versus
manual
judgments
at
finding
the
correct
pairwise
e
a
e
b
conclusions
using
only
a
small
pilot
sample
of
manually
evaluated
queries
we
quantify
this
utility
by
measuring
the
number
of
errant
pairwise
conclusions
each
of
them
yield
and
the
number
of
correct
conclusions
they
miss
this
is
a
typical
method
of
evaluating
pairwise
conclusions
in
filtering
and
categorization
beitzel
et
al
2004a
manmatha
et
al
2002
our
motivation
for
focusing
on
binary
pairwise
conclusions
themselves
as
opposed
to
the
underlying
reproducibility
probability
estimates
is
twofold
first
we
found
in
section
3
3
that
for
reasonable
sample
sizes
only
very
high
reproducibility
probability
estimates
are
reliable
based
on
that
analysis
throughout
the
following
evaluation
we
only
treat
reproducibility
probability
estimates
greater
than
99
as
asserting
a
conclusion
second
practitioners
are
likely
more
concerned
with
making
errant
conclusions
rather
than
the
accuracy
of
actual
values
of
reproducibility
probability
estimates
for
the
same
reasons
we
provide
the
raw
counts
of
errors
rather
than
their
percentages
as
the
magnitude
of
number
of
errors
is
often
of
at
least
as
much
concern
as
their
proportions
unlike
using
only
the
correlation
of
engine
rankings
to
compare
evaluations
this
framework
focuses
on
conclusions
with
high
reproducibility
probability
accounting
for
ties
and
exposing
whether
an
evaluation
is
too
weak
to
find
correct
conclusions
or
too
confident
in
errant
conclusions
comparing
evaluations
is
complicated
by
the
need
to
define
the
correct
conclusions
for
example
if
an
evaluation
of
300
queries
finds
that
e
a
outperforms
e
b
and
a
larger
evaluation
of
800
queries
finds
the
same
thing
but
if
it
also
shows
that
300
was
not
enough
to
reliably
conclude
that
is
the
conclusion
e
a
e
b
based
on
the
initial
300
queries
errant
to
mitigate
these
issues
each
of
our
analyses
spans
several
benchmark
query
sample
sizes
most
easily
characterized
by
the
bootstrap
sample
size
m
since
we
vary
the
size
of
the
pilot
samples
because
our
baseline
is
purely
manual
judgments
the
following
analysis
also
provides
an
interesting
corollary
to
our
investigation
into
the
reliability
of
reproducibility
probability
estimates
from
manual
judgments
as
it
further
describes
the
type
of
errant
conclusions
they
cause
4
3
1
results
of
predicting
from
auto-manual
mixed
samples
first
we
evaluate
the
utility
of
the
prediction
procedure
described
in
section
4
2
1
against
simply
using
the
pilot
sample
of
manually
evaluated
queries
alone
in
the
task
of
predicting
what
conclusions
will
be
found
with
larger
query
sample
sizes
than
those
that
have
been
evaluated
we
seek
to
determine
the
range
of
r
man
the
ratio
of
manual
to
automatically
judged
queries
and
nman
the
size
of
the
pilot
sample
parameters
for
which
the
semiautomatic
procedure
substantially
reduces
errors
compared
to
the
manual
as
we
did
in
section
3
3
we
analyze
the
manual
method
by
finding
the
set
of
conclusions
from
each
of
20
different
distinct
query
samples
q
man
with
50
more
queries
than
the
size
we
bootstrap
with
a
mixture
of
automatically
and
manually
evaluated
queries
in
the
semiautomatic
method
the
need
for
a
larger
pilot
manual
sample
than
the
bootstrap
sample
size
needed
to
prevent
a
large
number
of
ties
is
diminished
to
ensure
a
conservative
evaluation
we
therefore
used
sets
q
man
of
size
nman
e
m
for
the
semiautomatic
method
man
we
begin
with
an
examination
of
the
navigational
evaluation
using
the
best
page
mrr
manual
evaluation
and
the
title-match
automatic
approach
in
figure
10
we
compare
the
correct
set
of
manual
conclusions
based
on
our
benchmark
pilot
of
all
896
queries
bootstrapped
into
sets
of
850
a
copy
of
figure
5
for
convenience
to
those
from
one
of
the
twenty
semiautomatic
prediction
runs
this
is
in
fact
the
worst
case
the
largest
number
of
missed
conclusions
out
of
the
twenty
pilot
q
man
samples
of
size
350
for
the
m
850
e
m
350
test
man
comparing
these
example
semiautomatic
conclusions
in
figure
10
to
those
of
the
purely
automatic
technique
in
figure
7
shows
that
the
same
general
discrepancies
exist
but
their
severity
is
markedly
decreased
the
semiautomatic
still
ranks
e10
and
e6
relatively
too
low
and
e4
and
e5
higher
than
the
manual
just
as
the
automatic
method
did
however
the
number
of
errors
is
dramatically
fewer
because
it
commits
these
infractions
in
only
a
small
number
of
engine
pairs
whereas
the
automatic
method
is
certain
of
its
incorrectness
in
many
more
cases
this
serves
as
an
illustrative
example
of
how
the
aggregated
errors
in
the
following
tables
such
as
table
iii
are
counted
recalling
that
any
path
from
a
higher
node
to
a
lower
one
implies
that
engine
outperforms
the
lower
one
each
of
these
sets
contain
16
distinct
conclusions
by
chance
as
recorded
in
the
final
row
of
table
iii
this
case
of
the
semiautomatic
technique
misses
7
of
the
16
correct
conclusions
the
largest
absolute
number
of
them
across
all
20
pilot
samples
the
missed
conclusions
are
r
e1
e2
e4
r
e1
e5
e7
e8
e10
r
e6
e9
of
the
16
conclusions
this
case
draws
9
are
false
alarms
errant
false
positives
r
r
r
r
e2
e5
e10
e6
e3
e4
e7
e6
e5
e8
e8
e9
the
first
column
is
the
benchmark
bootstrap
sample
size
taken
from
the
pilot
of
all
896
that
we
compare
with
both
the
small
manual
and
semiautomatic
the
expected
number
of
manual
queries
in
each
test
bootstrap
sample
for
the
semiautomatic
approach
is
given
in
the
second
column
this
is
equivalent
to
the
test
bootstrap
sample
size
for
the
purely
manual
approach
as
we
are
interested
in
how
well
a
small
number
of
manually
evaluated
queries
predict
the
conclusions
of
a
larger
number
the
probability
of
a
false
alarm
is
expressed
as
the
ratio
of
the
average
number
of
false
alarms
to
the
average
number
of
conclusions
drawn
the
maximum
absolute
number
of
false
alarms
across
all
20
runs
is
given
with
its
associated
number
of
conclusions
on
that
pilot
sample
the
probability
of
a
miss
is
based
on
the
number
of
correct
conclusions
which
is
constant
for
each
benchmark
m
the
same
for
the
manual
and
semiautomatic
method
there
is
one
special
case
e
m
0
where
a
purely
automatic
apman
proach
is
provided
that
case
does
not
make
use
of
any
pilot
manual
samples
so
there
is
only
a
single
result
table
iii
includes
selected
rows
where
the
semiautomatic
approach
reduces
errors
dramatically
compared
to
the
manual
complete
results
for
these
and
other
ratios
of
manual
to
automatic
results
are
provided
in
appendix
a
2
predictions
based
on
expanding
the
small
manual
sample
with
queries
automatically
evaluated
using
title-match
typically
miss
approximately
half
as
many
of
the
correct
conclusions
as
those
from
the
manual
sample
alone
we
examine
predictions
to
four
larger
sizes
300
to
investigate
our
ability
to
predict
dramatic
differences
with
very
few
judgments
450
the
first
point
when
high
reproducibility
probability
estimates
in
the
manual
case
begin
to
become
reliable
see
table
ii
600
where
manual
conclusions
are
reliable
and
850
the
most
detailed
conclusions
our
set
of
judgments
can
support
the
small
number
of
correct
conclusions
three
in
the
300
queries
case
makes
it
difficult
to
choose
one
over
the
other
as
both
the
manual
and
semiautomatic
methods
have
difficulty
the
manual
one
often
misses
all
three
while
the
semiautomatic
one
draws
far
too
many
conclusions
in
general
with
over
half
of
them
being
false
alarms
random
performance
however
would
draw
nearly
all
false
alarms
as
only
three
conclusions
of
45
are
correct
across
the
other
prediction
sizes
the
manual
method
often
misses
nearly
all
the
correct
conclusions
at
a
maximum
the
semiautomatic
often
cuts
this
by
half
its
number
of
false
alarms
however
is
greater
than
when
using
manual
queries
alone
this
can
be
mitigated
by
incorporating
a
large
enough
ratio
of
manual
queries
see
appendix
a
2
which
also
reduces
the
number
of
conclusions
it
draws
in
general
the
denominator
of
the
probability
of
false
alarm
of
course
larger
available
pilot
samples
for
the
manual
method
increase
the
number
of
conclusions
it
draws
on
average
subsequently
decreasing
the
average
number
of
misses
with
little
increase
in
false
alarms
when
a
larger
number
of
manual
judgments
are
available
the
semiautomatic
method
may
not
be
justified
compared
to
not
being
able
to
draw
any
conclusions
at
all
even
a
prediction
method
prone
to
some
degree
of
false
alarms
is
likely
useful
but
how
do
we
determine
the
bounds
of
this
utility
clearly
we
need
a
combined
metric
to
compare
these
two
methods
and
determine
when
the
semiautomatic
method
s
relative
benefits
justify
its
use
to
directly
compare
the
cost
of
errors
in
the
manual
and
semiautomatic
methods
we
leverage
a
standard
cost
function
equation
3
adopted
from
the
topic
detection
and
tracking
tdt
conference
manmatha
et
al
2002
a
lower
cost
indicates
fewer
errors
were
made
this
combines
the
ratios
of
errors
shown
in
the
table
with
relative
costs
for
each
type
of
error
and
normalizes
them
by
the
relative
number
of
correct
conclusions
in
general
in
our
calculation
of
p
rel
we
assume
the
maximum
number
of
pair-wise
conclusions
that
could
be
found
among
our
10
engines
with
45
as
the
denominator
because
the
actual
numbers
of
correct
conclusions
for
our
four
prediction
sizes
are
much
less
than
45
this
may
inherently
provide
extra
weight
to
the
false
alarm
errors
equation
3
the
tdt
cost
function
in
the
prediction
task
we
set
cmiss
5
cfa
to
reflect
the
importance
of
finding
correct
conclusions
over
suggesting
errant
ones
with
the
cost
of
misses
twice
or
equal
to
false
alarms
the
manual
method
typically
outperforms
the
semiautomatic
although
this
may
be
inflated
by
the
aforementioned
bias
from
p
rel
we
hypothesized
that
the
key
parameter
was
the
ratio
of
manually
judged
queries
in
the
bootstrap
samples
regardless
of
the
overall
magnitude
of
the
sample
in
figure
11
we
show
the
costs
for
the
manual
and
semiautomatic
methods
at
various
ratios
of
manually
evaluated
queries
to
the
predicted
query
sample
size
when
predicting
sizes
of
450
600
and
850
this
and
each
of
the
following
cost
graphs
include
trend
lines
for
readability
created
using
a
second
order
polynomial
regression
since
that
yielded
the
largest
r
2
fitness
measure
for
each
graph
we
do
not
intend
to
make
any
general
assertions
about
the
shape
of
such
curves
as
it
is
obvious
they
differ
depending
on
the
automatic
technique
used
errors
are
very
highly
correlated
to
the
ratio
of
manual
judgments
regardless
of
total
sample
size
when
less
than
50
of
the
sample
size
to
be
predicted
has
been
manually
evaluated
the
semiautomatic
technique
is
more
effective
at
predicting
conclusions
than
the
smaller
number
of
manually
judged
queries
alone
when
roughly
half
of
the
sample
size
to
be
predicted
has
been
manually
evaluated
the
cost
of
false
alarms
introduced
by
the
semiautomatic
method
outweighs
the
reduction
in
missed
correct
conclusions
compared
to
using
the
manual
sample
alone
we
also
hypothesized
that
conclusions
with
dramatically
differing
engines
could
be
predicted
with
very
few
manual
judgments
as
we
saw
in
the
raw
error
counts
of
table
iii
however
predicting
conclusions
at
sample
size
300
using
the
semiautomatic
technique
results
in
so
many
false
alarms
that
its
cost
is
higher
than
using
the
small
manual
sets
alone
despite
their
propensity
to
miss
many
relevant
conclusions
see
figure
12
when
no
manual
judgments
are
available
however
the
cost
of
errors
from
the
purely
automatic
method
is
not
terribly
high
again
it
is
likely
to
be
useful
compared
to
not
being
able
to
predict
any
conclusions
whatsoever
the
proposed
semiautomatic
framework
and
metrics
enable
us
to
compare
the
effectiveness
of
different
automatic
judgment
techniques
in
the
hopes
of
moving
beyond
these
na
ve
ones
we
performed
the
same
experiments
and
i
analysis
with
combining
the
average
precision
at
10
manual
judgments
and
category-match
automatic
judgments
the
complete
error
counts
are
included
in
appendix
a
2
errors
in
the
semiautomatic
informational
evaluation
are
also
very
highly
correlated
with
the
ratio
of
manual
results
as
evidenced
by
figure
13
as
with
title-match
predicting
distant
conclusions
is
more
useful
than
nearer
ones
however
integrating
the
category-match
judgments
does
not
offer
as
much
benefit
as
those
of
title-match
in
the
navigational
evaluation
the
number
of
false
alarms
does
not
decrease
as
quickly
with
larger
ratios
of
manually
evaluated
queries
and
the
number
of
misses
actually
increases
slightly
whereas
it
decreases
with
title-match
we
believe
this
is
because
the
category-match
evaluation
has
more
disagreement
with
the
manual
avgp
evaluation
causing
the
integration
of
more
manual
judgments
to
reduce
the
number
of
both
correct
and
incorrect
category-match
predictions
like
the
navigational
evaluation
however
the
manual
samples
alone
often
miss
nearly
all
of
the
correct
conclusions
just
as
with
the
navigational
evaluation
predicting
conclusions
for
small
sample
sizes
such
as
300
is
better
achieved
with
very
few
manual
judgments
than
with
the
semiautomatic
technique
due
to
the
large
number
of
false
alarms
see
appendix
a
2
4
3
2
results
of
filtering
conclusions
from
small
manual
samples
next
we
evaluate
the
utility
of
the
filtering
procedure
described
in
section
4
2
2
as
opposed
to
simply
using
the
manually
evaluated
queries
alone
the
intent
here
is
to
reduce
the
number
of
false
alarms
from
sample
sizes
too
small
to
ensure
reliability
as
per
section
3
3
we
seek
to
determine
the
range
of
manually
evaluated
queries
nman
for
which
the
semiautomatic
technique
is
beneficial
as
in
our
analysis
of
prediction
we
create
20
distinct
query
samples
q
man
of
size
nman
m
50
and
compare
the
set
of
conclusions
from
each
to
that
of
bootstrapping
our
entire
pilot
sample
of
896
into
sets
of
size
m
we
begin
with
an
examination
of
the
navigational
evaluation
using
the
best
page
mrr
manual
evaluation
and
the
title-match
automatic
approach
see
table
iv
using
the
same
metrics
as
in
the
previous
section
it
is
clear
from
table
iv
that
semiautomatic
filtering
reduces
false
alarms
by
approximately
half
throughout
the
experiments
while
not
substantially
increasing
misses
especially
the
maximum
number
of
them
at
nman
500
there
is
a
dramatic
decrease
in
the
number
of
false
alarms
interestingly
this
correlates
with
the
smallest
size
at
which
reproducibility
probability
estimates
begin
to
become
reliable
across
all
metrics
in
table
ii
to
compare
the
semiautomatic
method
to
the
manual
with
a
single
metric
we
again
use
the
tdt
cost
function
defined
in
equation
3
in
contrast
to
predicting
conclusions
filtering
increases
reliability
of
candidate
conclusions
so
we
set
the
cost
of
false
alarms
to
be
twice
that
of
misses
with
the
costs
set
equal
the
manual
approach
is
preferred
for
some
sample
sizes
in
figure
14
we
show
the
cost
of
the
manual
and
semiautomatic
methods
at
increasing
sample
sizes
here
the
steep
drop
in
false
alarms
causes
the
corresponding
total
cost
to
drop
dramatically
with
samples
of
size
500
and
above
by
600
the
costs
are
roughly
equivalent
but
filtering
can
still
be
useful
to
ensure
the
reliability
of
a
conclusion
to
a
stricter
standard
as
evidenced
by
the
raw
counts
in
table
iv
the
results
for
the
informational
search
task
and
category-match
automatic
judgments
are
similar
unlike
the
navigational
evaluation
however
the
number
of
false
alarm
and
miss
errors
for
the
semiautomatic
technique
increases
consistently
with
sample
size
however
it
still
cuts
the
average
number
of
false
alarms
by
approximately
half
like
the
navigational
evaluation
there
is
a
drop
in
cost
see
figure
15
with
samples
of
size
500
and
above
but
unlike
it
the
utility
of
filtering
is
also
immediately
diminished
at
that
same
point
5
conclusions
dynamic
environments
such
as
the
world
wide
web
demand
frequent
repetition
of
costly
search
effectiveness
evaluations
we
have
detailed
a
semiautomatic
framework
that
combines
automatic
evaluation
with
manual
judgments
to
make
this
feasible
we
employ
methods
for
comparing
conclusions
of
one
evaluation
to
another
that
go
beyond
simple
correlation
of
engine
rankings
compared
to
small
numbers
of
manually
judged
queries
alone
semiautomatic
prediction
often
reduces
the
number
of
missed
correct
conclusions
by
half
and
semiautomatic
filtering
reduces
the
number
of
errant
conclusions
by
half
this
provides
evaluators
with
insight
into
conclusions
before
naively
evaluating
every
engine
over
the
requisite
number
of
queries
for
a
reliable
evaluation
to
validate
this
framework
we
leveraged
reproducibility
probability
to
determine
which
conclusions
generalize
to
the
query
population
as
a
whole
applying
this
method
to
our
own
precision-oriented
manual
web
search
evaluation
over
896
queries
shows
that
the
query
sample
sizes
required
to
ensure
reliability
in
such
evaluations
are
often
much
larger
than
those
previously
studied
650
in
our
environment
because
precision-oriented
evaluations
are
performed
without
system
pooling
they
do
not
depend
on
the
number
of
engines
being
judged
enabling
evaluation
strategies
that
reduce
effort
by
discarding
poorly
performing
engines
early
however
semiautomatic
methods
such
as
those
proposed
are
needed
to
exploit
this
by
building
query
samples
of
sufficient
size
before
manually
evaluating
each
one
in
a
conservative
example
from
our
navigational
evaluation
a
combination
of
semiautomatic
filtering
and
prediction
using
only
300
manually
judged
queries
would
enable
us
to
reliably
conclude
that
e6
and
e9
are
indeed
the
worst
performing
engines
removing
them
from
the
evaluation
would
reduce
the
size
of
the
result
pools
in
the
following
350
queries
left
to
evaluate
by
19
based
on
overlap
analysis
in
jensen
2006
there
is
a
great
deal
of
future
work
in
this
area
using
this
framework
we
will
evaluate
and
refine
other
automatic
evaluation
techniques
especially
implicit
preferences
such
as
clickthrough
data
to
determine
which
or
what
combination
best
enables
semiautomatic
methods
to
determine
the
correct
conclusions
with
fewer
manual
judgments
we
will
also
further
investigate
manual
judgment
techniques
for
those
that
optimize
the
effort
required
to
reach
a
desired
level
of
reliability
such
as
judgments
with
varying
levels
of
relevance
beyond
binary
in
addition
each
automatic
evaluation
technique
has
its
own
spamming
issues
that
need
to
be
investigated
strategy-based
instruction
lessons
learned
in
teaching
the
effective
and
efficient
use
of
computer
applications
suresh
k
bhavnani
university
of
michigan
frederick
a
peck
university
of
colorado
and
frederick
reif
carnegie
mellon
university
numerous
studies
have
shown
that
many
users
do
not
acquire
the
knowledge
necessary
for
the
effective
and
efficient
use
of
computer
applications
such
as
spreadsheets
and
web-authoring
tools
while
many
cognitive
cultural
and
social
reasons
have
been
offered
to
explain
this
phenomenon
there
have
been
few
systematic
attempts
to
address
it
this
article
describes
how
we
identified
a
framework
to
organize
effective
and
efficient
strategies
to
use
computer
applications
and
used
an
approach
called
strategy-based
instruction
to
teach
those
strategies
over
five
years
to
almost
400
students
controlled
experiments
demonstrated
that
the
instructional
approach
1
enables
students
to
learn
strategies
without
harming
command
knowledge
2
benefits
students
from
technical
and
nontechnical
majors
and
3
is
robust
across
different
instructional
contexts
and
new
applications
real-world
classroom
experience
of
teaching
strategy-based
instruction
over
several
instantiations
has
enabled
the
approach
to
be
disseminated
to
other
universities
the
lessons
learned
throughout
the
process
of
design
implementation
evaluation
and
dissemination
should
allow
teaching
a
large
number
of
users
in
many
organizations
to
rapidly
acquire
the
strategic
knowledge
to
make
more
effective
and
efficient
use
of
computer
applications
1
introduction
this
article
reports
our
experience
in
the
design
implementation
evaluation
and
dissemination
of
a
teaching
approach
called
strategy-based
instruction
this
approach
evolved
over
five
years
through
teaching
almost
400
students
is
designed
to
teach
effective
and
efficient
strategies
to
use
complex
computer
applications
such
as
spreadsheets
and
web-authoring
tools
strategy-based
instruction
is
motivated
by
several
empirical
studies
which
have
reported
that
users
have
difficulty
in
acquiring
effective
and
efficient
strategies
to
use
computer
authoring
applications
these
empirical
studies
in
the
use
of
unix
doane
et
al
1990
word
processors
rosson
1983
spreadsheets
nilsen
et
al
1993
cragg
and
king
1993
and
computer-aided
drafting
cad
systems
bhavnani
and
john
2000
have
shown
that
while
most
users
can
easily
learn
how
to
use
basic
commands
few
of
them
acquire
the
knowledge
to
use
the
commands
effectively
and
efficiently
for
example
nilsen
et
al
1993
observed
experienced
spreadsheet
users
perform
a
task
requiring
a
change
of
width
of
several
adjacent
columns
with
the
exception
of
one
they
found
that
most
of
the
users
modified
the
column
widths
one-by-one
in
order
to
avoid
modifying
the
exception
however
another
method
of
performing
this
task
is
to
aggregate
all
the
columns
including
the
exception
modify
their
widths
and
then
modify
the
exception
back
to
its
original
width
this
method
avoids
the
time-consuming
and
error-prone
steps
of
changing
the
width
of
each
column
the
method
is
therefore
efficient
because
it
reduces
task
time
and
effective
because
it
reduces
errors
in
the
end
product
while
some
users
do
in
fact
acquire
efficient
and
effective
methods
to
become
experts
why
do
many
other
users
persist
in
using
inefficient
and
ineffective
methods
to
perform
common
computer
tasks
analyses
of
tasks
like
the
previous
one
have
led
researchers
to
conclude
that
users
are
likely
to
change
a
method
to
perform
a
task
if
that
method
fails
to
achieve
the
intended
goal
however
users
are
more
likely
to
not
change
their
methods
if
they
succeed
in
achieving
goals
even
if
the
methods
are
inefficient
for
example
singley
and
anderson
1989
state
productions
which
produce
clearly
inappropriate
actions
contribute
to
poor
initial
performance
on
a
transfer
task
but
are
quickly
weeded
out
productions
that
generate
actions
which
are
merely
non-optimal
however
are
more
difficult
to
detect
and
persist
for
longer
periods
p
119
more
recently
fu
and
gray
2004
suggest
that
most
users
persist
in
using
suboptimal
methods
e
g
using
spaces
to
center
a
word
on
a
page
because
they
are
general
purpose
and
therefore
useful
for
a
wide
range
of
similar
tasks
furthermore
because
such
suboptimal
methods
provide
immediate
incremental
feedback
about
progress
towards
the
user
s
goal
they
become
preferred
methods
over
time
unfortunately
for
the
user
these
preferred
methods
are
highly
inefficient
when
used
for
complex
tasks
other
reasons
for
a
review
see
bhavnani
and
john
2000
that
might
conspire
against
users
becoming
more
effective
and
efficient
in
using
computer
applications
include
prior
knowledge
dominating
current
performance
thus
leading
to
the
einstellung
effect
luchins
and
luchins
1970
flemming
et
al
1997
a
production
bias
carroll
and
rosson
1987
which
results
in
users
focusing
on
the
task
at
hand
rather
than
on
learning
to
use
the
system
more
efficiently
few
opportunities
for
acquiring
effective
methods
in
work
environments
bhavnani
et
al
1996
and
the
lack
of
effective
and
efficient
methods
made
explicit
in
instructional
material
bhavnani
and
john
1996
bhavnani
1998
furthermore
sources
of
knowledge
to
use
computer
applications
like
help
and
user
manuals
either
focus
on
command
instructions
for
simple
tasks
or
focus
on
methods
to
perform
complex
tasks
methods
that
are
task-specific
and
difficult
to
generalize
bhavnani
1998
given
the
many
reasons
that
conspire
against
users
acquiring
efficient
and
effective
methods
can
explicit
instruction
address
this
issue
over
the
last
20
years
researchers
have
stressed
the
need
for
computer
literacy
in
undergraduate
education
sellars
1988
identified
the
different
stages
of
computer
literacy
halaris
and
sloan
1985
and
designed
approaches
to
teach
application
commands
such
as
through
minimalist
documentation
carroll
et
al
1987
more
recently
researchers
have
explored
extensions
of
computer
literacy
to
new
uses
of
technology
e
g
as
a
communication
device
and
to
social
aspects
of
technology
e
g
ethical
computing
guidelines
goldweber
et
al
1994
international
society
for
technology
in
education
1999
hoffman
and
blake
2003
finally
there
have
been
numerous
approaches
for
teaching
computer
skills
through
online
tutoring
systems
e
g
shaw
and
polovina
1999
and
comparisons
of
different
instructional
approaches
such
as
tutors
and
discovery
learning
e
g
charney
et
al
1990
while
this
research
provides
important
insights
into
the
need
and
process
of
teaching
users
how
to
use
computer
applications
they
have
mostly
focused
on
teaching
how
to
use
basic
commands
however
several
studies
have
shown
that
most
users
do
not
acquire
efficient
and
effective
methods
just
by
learning
how
to
use
commands
for
example
architects
despite
formal
cad
training
to
use
commands
and
many
years
of
experience
using
the
cad
system
did
not
use
effective
and
efficient
methods
in
real-world
tasks
bhavnani
et
al
1996
to
address
this
situation
we
hypothesized
that
users
might
benefit
from
explicit
instruction
on
effective
and
efficient
methods
to
use
computer
applications
for
example
in
addition
to
learning
how
to
select
and
modify
columns
in
a
spreadsheet
we
hypothesized
that
users
might
benefit
by
also
learning
the
method
of
dealing
with
exceptions
this
method
is
general
because
it
can
be
used
to
deal
with
a
wide
range
of
tasks
involving
different
information
objects
e
g
words
graphics
formulas
we
refer
to
such
general
and
goal-directed
methods
as
strategies
to
teach
efficient
and
effective
strategies
we
first
need
to
know
what
they
are
unfortunately
there
has
been
relatively
little
research
in
identifying
effective
and
efficient
strategies1
for
using
computer
applications
for
example
as
discussed
earlier
nilsen
et
al
1993
identified
a
few
efficient
methods
to
perform
spreadsheet
tasks
and
lee
and
barnard
1993
discuss
a
method
to
compare
different
parts
of
a
document
by
using
the
split
window
command
in
neither
case
has
there
been
an
attempt
to
generalize
these
methods
nor
have
they
been
organized
in
a
framework
furthermore
while
much
research
has
focused
on
teaching
computer
commands
we
have
found
no
attempts
to
teach
and
evaluate
strategies
to
use
computer
applications
this
article
describes
our
experience
over
five
years
to
1
develop
a
framework
to
identify
and
organize
strategies
that
generalize
across
computerauthoring
applications
and
2
design
implement
evaluate
and
disseminate
an
instructional
framework
to
teach
these
strategies
section
2
reviews
our
prior
research
that
focused
on
the
design
of
the
aforementioned
two
frameworks
and
how
they
were
used
to
implement
and
evaluate
a
prototype
for
strategy-based
instruction
section
3
discusses
our
more
recent
research
which
evaluated
the
robustness
of
the
prototype
as
it
was
extended
to
teach
new
applications
to
new
populations
and
in
a
new
context
section
4
generalizes
our
experiences
by
indicating
five
lessons
learned
in
teaching
strategy-based
instruction
we
conclude
with
reflections
on
our
experience
in
designing
implementing
and
evaluating
the
strategy-based
instruction
to
almost
400
students
over
five
years
and
with
research
questions
that
need
to
be
addressed
to
make
more
users
effective
and
efficient
in
the
use
of
computer
applications
2
design
implementation
and
evaluation
of
a
strategy-based
instructional
prototype
our
research
on
strategy-based
instruction
began
with
the
design
of
the
following
two
frameworks
1
a
strategy
framework
that
specified
the
general
powers
of
computers
which
were
used
to
identify
efficient
and
effective
strategies
and
2
a
strategy-based
instructional
framework
that
specified
the
general
principles
of
instruction
which
were
used
to
identify
an
approach
for
teaching
strategies
to
novice
users
these
two
frameworks
were
used
to
implement
a
prototype
of
strategy-based
instruction
which
was
then
evaluated
in
controlled
classroom
experiments
2
1
design
of
a
strategy
framework
the
strategy
framework
was
developed
through
a
literature
review
of
effective
and
efficient
methods
for
using
computer
applications
bhavnani
and
john
2000
observation
of
users
performing
real-world
tasks
bhavnani
et
al
1996
analysis
of
the
features
of
applications
bhavnani
and
john
1998
and
a
goms
card
et
al
1983
john
and
kieras
1996
analysis
of
key
strategies
bhavnani
and
john
2000
this
process
led
us
to
identify
a
strategy
framework
consisting
of
9
strategies
based
on
4
general
functions
or
powers
of
computer
applications
iteration
propagation
organization
and
visualization
2
the
first
column
of
appendix-a
shows
the
9
general
strategies
and
the
remaining
columns
show
how
those
strategies
can
be
instantiated
across
different
authoring
applications
for
example
the
strategy
described
earlier
to
modify
many
columns
with
an
exception
an
instantiation
of
the
handle
exceptions
last
strategy
in
appendix-a
is
efficient
because
it
exploits
the
power
of
iteration
provided
by
most
authoring
tools
instead
of
the
user
modifying
each
column
the
strategy
enables
the
iterative
task
to
be
delegated
to
the
computer
given
some
constraints
such
strategies
are
critical
to
learn
because
real-world
users
as
shown
by
nilsen
et
al
1993
typically
miss
opportunities
to
use
such
strategies
furthermore
a
goms
analysis
of
such
iteration
strategies
found
that
they
could
lead
to
a
reduction
in
time
of
between
40-70
and
to
a
reduction
in
the
probability
of
errors
bhavnani
and
john
2000
similarly
propagation
strategies
exploit
the
power
of
computers
to
modify
objects
that
are
connected
through
explicit
dependencies
these
strategies
allow
users
to
propagate
changes
to
large
numbers
of
interconnected
objects
for
example
the
strategy
make
dependencies
known
to
the
computer
is
useful
in
word
processors
in
the
use
of
style
here
a
user
can
create
paragraphs
that
need
to
share
a
common
format
specification
when
the
specification
is
modified
all
the
dependent
paragraphs
are
automatically
changed
similarly
formulas
in
a
spreadsheet
can
be
linked
to
dependent
data
or
graphic
elements
in
a
cad
system
can
be
linked
to
a
common
graphic
definition
of
objects
organization
strategies
exploit
the
power
of
computers
to
construct
and
maintain
the
organization
of
information
such
as
tables
and
lists
for
example
the
strategy
make
organizations
known
to
the
computer
is
useful
in
a
word
processor
in
the
use
of
a
table
in
contrast
to
using
tabs
to
construct
a
table
whose
organization
may
not
be
maintained
when
the
contents
are
modified
using
the
insert
table
command
in
msword
enables
the
computer
to
maintain
the
tabular
organization
under
any
modification
of
its
contents
similarly
data
for
different
years
in
a
spreadsheet
can
be
organized
in
separate
sheets
for
easy
access
and
manipulation
finally
visualization
strategies
exploit
the
power
of
computers
to
display
information
selectively
without
altering
its
content
for
example
the
strategy
view
parts
of
spread-out
information
to
fit
simultaneously
on
the
screen
addresses
the
limited
screen
space
of
most
computer
screens
for
instance
a
user
might
need
to
compare
the
contents
of
a
table
at
the
beginning
of
a
long
word
processing
document
to
the
contents
of
a
table
in
the
middle
of
the
same
document
in
such
cases
instead
of
scrolling
back
and
forth
between
the
tables
it
is
more
efficient
and
less
error-prone
to
set
up
distinct
views
e
g
through
the
use
of
the
split-window
command
that
focus
on
each
table
and
that
can
be
viewed
simultaneously
on
the
screen
this
strategy
is
clearly
useful
in
large
documents
containing
text
numbers
or
graphic
elements
and
is
therefore
generally
useful
across
applications
using
such
objects
as
in
most
performance-improvement
methods
these
strategies
trade
off
the
effort
to
use
a
strategy
and
the
realized
benefits
for
example
iteration
strategies
are
more
beneficial
when
they
are
used
for
many
elements
rather
than
a
few
bhavnani
and
john
1998
furthermore
it
would
not
be
compelling
to
use
a
strategy
that
saves
time
when
time
is
not
a
critical
factor
to
a
user
therefore
strategies
are
more
cost
effective
for
complex
tasks
and
where
the
performance
gains
are
of
value
to
the
user
it
is
therefore
as
important
to
know
when
to
use
a
strategy
as
it
is
to
know
how
to
execute
it
2
2
design
of
a
strategy-based
instructional
framework
as
suggested
by
many
researchers
e
g
klahr
and
carver
1988
gong
and
elkerton
1990
we
decided
to
model
the
knowledge
required
to
use
the
strategies
before
we
designed
the
instruction
this
approach
enabled
us
to
gain
a
precise
understanding
of
the
knowledge
to
be
imparted
a
goms
analysis
of
the
strategies
bhavnani
and
john
2000
revealed
that
each
requires
three
knowledge
components
1
command
knowledge
that
includes
knowledge
of
the
existence
of
commands
their
location
and
the
methods
to
use
them
in
goms
terms
there
must
exist
a
method
with
operators
to
execute
the
command
2
application-specific
strategic
knowledge
that
includes
knowledge
of
the
existence
of
efficient
strategies
within
an
application
conditions
of
when
to
use
a
strategy
and
the
method
to
execute
the
strategy
by
sequencing
different
commands
in
goms
terms
there
must
be
a
selection
rule
that
recognizes
when
to
use
this
strategy
and
an
associated
method
to
sequence
different
commands
to
execute
the
strategy
3
application-general
strategic
knowledge
that
provides
knowledge
of
how
particular
applicationspecific
strategies
can
be
applied
across
applications
in
goms
terms
the
selection
rules
for
strategies
are
generally
stated
and
can
be
instantiated
in
different
task
situations
while
the
goms
modeling
guided
us
towards
a
more
precise
understanding
of
what
to
teach
it
did
not
provide
guidance
on
how
to
teach
the
above
three
knowledge
components
therefore
we
exploited
existing
educational
research
to
understand
how
to
teach
the
knowledge
components
we
now
describe
how
we
designed
an
instructional
framework
by
combining
our
understanding
of
the
knowledge
components
required
to
use
effective
and
efficient
strategies
with
the
existing
research
on
how
best
to
teach
different
types
of
skills
2
2
1
command
knowledge
our
approach
of
when
and
how
to
teach
command
knowledge
was
guided
by
previous
research
in
the
psychology
and
education
literature
anderson
2000
p
387
recommended
that
it
was
important
to
teach
component
skills
before
teaching
high-level
skills
that
included
those
component
skills
this
suggested
to
us
that
command
knowledge
should
be
taught
before
strategies
that
used
those
commands
this
was
further
verified
in
our
early
pilots
bhavnani
et
al
1999
where
we
attempted
to
first
teach
general
strategies
as
a
unifying
framework
for
later
teaching
the
commands
however
this
approach
resulted
in
a
course
that
was
not
motivating
for
the
students
because
the
strategies
were
too
abstract
without
the
command
knowledge
our
final
prototype
therefore
taught
commands
before
teaching
the
strategies
prior
research
has
also
shown
the
importance
of
active
processing
whereby
students
are
made
to
engage
in
a
task
instead
of
merely
observing
passively
how
others
perform
the
task
such
active
processing
has
been
shown
to
result
in
a
deeper
understanding
of
the
imparted
knowledge
nicholls
1989
nolen
1996
2003
we
therefore
designed
the
instruction
in
two
parts
1
demonstration
of
commands
where
the
students
watched
the
instructor
execute
the
steps
of
a
command
while
this
first
step
was
passive
it
enabled
the
student
to
begin
to
acquire
the
declarative
knowledge
of
the
location
goal
and
process
of
using
the
command
2
practice
of
commands
where
the
students
performed
on
their
own
a
task
that
was
different
from
the
one
demonstrated
but
required
the
same
commands
such
an
approach
was
used
to
encourage
active
processing
for
example
the
students
were
shown
how
to
use
commands
to
view
a
document
such
as
split
window
scroll
and
new
window
and
then
given
an
opportunity
to
practice
the
commands
in
the
context
of
a
new
task
prior
research
also
suggested
that
students
typically
have
higher
intrinsic
motivation
when
they
are
taught
with
examples
that
are
relevant
to
them
and
to
the
real-world
pintrich
and
schunk
1996
myers
1989
mccade
2001
eisenberg
and
johnson
2002
the
in-class
tasks
for
demonstration
and
practice
as
shown
in
appendix-d
were
therefore
carefully
designed
to
be
meaningful
and
relevant
to
the
students
for
example
the
tasks
for
the
technical
students
included
organizing
information
related
to
salaries
for
teaching
assistants
and
tasks
for
the
art
students
included
organizing
information
related
to
art
and
music
these
tasks
were
designed
based
on
input
from
the
student
instructors
who
had
experience
in
teaching
the
cmu
freshman
students
in
previous
years
and
therefore
had
first-hand
knowledge
of
the
tasks
that
were
relevant
to
these
students
2
2
2
application-specific
strategic
knowledge
our
approach
to
teaching
application-specific
strategic
knowledge
was
guided
by
research
which
has
shown
that
higher
retention
of
knowledge
can
often
be
achieved
when
students
construct
knowledge
through
the
process
of
guided
discovery
e
g
brown
and
palinscar
1989
in
addition
to
using
the
notion
of
guided
discovery
our
approach
was
also
informed
by
the
importance
of
making
explicit
the
conditions
under
which
a
strategy
is
useful
singley
and
anderson
1989
we
implemented
the
notion
of
guided
discovery
by
engaging
the
students
in
an
interactive
session
where
they
were
asked
to
describe
how
they
would
use
the
commands
just
practiced
to
efficiently
perform
a
complex
task
for
example
after
being
introduced
to
the
split
window
command
the
students
were
shown
a
long
document
with
many
short
bulleted
lists
and
asked
to
discuss
a
method
for
bringing
three
nonadjacent
items
from
the
last
list
to
the
third
list
the
instructors
provided
feedback
for
the
methods
suggested
by
the
students
by
discussing
the
trade-offs
and
then
demonstrated
the
efficient
strategy
of
splitting
the
window
before
moving
the
items
these
discussions
made
explicit
the
conditions
which
best
motivated
the
use
of
the
applicationspecific
strategy
2
2
3
application-general
strategic
knowledge
our
approach
to
teaching
application-general
strategic
knowledge
was
guided
by
two
important
findings
in
the
acquisition
of
knowledge
1
the
transfer
of
strategies
can
be
achieved
by
teaching
the
general
form
of
a
strategy
bossock
and
holyoak
1989
fong
et
al
1986
and
through
multiple
examples
of
the
same
strategy
gick
and
holyoak
1983
and
2
higher
retention
can
be
achieved
by
revisiting
the
same
knowledge
at
regular
and
reasonably
extended
intervals
in
a
phenomenon
called
the
spacing
effect
e
g
hintzman
1969
underwood
1969
anderson
and
milson
1989
anderson
2000
we
implemented
the
notion
of
teaching
application-general
strategic
knowledge
in
multiple
contexts
by
first
presenting
the
general
form
of
the
strategy
and
then
showing
how
it
could
be
used
across
many
computer
applications
for
example
after
the
split-window
strategy
was
discussed
and
demonstrated
within
an
application
as
discussed
above
the
strategy
was
generalized
to
view
parts
of
spread-out
information
to
fit
simultaneously
on
the
screen
by
pointing
it
out
in
a
strategy
handout
similar
to
the
table
shown
in
appendixa
this
handout
contained
all
the
strategies
and
examples
of
their
instantiation
across
the
applications
taught
similarly
the
application-specific
strategy
of
using
the
styles
command
in
word
to
efficiently
and
effectively
modify
text
was
generalized
to
the
strategy
make
dependencies
known
to
the
computer
by
pointing
it
out
in
the
handout
to
leverage
the
spacing
effect
to
enhance
retention
we
taught
the
same
strategy
in
subsequent
applications
2
3
implementation
of
the
strategy-based
instructional
framework
the
strategy-based
instructional
framework
was
implemented
as
a
prototype
in
the
context
of
an
existing
seven-week
required
course
for
freshman
students
at
carnegie
mellon
university
cmu
this
course
focused
on
teaching
a
set
of
commands
to
the
freshman
students
to
provide
an
experimental
comparison
our
implementation
taught
the
same
commands
and
the
same
sequence
of
applications
unix
msword
then
excel
and
took
the
same
instruction
time
as
the
regular
cmu
instruction
3
classes
of
50
minutes
each
for
unix
msword
and
msexcel
the
strategy-based
implementation
followed
the
template
shown
in
figure
1
the
command
instruction
began
with
a
demonstration
of
a
small
set
of
commands
in
the
context
of
simple
tasks
step
1
for
example
the
instructor
introduced
different
ways
to
view
a
document
in
msword
through
the
use
of
split
window
and
scroll
the
students
were
then
told
to
practice
the
commands
just
taught
with
a
new
task
step
2
this
demonstration
and
practice
was
followed
by
instruction
for
the
next
set
of
commands
step
3
in
this
case
these
commands
involved
using
new
window
and
zoom
all
the
commands
taught
until
then
in
the
class
were
summarized
step
4
the
command
instruction
was
followed
by
application-specific
strategy
instruction
for
example
the
instructor
opened
a
3-page
document
that
had
11
different
bulleted
lists
the
students
were
asked
how
they
would
move
3
nonadjacent
bulleted
items
from
the
last
list
to
the
third
list
in
the
document
here
the
instructor
encouraged
the
students
to
discuss
alternate
methods
to
do
the
task
by
using
the
commands
they
had
just
learned
step
5
then
the
instructor
stated
that
the
advantage
of
using
the
split
window
or
new
window
to
perform
the
task
was
to
avoid
having
to
repeatedly
scroll
up
and
down
between
the
lists
step
6
the
instructor
demonstrated
this
method
in
a
practice
document
and
contrasted
it
with
the
inefficient
method
of
scrolling
step
7
the
students
were
then
given
the
strategy
handout
containing
the
strategies
and
their
instantiations
across
the
applications
as
previously
described
the
application-specific
strategy
just
taught
was
abstracted
to
the
general
strategy
view
parts
of
spread-out
information
to
fit
simultaneously
on
the
screen
the
students
were
asked
to
locate
the
strategy
just
taught
in
their
handout
and
were
shown
how
they
generalized
across
applications
step
8
steps
5-8
were
repeated
for
other
complex
tasks
demonstrating
the
utility
of
other
strategies
step
9
all
the
strategies
presented
in
the
class
were
then
summarized
by
explicitly
pointing
them
out
in
the
handout
step
10
the
steps
were
repeated
for
each
application
unix
msword
and
excel
the
presented
approach
contrasts
with
the
traditional
approach
of
teaching
such
applications
for
example
instructors
of
the
existing
cmu
course
are
taught
to
teach
commands
in
the
context
of
simple
tasks
steps
1-3
however
the
students
never
receive
instruction
on
how
to
assemble
the
commands
to
perform
complex
tasks
effectively
and
efficiently
thus
they
do
not
receive
any
instruction
on
the
general
nature
of
effective
and
efficient
methods
and
do
not
acquire
strategic
knowledge
that
they
can
use
across
applications
both
versions
of
the
course
were
taught
by
undergraduate
students
who
were
trained
to
teach
the
respective
courses
all
the
time
in
the
traditional
course
was
spent
on
teaching
pertinent
commands
and
on
examples
illustrating
their
use
in
the
strategy-based
course
the
time
was
spent
teaching
both
commands
and
general
strategies
but
because
the
teaching
of
these
was
tightly
integrated
the
total
time
spent
by
students
was
the
same
in
both
courses
2
4
evaluation
of
the
strategy-based
instruction
prototype
we
conducted
two
experiments
to
test
the
strategy-based
instruction
with
two
different
populations
the
first
experiment
cmu-1
was
conducted
with
science
and
engineering
students
and
was
designed
to
address
the
question
does
the
proposed
strategy-based
instructional
approach
help
the
acquisition
of
strategic
knowledge
without
harming
the
acquisition
of
command
knowledge
the
second
experiment
cmu-2
was
conducted
with
a
population
of
art
students
and
addressed
the
question
how
effective
is
strategy-based
instruction
for
teaching
students
with
non-technical
majors
2
4
1
method
for
cmu-1
the
students
were
divided
into
two
groups
the
command
group
received
the
instruction
ordinarily
provided
by
cmu
and
the
strategy
group
received
the
experimental
strategy-based
instruction
students
were
randomly
assigned
across
both
treatments
and
then
balanced
by
major
i
e
each
treatment
had
equal
numbers
of
students
from
each
technical
discipline
this
assignment
resulted
in
87
students
in
the
command
group
and
84
students
in
the
strategy
group
instructor
training
each
group
had
a
main
instructor
and
a
secondary
instructor
both
of
whom
were
undergraduate
students
at
the
university
the
main
instructor
taught
the
course
content
in
front
of
the
classroom
through
a
desktop
computer
connected
to
an
overhead
projector
the
role
of
the
secondary
instructor
was
to
provide
assistance
to
students
who
had
difficulty
following
the
instruction
or
had
trouble
with
the
computers
the
instructors
in
both
conditions
had
taught
the
existing
cmu
course
before
had
equivalent
experience
in
teaching
and
in
the
use
of
commands
and
were
considered
to
be
effective
instructors
all
the
instructors
therefore
had
received
the
same
instruction
on
how
to
teach
commands
but
the
strategy-group
instructors
got
extra
instruction
to
teach
the
general
strategies
instructors
in
both
groups
were
given
teaching
guides
to
help
teach
content
the
teaching
guides
for
the
command
group
consisted
of
a
list
of
commands
and
practice
files
developed
by
a
commercial
company
this
instructional
approach
is
typically
used
to
teach
computer
applications
in
educational
and
commercial
organizations
and
therefore
represented
a
realistic
comparison
condition
the
teaching
guides
for
the
strategy
group
included
the
same
commands
as
those
taught
in
the
command
group
but
in
addition
contained
instruction
on
how
to
teach
the
general
strategies
with
appropriate
demonstration
and
practice
examples
furthermore
the
strategy
instruction
included
problemsolving
requiring
interaction
with
the
students
our
guides
provided
the
overall
structure
for
instruction
but
excluded
the
actual
words
to
be
used
during
instruction
thus
the
guide
allowed
situated
elaboration
and
improvisation
by
the
instructors
while
such
teaching
guides
provide
structure
to
scaffold
new
instructors
and
enable
teaching
consistency
across
instructors
they
also
allow
for
creative
instructor
elaboration
which
could
lead
to
improved
learning
by
students
bereiter
2002
borko
and
livingston
1989
palinscar
1998
yinger
1987
this
balance
of
structure
and
improvisation
is
similar
to
how
experienced
teachers
typically
design
their
instruction
brown
and
edelson
2001
the
instruction
in
the
strategy
group
could
be
done
in
the
same
amount
of
time
as
in
the
command
group
because
the
strategies
were
tightly
integrated
into
the
teaching
of
the
commands
and
concretely
illustrated
in
the
strategy
handout
in
addition
we
created
handouts
to
explicitly
show
students
how
the
strategies
in
appendix
a
generalized
across
applications
3
posttest
tasks
the
posttest
was
given
in
a
computer
laboratory
and
consisted
of
three
sets
of
tasks
one
each
in
unix
msword
and
msexcel
as
shown
in
appendix
b
the
students
were
presented
with
tasks
and
instructions
on
paper
these
tasks
required
them
to
use
online
applications
and
files
which
consisted
of
a
unix
directory
populated
with
files
msword
to
create
a
new
file
and
an
msexcel
file
containing
a
spreadsheet
the
instructions
also
required
the
students
to
complete
after
each
task
a
brief
questionnaire
in
which
they
were
asked
to
explain
their
method
for
completing
the
task
and
their
rationale
for
using
that
method
finally
the
students
were
instructed
to
save
the
resulting
directories
and
files
which
were
checked
before
the
students
left
the
computer
laboratory
the
tasks
were
designed
to
take
a
maximum
of
1
5
hours
but
there
was
no
time
limit
given
to
the
students
the
students
were
spaced
out
in
the
computer
laboratory
to
ensure
that
they
could
not
see
the
details
of
the
computer
screen
of
other
students
in
the
experiment
embedded
in
the
previous
three
sets
of
tasks
were
13
opportunities4
shown
in
the
first
column
of
table
i
to
use
the
9
general
strategies
shown
in
appendix
a
for
example
task
3a
in
msexcel
required
the
students
to
find
which
of
two
pairs
of
days
had
the
lowest
temperature
in
a
large
spreadsheet
containing
temperature
data
one
way
to
perform
this
task
was
to
scroll
up
and
down
the
spreadsheet
in
order
to
compare
the
dates
another
way
to
perform
the
task
was
to
split
the
screen
into
two
panes
so
that
the
top
pane
would
display
at
all
times
the
column
headings
containing
the
dates
and
the
bottom
would
be
used
to
scroll
through
the
temperature
data
this
approach
provided
a
quicker
and
more
accurate
comparison
of
dates
each
of
these
strategy
opportunities
was
different
in
content
from
the
tasks
taught
in
the
experimental
course
participation
in
the
posttest
was
voluntary
students
were
requested
to
participate
in
the
posttest
for
25
and
were
informed
that
their
performance
on
the
posttest
would
not
affect
their
grade
this
recruitment
yielded
42
of
the
total
87
students
from
the
command
group
and
48
of
the
total
84
students
from
the
strategy
group
3
see
bhavnani
et
al
2001
for
a
detailed
description
of
the
course
implementation
and
http
//www
si
umich
edu/strategycourse/
for
the
teaching
guides
and
handouts
also
available
in
the
acm
digital
library
4
one
msword
task
did
not
motivate
the
use
of
the
strategy
that
it
was
designed
to
test
and
therefore
was
excluded
from
the
analysis
this
left
12
opportunities
to
use
8
strategies
2
4
2
method
for
cmu-2
the
method
for
cmu-2
was
similar
to
cmu-1
except
that
the
population
consisted
of
only
art
students
the
command
group
and
the
strategy
group
consisted
of
24
and
25
art
students
respectively
similar
to
the
cmu-1
experiment
students
were
requested
to
participate
in
the
posttest
for
25
the
recruitment
yielded
17
students
from
the
command
group
and
19
students
from
the
strategy
group
2
4
3
data
collection
we
collected
and
analyzed
five
types
of
data
1
screen-capture
videos
which
recorded
the
computer
interaction
of
each
student
2
command
logs
which
consisted
of
unix
history
files
and
msexcel
macro
files
these
files
contained
a
list
of
commands
executed
by
the
students
in
a
format
that
could
be
automatically
analyzed
by
computer
scripts
3
completed
task
files
which
were
collected
in
word
and
excel
4
qualitative
descriptions
authored
by
each
of
the
students
in
which
they
explained
how
they
completed
each
task
and
the
rationale
for
their
method
were
collected
5
exam
scores
which
were
based
on
an
exam
required
by
all
students
enrolled
in
the
course
and
tested
only
command
knowledge
were
used
to
check
whether
the
strategy
instruction
harmed
command
knowledge
2
4
4
analysis
the
focus
of
our
study
was
to
analyze
whether
or
not
students
recognized
the
opportunity
to
use
a
strategy
therefore
for
each
student
we
analyzed
each
of
the
12
strategy
opportunities
for
evidence
of
strategy
use
for
each
opportunity
students
were
given
a
binary
score
indicating
whether
or
not
they
used
the
strategy
in
that
particular
opportunity
this
led
to
nominallevel
data
for
each
strategy
i
e
a
student
either
used
a
strategy
or
did
not
table
i
shows
the
strategy
opportunities
for
each
task
the
criteria
for
strategy
use
and
the
method
used
to
analyze
the
strategy
where
possible
we
used
computational
methods
to
analyze
the
data
in
order
to
reduce
errors
in
all
cases
whenever
there
was
any
chance
for
ambiguity
the
data
were
double-checked
in
another
form
such
as
the
screen
capture
video
or
the
written
descriptions
for
example
as
shown
in
row
e
of
table
i
task
2a
in
msword
included
an
opportunity
to
use
the
strategy
make
dependencies
known
to
the
computer
students
were
given
credit
for
having
used
this
strategy
if
they
explicitly
defined
and
used
at
least
one
style
using
the
msword
style
command
strategy
use
was
assessed
by
first
looking
at
each
student
s
completed
task
file
for
evidence
of
style
use
and
then
confirmed
through
analysis
of
the
screen
capture
videos
this
confirmation
was
necessary
because
msword
sometimes
automatically
assigns
styles
to
text
2
4
5
results
cmu-1
we
performed
a
two-step
analysis
of
the
data
first
we
tested
for
an
overall
effect
by
analyzing
the
proportion
of
strategy
opportunities
used
by
each
student
in
the
command
and
strategy
groups
for
the
command
group
the
mean
proportion
of
strategies
used
was
42
66
sd
16
while
for
the
strategy
group
the
mean
proportion
of
strategies
used
was
68
58
sd
21
therefore
a
typical
student
in
the
command
group
used
42
66
of
the
strategy
opportunities
shown
in
table
ii
while
a
typical
student
in
the
strategy
group
used
68
58
this
difference
was
statistically
significant
based
on
a
t-test
of
the
proportions
t
88
6
73
p
001
in
the
second
step
of
our
analysis
we
tested
each
strategy
individually
to
provide
a
more
fine-grained
view
of
the
data
as
reported
elsewhere
bhavnani
et
al
2001
and
as
shown
in
table
ii
the
strategy
group
did
significantly
better
than
the
command
group
in
exploiting
seven
strategy
opportunities
p
0
05
for
each
of
the
seven
strategies
based
on
chi-square
tests
on
the
frequencies
in
each
group5
furthermore
there
was
no
statistically
significant
difference
in
command
knowledge
between
the
two
groups
as
measured
by
mean
scores
on
the
in-class
exams
which
tested
only
command
knowledge
as
discussed
previously
mean
for
command
group
96
07
mean
for
strategy
group
95
54
t
511
0
63
p
53
the
results
demonstrate
that
students
could
in
fact
be
taught
to
recognize
opportunities
to
use
efficient
strategies
and
to
execute
them
closer
inspection
showed
that
five
strategies
opportunities
d
i
j
k
l
in
table
ii
may
be
easily
acquired
just
by
learning
commands
for
example
even
though
the
command
group
was
given
only
command
instruction
all
of
them
recognized
the
opportunity
to
use
formulas
in
the
spreadsheet
task
i
one
explanation
is
that
once
commands
such
as
formulas
in
msexcel
are
learned
the
alternate
methods
e
g
doing
manual
calculations
in
the
spreadsheet
are
just
too
inefficient
to
be
considered
this
could
also
explain
why
two
sets
of
strategy
opportunities
f/k
and
c/j
each
testing
the
same
general
strategy
but
with
different
commands
had
very
different
usage
profiles
especially
for
the
command
group
these
results
also
confirm
laboratory
studies
which
show
that
under
certain
conditions
a
strategy
to
reuse
information
through
cut-and-paste
can
be
discovered
just
by
knowing
commands
charman
and
howes
2003
future
research
needs
to
explore
more
closely
what
makes
certain
strategy
opportunities
more
difficult
to
detect
compared
to
others
although
the
strategy
group
did
significantly
better
for
strategy
opportunities
b
and
h
the
actual
numbers
of
students
exploiting
those
opportunities
was
small
this
suggests
that
these
strategies
required
more
instruction
than
we
provided
however
because
zoom
often
leads
users
to
become
disoriented
the
lack
of
zoom
use
could
also
be
a
conscious
choice
to
avoid
such
problems
overall
the
results
showed
that
most
of
the
strategies
can
be
taught
in
the
same
amount
of
time
as
the
regular
approach
without
harming
the
acquisition
of
command-knowledge
cmu-2
as
shown
in
table
ii
the
results
of
cmu-2
were
similar
to
those
in
cmu-1
the
overall
effect
remained
command
group
mean
proportion
37
25
sd
19
strategy
group
mean
proportion
58
33
sd
20
t
34
3
21
p
01
furthermore
six
of
the
strategy
opportunities
showed
a
significant
difference
between
the
two
groups
these
results
show
the
utility
of
the
strategybased
instruction
for
students
with
very
little
technical
background
finally
command
knowledge
was
significantly
higher
in
the
strategy
group
than
the
command
group
command
group
mean
86
71
strategy
group
mean
95
47
t
151
3
97
p
001
this
suggests
that
teaching
commands
with
strategies
might
have
the
added
benefit
of
improving
command
knowledge
2
5
summary
of
the
strategy-based
instructional
prototype
in
phase-1
of
our
research
we
1
identified
a
strategy
framework
that
helped
to
organize
nine
effective
and
efficient
strategies
that
were
general
across
applications
2
identified
a
framework
to
teach
the
knowledge
components
3
implemented
the
instructional
framework
and
4
tested
the
instructional
design
in
two
controlled
experiments
furthermore
we
learned
that
a
few
strategies
did
not
require
explicit
instruction
while
others
required
more
instruction
than
we
expected
the
results
of
these
experiments
showed
that
the
strategy-based
instruction
1
enabled
students
to
learn
effective
and
efficient
strategies
2
benefited
student
populations
with
both
technical
and
nontechnical
majors
3
did
not
require
extra
time
compared
to
the
traditional
approach
focused
on
command
knowledge
and
4
did
not
harm
the
acquisition
of
command
knowledge
in
our
next
phase
we
tested
whether
the
instructional
framework
could
be
extended
to
new
applications
and
in
a
different
context
compared
to
the
prototype
3
extension
to
a
new
application
and
a
new
population
because
the
first
author
moved
to
the
university
of
michigan
we
had
the
opportunity
to
test
whether
the
strategy
framework
and
the
instructional
framework
were
robust
in
the
new
university
context
with
a
different
population
of
freshman
art
students
such
a
test
of
robustness
is
critical
because
educational
interventions
can
easily
fail
to
produce
positive
results
when
used
in
a
context
where
the
original
authors
have
less
control
brown
1992
however
testing
the
robustness
of
an
instructional
approach
in
a
new
realworld
context
usually
comes
at
the
cost
of
trading
off
experimental
control
a
common
problem
in
testing
educational
interventions
in
real
world
contexts
national
research
council
2000
brown
1992
as
described
in
the
following
section
we
had
to
make
many
modifications
to
the
instructional
design
to
fit
the
new
context
this
loss
of
control
prevented
us
from
making
statistical
comparisons
with
the
experiments
at
cmu
furthermore
unlike
many
user
tests
of
systems
that
take
around
an
hour
per
user
educational
field
experiments
may
take
an
entire
semester
this
long
time
span
reduces
the
kind
of
manipulations
that
can
be
done
practically
however
the
experience
of
testing
our
approach
in
a
new
context
led
to
insights
of
using
strategy-based
instruction
in
new
contexts
and
how
the
approach
could
be
disseminated
to
different
institutions
3
1
extension
of
the
strategy-based
instructional
design
the
new
context
had
three
major
differences
that
tested
the
robustness
of
our
strategy
and
instructional
frameworks
1
unix
was
not
taught
as
it
was
not
considered
an
application
that
was
particularly
useful
for
the
michigan
art
students
2
the
art
department
faculty
requested
that
the
course
be
extended
to
teach
dreamweaver
a
web-authoring
application
3
at
the
request
of
the
faculty
an
extra
day
was
added
for
teaching
each
application
to
enable
the
students
to
perform
a
summative
task
e
g
creating
a
resume
in
word
to
test
the
changes
we
asked
the
question
could
the
strategy-based
instructional
approach
be
applied
to
teach
new
applications
3
1
1
extension
to
a
new
application
to
develop
teaching
guides
for
dreamweaver
we
followed
a
three-step
process
1
identify
in
each
application
the
commands
that
were
appropriate
to
teach
freshman
art
students
this
was
done
in
consultation
with
the
arts
faculty
2
instantiate
the
9
general
strategies
in
dreamweaver
3
construct
for
each
application
a
4-day
teaching
guide
that
closely
followed
our
original
instructional
design
framework
but
added
an
extra
day
for
each
application
we
decided
to
teach
in
dreamweaver
17
commands
which
the
arts
faculty
agreed
were
useful
for
students
to
build
a
basic
web
site
where
the
students
could
upload
their
graphics
and
music
files
the
commands
ranged
from
open
new
file
to
define
website
and
publish
website
appendix
a
shows
how
each
of
the
9
strategies
were
instantiated
in
dreamweaver
using
the
chosen
commands
for
example
the
strategy
make
organizations
known
to
the
computer
was
instantiated
in
dreamweaver
through
commands
to
create
and
modify
tables
which
are
used
by
most
web
site
designers
to
organize
content
in
a
web
page
appendix
d
shows
the
portions
of
the
dreamweaver
teaching
guide
concerned
with
the
commands
to
use
tables
while
the
strategies
themselves
generalized
with
minimal
effort
our
main
difficulty
was
in
designing
the
demonstration
practice
and
problems-solving
tasks
for
dreamweaver
this
was
because
while
it
is
easy
to
construct
web
sites
that
have
minimal
functionality
a
credible
looking
web
site
that
was
motivating
and
relevant
to
the
art
students
required
a
considerable
effort
in
graphic
design
as
we
did
not
have
these
graphic
design
skills
we
employed
a
graphic
designer
to
construct
a
web
site
with
many
well-designed
pages
each
of
which
demonstrated
the
use
of
the
commands
and
strategies
that
we
had
chosen
to
teach
therefore
we
were
able
to
instantiate
the
nine
strategies
in
a
new
application
and
to
create
a
teaching
guide
for
the
new
application
this
demonstrated
that
we
were
successful
in
extending
both
the
strategy
framework
and
the
instructional
design
framework
to
a
new
application
furthermore
we
learned
that
applications
varied
in
the
amount
of
setup
costs
to
build
demonstration
and
practice
examples
that
were
relevant
and
motivating
3
2
effect
of
strategy-based
instruction
on
a
new
population
to
test
the
robustness
of
the
instructional
design
in
a
new
context
we
conducted
an
experiment
henceforth
referred
to
as
um-1
with
the
art
students
at
michigan
our
goal
led
to
the
following
research
question
how
robust
was
the
strategy-based
instructional
approach
when
implemented
in
a
new
context
with
a
new
population
and
with
less
control
because
retention
of
knowledge
is
critical
in
learning
we
also
wished
to
investigate
how
well
the
strategies
were
retained
over
time
this
was
not
investigated
in
the
cmu
experiments
as
the
posttests
were
given
immediately
after
the
instruction
set
was
complete
the
above
goal
led
to
the
following
research
question
does
the
strategy-based
instructional
approach
help
the
retention
of
strategic
knowledge
over
time
in
addition
to
the
primary
research
goals
to
test
robustness
and
retention
we
also
used
this
opportunity
to
perform
a
small
exploratory
study
to
probe
the
role
of
the
general
form
of
a
strategy
in
its
transfer
across
applications
3
2
1
method
for
experiment
with
art
students
um-1
as
discussed
in
section
2
2
our
goms
modeling
helped
to
pinpoint
three
types
of
knowledge
that
were
important
for
the
strategic
use
of
computer
applications
1
command
knowledge
e
g
the
existence
of
the
split
window
command
its
location
and
the
method
of
how
to
use
it
2
application-specific
strategic
knowledge
e
g
the
existence
of
a
strategy
to
modify
distant
parts
of
a
msword
document
by
using
the
split-window
command
3
application-general
strategic
knowledge
e
g
the
general
form
of
the
application-specific
strategy
so
that
it
can
be
used
across
applications
the
cmu-1
and
cmu-2
experiments
compared
students
who
were
taught
only
command
knowledge
command
group
to
students
who
were
taught
all
the
three
knowledge
components
strategy
group
in
the
current
experiment
fifty
art
students
at
the
university
of
michigan
were
randomly
divided
into
two
equal
groups
and
then
balanced
by
prior
experience
in
word
and
excel
one
group
was
given
the
same
instruction
as
the
strategy
group
in
the
cmu
experiments
with
the
modifications
discussed
in
section
3
1
and
is
therefore
still
called
the
strategy
group
analysis
of
how
the
strategy
group
performed
the
posttest
tasks
will
reveal
robustness
of
the
instructional
framework
when
taught
in
a
new
context
in
addition
to
this
analysis
we
explored
the
explicit
role
of
the
general
form
of
the
strategy
by
teaching
the
second
half
of
the
class
only
command
knowledge
and
application-specific
strategic
knowledge
for
example
while
we
taught
how
to
use
the
split
window
command
and
when
to
use
it
strategically
within
msword
we
did
not
teach
the
general
form
of
the
strategy
view
parts
of
spread
out
information
simultaneously
on
the
screen
because
this
group
was
taught
command
knowledge
and
only
the
application-specific
strategic
knowledge
it
was
called
the
command
ap-specific
group
furthermore
to
test
if
the
general
form
of
the
strategy
was
necessary
for
transfer
across
applications
we
also
did
not
teach
the
strategy
view
parts
of
spread
out
information
simultaneously
on
the
screen
in
excel
in
the
strategy
group
therefore
while
the
command
ap-specific
group
had
no
general
strategy
instruction
at
all
the
strategy
group
had
instruction
for
all
general
strategies
in
all
applications
except
for
the
above
strategy
in
excel
this
manipulation
allowed
us
to
explore
if
1
the
general
form
of
the
strategy
was
necessary
for
its
use
within
an
application
and
2
if
the
general
form
was
necessary
for
transferring
that
knowledge
across
applications
the
latter
was
tested
in
a
single
condition
for
exploratory
purposes
table
iii
shows
the
different
knowledge
components
taught
in
each
group
the
posttest
data
were
collected
as
part
of
the
final
exam
required
by
all
students
this
yielded
25
students
in
each
condition
the
posttest
for
each
group
was
identical
to
the
msword
and
msexcel
posttest
tasks
used
in
cmu-1
because
tasks
for
dreamweaver
require
long
set-up
times
exceeding
the
time
constraints
of
the
exam
we
were
unable
to
test
strategies
in
that
application
the
retention
test
see
appendix
c
was
conducted
one
month
later
to
explore
how
well
the
students
in
both
conditions
retained
the
strategies
the
students
were
requested
to
take
part
in
the
retention
study
for
406
this
yielded
18
and
20
students
from
the
command
ap-specific
group
and
the
strategy
group
respectively
where
all
these
students
constituted
76
of
the
original
class
there
was
no
statistically
significant
difference
between
the
posttest
scores
of
the
students
taking
the
retention
test
and
the
posttest
scores
for
the
students
who
did
not
take
it
this
result
suggests
the
absence
of
a
self-selection
bias
in
the
students
who
opted
to
take
the
retention
test
the
students
in
the
retention
test
were
given
isomorphs
of
the
posttest
tasks
described
earlier
and
the
analysis
criteria
were
identical
to
those
used
in
the
cmu
experiments
3
2
2
results
and
discussion
as
discussed
earlier
a
statistical
comparison
between
um-1
and
cmu-2
could
not
be
done
because
the
um-1
students
had
one
extra
day
of
instruction
per
application
we
therefore
present
the
data
in
figure
2
mainly
to
provide
a
direct
comparison
with
the
cmu-2
data
as
shown7
the
strategy
group
had
a
high
rate
of
strategy
use
comparable
to
the
strategy
group
in
cmu-2
also
art
students
in
four
strategy
opportunities
i
j
k
and
l
there
was
a
small
drop
5
10
7
and
11
in
strategy
use
the
rest
had
either
equal
or
much
more
strategy
use
the
extra
instruction
might
explain
the
higher
scores
that
um-1
students
acquired
in
the
posttest
for
strategy
opportunities
but
the
results
certainly
suggest
that
the
instructional
framework
was
robust
in
a
new
context
just
as
in
the
cmu
experiments
we
performed
a
two-step
analysis
procedure
in
the
first
step
we
tested
for
an
overall
effect
by
analyzing
the
proportion
of
strategy
opportunities
used
by
each
student
in
the
command
appspecific
and
in
the
strategy
groups
this
test
showed
that
the
mean
proportions
for
both
groups
was
very
high
and
that
no
significant
difference
existed
between
the
two
groups
command
ap-specific
group
mean
proportion
72
89
sd
18
strategy
group
mean
proportion
72
44
sd
24
t
48
0034
p
997
this
result
was
expected
because
with
the
exception
of
view
parts
of
spread
out
information
simultaneously
on
the
screen
in
msexcel
both
groups
were
taught
application-specific
strategies
in
the
applications
in
which
they
were
assessed
as
discussed
earlier
neither
the
command
app-specific
group
nor
the
strategy
group
in
um-1
was
taught
the
general
form
of
the
strategy
view
parts
of
spread
out
information
simultaneously
on
the
screen
in
msexcel
however
the
strategy
group
was
taught
the
general
form
of
the
strategy
in
msword
while
the
command
ap-specific
group
was
taught
only
the
application-specific
form
of
the
above
strategy
in
msword
to
test
if
this
had
an
effect
on
transfer
we
performed
the
second
step
of
our
analysis
a
chi-square
test
on
each
individual
strategy
opportunity
as
shown
in
table
iv
there
were
no
significant
differences
between
the
groups
on
directly-instructed
strategies
see
table
vi
in
appendix
e
for
details
of
each
statistical
comparison
however
based
on
a
chi-square
test
on
the
frequencies
in
each
group
there
was
a
significant
difference
df
1
n
50
3
99
p
05
between
the
command
ap
specific
group
and
the
strategy
group
for
the
untaught
transfer
strategy
g
view
parts
of
spread
out
information
simultaneously
on
the
screen
in
msexcel
this
result
suggests
that
transfer
is
improved
when
a
strategy
is
taught
in
its
general
form
this
result
is
however
not
definitive
because
we
tested
for
transfer
in
only
one
instance
however
despite
the
fact
that
the
split
window
command
necessary
to
execute
the
strategy
in
word
and
excel
is
identical
in
both
applications
the
results
indicate
that
application-general
strategic
knowledge
may
be
important
to
enable
transfer
transfer
has
been
difficult
to
achieve
in
many
studies
see
singley
and
anderson
1989
for
an
extensive
discussion
it
is
possible
that
transfer
did
occur
in
our
exploratory
study
because
we
1
explicitly
taught
the
conditions
of
when
to
use
the
strategy
in
the
context
of
tasks
in
a
specific
application
2
made
users
aware
of
the
nature
of
tasks
that
warrants
the
strategy
and
3
taught
the
strategy
multiple
times
with
different
examples
because
this
was
true
for
all
the
strategies
that
we
taught
we
believe
our
instructional
framework
appears
to
be
well-suited
for
the
transfer
of
strategies
and
is
in
agreement
with
earlier
research
on
transfer
of
skills
bossock
and
holyoak
1989
fong
et
al
1986
gick
and
holyoak
1983
however
future
research
should
test
this
result
with
more
extensive
transfer
conditions
finally
the
results
of
the
retention
test
showed
high
retention
of
strategic
knowledge
across
all
the
strategy
opportunities
none
of
the
differences
between
the
posttest
and
retention
test
were
significant
in
msexcel
or
msword
for
either
group
thus
retention
of
strategic
knowledge
after
one
month
was
high
in
both
groups
3
2
3
dissemination
of
the
strategy-based
instructional
approach
the
success
of
our
strategy-based
instructional
approach
in
two
universities
led
to
requests
for
the
use
of
the
course
material
by
the
school
of
nursing
at
the
university
of
michigan
the
nursing
faculty
requested
that
the
course
be
used
to
teach
their
freshman
students
the
strategic
use
of
computer
applications
they
also
requested
that
we
teach
powerpoint
in
addition
to
msword
msexcel
and
dreamweaver
the
course
was
taught
in
two
iterations
at
the
nursing
school
with
minimal
involvement
of
the
original
authors
this
was
achieved
by
providing
written
instructions
to
graduate
students
who
were
hired
to
teach
the
course
these
instructions
included
the
teaching
guides
discussed
in
section
2
4
1
and
a
set
of
guidelines8
of
how
to
design
and
execute
the
teaching
guides
the
graduate
student
instructors
modified
and
executed
the
course
with
minimal
involvement
of
the
original
authors
additionally
we
received
requests
from
two
other
universities9
that
wished
to
explore
how
to
provide
strategy-based
instruction
to
freshman
students
3
3
summary
of
extending
strategy-based
instruction
to
new
applications
and
populations
in
phase
2
of
our
research
on
strategy-based
instruction
we
tested
whether
the
strategy
and
instructional
frameworks
could
be
extended
to
a
context
requiring
a
new
computer
application
and
using
a
new
population
of
arts
students
furthermore
we
tested
whether
strategic
knowledge
could
be
retained
over
time
the
results
showed
that
the
strategy
framework
and
the
instructional
framework
could
be
successfully
extended
to
new
applications
and
to
new
populations
and
that
strategic
knowledge
was
retained
even
after
one
month
furthermore
an
exploratory
study
suggested
that
transfer
of
strategic
skill
to
a
new
application
improves
significantly
when
the
strategy
is
taught
in
its
general
form
finally
the
course
material
was
disseminated
to
another
context
within
the
university
of
michigan
and
to
the
university
of
western
australia
4
lessons
learned
in
teaching
the
strategic
use
of
complex
computer
applications
over
the
five-year
span
of
our
research
program
we
have
learned
lessons
related
to
1
the
need
to
teach
strategies
explicitly
2
the
organization
of
strategies
3
approaches
to
teach
commands
and
strategies
4
guidelines
for
creating
and
teaching
strategy-based
instruction
and
5
the
effects
of
strategy-based
instruction
on
learning
4
1
strategies
need
to
be
taught
explicitly
several
studies
have
shown
that
many
users
do
not
acquire
effective
and
efficient
strategies
to
use
computer
applications
in
fact
as
discussed
in
section
1
there
are
many
reasons
that
conspire
against
users
discovering
and
using
helpful
strategies
despite
many
years
of
experience
and
despite
well-designed
interfaces
while
we
have
experimented
with
online
intelligent
help
systems
bhavnani
et
al
1996
and
are
open
to
other
approaches
to
deliver
instruction
we
have
come
to
believe
that
whatever
the
medium
and
style
of
instruction
most
users
need
to
be
taught
strategic
knowledge
explicitly
before
they
acquire
a
wide
range
of
effective
and
efficient
strategies
4
2
strategies
exploit
general
powers
of
computers
to
teach
strategies
explicitly
we
first
need
to
know
what
they
are
while
several
studies
had
identified
the
need
to
teach
effective
and
efficient
strategies
none
of
the
studies
had
identified
a
systematic
approach
to
organize
them
because
we
wished
to
identify
strategies
that
generalized
across
authoring
applications
we
focused
on
the
general
functions
or
powers
that
these
applications
provided
this
led
us
to
organize
strategies
based
on
four
powers
of
computer
applications
iteration
propagation
organization
and
visualization
in
addition
to
helping
us
to
organize
the
general
strategies
and
instantiate
them
systematically
across
applications
the
framework
can
be
extended
to
include
new
strategies
as
new
powers
of
computer
applications
are
discovered
and
provided
to
users
4
3
strategies
should
be
taught
in
combination
with
commands
a
critical
component
of
strategy-based
instruction
is
that
commands
should
be
tightly
integrated
with
the
teaching
of
strategies
through
our
early
prototypes
we
learned
that
an
effective
way
to
teach
strategies
was
to
first
teach
how
to
use
a
small
set
of
commands
then
immediately
teach
when
to
use
those
commands
followed
by
teaching
the
general
form
of
the
strategy
other
approaches
such
as
providing
a
general
framework
of
all
the
strategies
before
teaching
commands
did
not
motivate
students
in
the
classroom
we
believe
this
is
because
learning
strategies
in
the
absence
of
knowing
how
to
implement
them
with
commands
is
too
abstract
4
4
general
strategies
can
enable
users
to
acquire
strategic
knowledge
while
there
has
been
research
in
teaching
strategies
to
perform
tasks
in
a
wide
range
of
domains
such
as
math
and
reading
we
found
no
systematic
studies
that
explored
how
to
teach
general
strategies
to
use
computer
applications
our
research
has
shown
that
for
the
most
part
merely
learning
commands
does
not
easily
lead
users
to
acquire
many
strategies
our
experiments
have
shown
that
to
learn
how
to
use
efficient
and
effective
strategies
to
use
computer
applications
in
a
short
amount
of
time
users
should
be
explicitly
taught
1
the
commands
needed
for
a
strategy
2
the
conditions
under
which
a
strategy
is
useful
and
3
the
strategy
itself
in
its
application-specific
form
if
users
are
expected
to
transfer
strategies
across
applications
there
is
some
indication
that
the
general
form
can
significantly
improve
transfer
even
when
the
commands
to
use
the
strategies
are
virtually
identical
5
reflections
this
article
has
focused
on
our
five
years
of
research
related
to
strategy-based
instruction
however
our
research
path
began
much
earlier
when
we
first
conducted
an
ethnographic
study
to
observe
how
architects
were
using
cad
systems
to
perform
real-world
tasks
bhavnani
et
al
1996
this
study
revealed
that
the
architects
were
not
using
effective
and
efficient
strategies
despite
knowing
how
to
use
the
commands
on
the
interface
and
despite
many
years
of
experience
in
using
the
cad
system
cognitive
analysis
of
these
real-world
tasks
suggested
the
existence
of
strategies
that
could
improve
the
performance
of
the
users
because
the
observed
users
had
few
problems
with
the
interface
we
decided
to
focus
on
strategy-based
instruction
to
address
the
ineffective
and
inefficient
use
of
computer
applications
our
decision
to
pursue
strategy-based
instruction
has
often
been
criticized
for
attempting
to
change
users
to
fit
poorly
designed
systems
this
argument
takes
the
position
that
the
need
for
instruction
represents
a
failure
in
the
design
of
the
interface
and
therefore
that
instead
of
attempting
to
change
the
user
through
instruction
we
should
attempt
to
change
the
interface
so
that
it
enables
users
to
spontaneously
discover
and
use
effective
strategies
we
believe
this
argument
ignores
important
characteristics
of
the
problem
consider
the
split
window
command
available
in
msword
and
msexcel
this
command
is
explicitly
designed
to
perform
the
simple
goal
of
dividing
a
window
into
two
panes
and
is
useful
in
a
wide
range
of
higher-level
editing
tasks
however
while
a
user
might
learn
from
the
interface
that
the
split
window
command
can
divide
the
window
into
two
panes
it
is
much
more
difficult
to
learn
from
the
same
interface
when
best
to
use
that
command
to
know
when
to
divide
the
window
into
two
panes
a
user
must
recognize
specific
characteristics
in
the
higher-level
task
for
example
a
user
must
learn
to
recognize
that
when
two
information
objects
that
need
to
be
compared
are
far
apart
in
a
document
then
they
need
to
be
brought
together
on
the
screen
before
performing
the
comparison10
acquiring
knowledge
to
recognize
such
task-related
cues
is
difficult
and
as
demonstrated
by
our
experiments
such
recognition
often
does
not
happen
spontaneously
just
from
knowing
how
to
use
commands
as
described
in
section
2
knowledge
to
detect
characteristics
of
a
higher-level
task
and
connect
them
to
specific
commands
has
been
a
critical
part
of
the
strategic
knowledge
that
we
have
abstracted
and
taught
one
can
argue
that
interfaces
could
automatically
detect
characteristics
of
a
task
from
user
actions
and
then
suggest
to
the
user
when
to
use
more
efficient
and
effective
methods
this
approach
can
be
successful
when
the
detection
is
unambiguous
like
inferring
that
a
user
is
manually
creating
a
numbered
list
and
automatically
converting
the
text
into
a
numbered
list
however
in
most
cases
automatic
detection
of
task
characteristics
is
not
unambiguous
for
instance
even
intelligent
interfaces
would
have
difficulty
in
unambiguously
inferring
that
a
user
was
attempting
to
compare
two
distant
pieces
of
information
and
correctly
suggest
the
use
of
the
split
window
command
such
ambiguity
can
lead
to
complex
carroll
and
aaronson
1988
and
often
annoying
dialogs
with
the
user
to
resolve
the
ambiguity
we
have
explored
approaches
to
automatically
detect
opportunities
for
using
more
effective
strategies
bhavnani
et
al
1996
and
believe
such
research
should
continue
and
be
informed
by
the
results
reported
here
for
example
such
projects
should
target
those
strategies
like
the
ones
identified
in
our
experiments
which
are
difficult
to
acquire
just
by
knowing
commands
however
we
believe
that
such
attempts
should
where
possible
be
complemented
with
strategy-based
instruction
that
can
assist
users
in
acquiring
a
comprehensive
understanding
of
the
power
of
computers
and
how
best
to
exploit
them
such
knowledge
as
we
have
explored
could
have
the
added
advantage
of
being
transferable
across
applications
looking
back
our
research
has
attempted
to
reduce
the
cost
of
learning
by
using
strategies
this
was
achieved
by
teaching
users
how
to
use
commands
how
to
recognize
opportunities
to
use
them
strategically
within
an
application
and
then
how
such
opportunities
generalize
across
applications
while
our
overall
approach
has
been
fairly
successful
it
may
be
insufficiently
motivating
for
students
who
already
have
a
substantial
knowledge
of
commands
accordingly
we
believe
that
it
might
be
useful
to
develop
a
course
that
focuses
only
on
strategic
knowledge
without
also
teaching
commands
we
are
also
exploring
the
development
of
a
minimalist
strategy
manual
that
would
provide
brief
online
instruction
of
strategies
for
use
with
different
computer
applications
furthermore
while
we
have
focused
on
delivering
strategy
instruction
in
a
classroom
context
we
would
also
like
to
explore
how
the
same
material
could
be
delivered
through
computer-based
tutors
future
research
also
needs
to
explore
how
well
the
strategies
transfer
across
applications
and
are
retained
over
longer
periods
of
time
furthermore
as
discussed
by
others
payne
et
al
2001
we
need
to
better
understand
the
attributes
of
and
conditions
under
which
some
strategies
are
automatically
acquired
just
by
learning
commands
more
research
is
needed
to
investigate
how
best
to
teach
instructors
how
to
design
and
execute
strategy-based
instruction
effectively
finally
the
ineffective
use
of
computer
applications
is
not
unique
to
authoring
applications
many
users
have
difficulty
in
acquiring
strategies
to
perform
effective
searches
on
the
web
bhavnani
2001
and
we
believe
that
our
instructional
framework
could
be
adapted
to
teach
strategic
knowledge
to
improve
information-seeking
behavior
bhavnani
2005
bhavnani
et
al
2006
our
hope
is
that
such
research
will
help
achieve
our
ultimate
goal
of
making
users
more
effective
and
efficient
in
the
use
of
a
wide
range
of
computer
applications
prism
interaction
for
enhancing
control
in
immersive
virtual
environments
scott
frees
ramapo
college
of
new
jersey
g
drew
kessler
sarnoff
corporation
and
edwin
kay
lehigh
university
when
directly
manipulating
3d
objects
in
an
immersive
environment
we
cannot
normally
achieve
the
accuracy
and
control
that
we
have
in
the
real
world
this
reduced
accuracy
stems
from
hand
instability
we
present
prism
which
dynamically
adjusts
the
c/d
ratio
between
the
hand
and
the
controlled
object
to
provide
increased
control
when
moving
slowly
and
direct
unconstrained
interaction
when
moving
rapidly
we
describe
prism
object
translation
and
rotation
and
present
user
studies
demonstrating
their
effectiveness
in
addition
we
describe
a
prismenhanced
version
of
ray
casting
which
is
shown
to
increase
the
speed
and
accuracy
of
object
selection
1
introduction
designing
models
within
immersive
virtual
environments
is
unique
in
that
we
directly
manipulate
objects
in
a
manner
similar
to
how
we
work
in
the
physical
world
direct
3d
interaction
offers
a
highly
intuitive
and
transparent
user
interface
in
contrast
to
using
a
mouse
and
keyboard
to
indirectly
control
a
3d
model
through
two-dimensional
abstractions
a
significant
drawback
of
direct
interaction
however
is
the
limited
accuracy
and
fidelity
of
interactions
in
the
physical
world
we
use
an
object
s
weight
and
inertia
along
with
friction
and
supporting
objects
to
control
our
hand
movements
and
counteract
hand
instability
in
the
virtual
world
these
aids
are
normally
not
at
our
disposal
and
hand
instability
dramatically
reduces
the
accuracy
with
which
we
interact
with
the
world
hand
instability
affects
many
of
the
design
choices
made
when
developing
an
immersive
application
objects
must
be
large
enough
to
be
easily
selected
limiting
the
complexity
of
the
world
in
which
a
user
can
work
once
the
object
is
selected
the
user
must
be
able
to
precisely
specify
its
final
position
and
orientation
one
might
provide
anchor
mapes
and
moshell
1995
and
grid
points
beir
1990
which
snap
the
object
to
predefined
and
thus
limited
positions
and
orientations
in
the
world
another
technique
is
to
explicitly
zoom
in
or
scale
up
the
workspace
in
order
to
work
in
small
areas
and
make
fine
adjustments
to
objects
mine
et
al
1997
bederson
et
al
2000
this
works
well
in
some
instances
but
in
an
immersive
environment
occlusion
and
the
loss
of
context
can
be
problematic
the
virtual
inertia
technique
ruddle
et
al
2002
addresses
hand
instability
directly
by
making
objects
resistant
to
movement
limiting
the
rate
in
which
they
can
be
moved
virtual
inertia
does
not
support
unconstrained
motion
when
precision
is
not
desired
however
yet
another
approach
is
to
use
indirect
object
control
perhaps
a
3d
menu
slider
or
button
once
again
hand
instability
limits
the
design
of
these
widgets
requiring
them
to
be
larger
than
their
2d
desktop
counterparts
the
increased
size
of
widgets
limits
the
number
of
widgets
that
can
simultaneously
be
placed
within
a
comfortable
working
space
we
present
a
3d
interaction
technique
called
prism
precise
and
rapid
interaction
through
scaled
manipulation
the
goal
of
prism
is
to
increase
the
accuracy
and
control
of
the
user
in
a
virtual
world
to
a
level
closer
to
what
they
are
accustomed
to
in
the
physical
world
prism
increases
the
control/display
cd
ratio
which
causes
the
cursor
or
object
to
move
and
rotate
more
slowly
than
the
user
s
hand
reducing
the
effect
of
hand
instability
one
challenge
of
improving
precision
is
deciding
how
to
constrain
interaction
without
sacrificing
speed
when
precision
is
not
desired
scaling
the
movement
and
rotation
of
the
hand
slows
the
speed
of
interaction
and
is
only
suitable
when
the
user
is
interested
in
accuracy
this
suggests
the
need
for
two
distinct
modes
one
that
scales
hand
movement
when
accuracy
and
precision
is
needed
and
one
that
provides
direct
unconstrained
interaction
when
moving
the
object
from
one
general
location
or
orientation
to
another
one
method
to
provide
these
two
modes
requires
the
user
to
explicitly
indicate
what
mode
is
desired
perhaps
through
a
menu
button
or
other
control
metaphor
recalling
fitts
law
fitts
1954
however
the
user
is
continuously
providing
the
interface
a
clue
as
to
whether
they
have
a
precise
goal
in
mind
fitts
law
states
that
as
the
target
gets
smaller
we
must
slow
our
hand
movement
conversely
if
the
target
is
large
we
can
and
will
move
more
quickly
this
implies
that
when
the
user
is
moving
their
hand
slowly
they
are
likely
to
have
a
precise
position
or
orientation
in
mind
prism
uses
this
premise
to
dynamically
adjust
the
cd
ratio
according
to
the
user
s
current
hand
speed
as
the
speed
of
the
hand
decreases
prism
increases
the
cd
ratio
to
provide
scaled
manipulation
and
filter
out
hand
instability
as
the
speed
of
the
hand
increases
prism
reduces
the
cd
ratio
back
towards
1
providing
direct
unconstrained
interaction
by
taking
advantage
of
the
principals
of
fitts
law
prism
provides
a
very
natural
control
metaphor
to
facilitate
modal
switching
the
user
s
natural
behavior
indicates
how
much
accuracy
and
precision
to
provide
this
type
of
dynamic
adjustment
of
the
cd
ratio
has
its
roots
in
2d
mouse/cursor
control
foley
et
al
1984
the
mouse
is
commonly
configured
to
cover
more
pixel
space
when
moved
at
high
speeds
and
less
when
moved
across
the
user
s
desk
slowly
the
next
section
provides
an
overview
of
other
techniques
that
relate
to
precision
interaction
in
both
2d
and
3d
section
3
describes
the
implementation
of
prism
translation
and
rotation
in
detail
section
4
discusses
how
prism
can
be
applied
to
several
common
tasks
in
an
immersive
environment
including
object
manipulation
ray
casting
and
widget
control
section
5
presents
a
series
of
user
studies
showing
the
effects
of
prism
followed
by
future
directions
and
conclusions
2
related
work
techniques
adjust
the
control
display
ratio
for
various
purposes
such
as
overview
vs
detailed
modes
while
navigating
documents
igarshi
and
hinckley
2000
and
desktop-based
3d
worlds
tan
et
al
2001
as
well
as
increasing
precision
on
desktop
blanch
et
al
2004
and
touch
screen
displays
albinsson
and
zhai
2003
although
similar
to
prism
in
concept
these
systems
along
with
mouse
cursor
control
involve
indirect
manipulation
where
reconciling
offset/divergence
between
the
physical
device
and
the
hand
is
a
more
straightforward
task
for
3d
direct
manipulation
dominjon
et
al
2005
altered
the
control/display
ratio
in
the
opposite
direction
from
prism
in
mixed
reality
environments
to
alter
the
user
s
perception
of
object
weight
for
their
purposes
users
could
not
see
the
divergence
between
the
hand
and
object
the
voodoo
dolls
technique
pierce
et
al
1999
enables
users
to
scale
their
workspace
and
thus
scale
their
movements
by
selecting
a
voodoo
doll
of
an
appropriate
size
prism
also
scales
down
the
user
s
movement
however
it
does
so
without
requiring
an
appropriately
sized
reference
object
which
might
not
always
be
available
other
techniques
use
relationships
between
objects
to
constrain
interaction
bukowski
and
s
quin
1995
stuerzlinger
and
smith
e
2002
these
techniques
still
require
some
form
of
direct
interaction
on
an
object
which
is
where
prism
can
be
useful
possibly
working
in
conjunction
with
these
techniques
although
prism
is
aimed
at
virtual
environments
without
force
feedback
many
applications
benefit
from
the
use
of
physical
props
to
aid
and
constrain
interaction
the
use
of
props
in
neurosurgery
applications
hinckley
et
al
1994b
menu
interaction
and
object
manipulation
lindeman
et
al
1999
ishii
and
ullmer
1997
have
been
shown
to
be
effective
although
there
is
little
doubt
that
manipulating
physical
props
helps
in
cognition
and
performance
props
are
specific
to
the
application
and
are
not
general
solutions
to
the
precision
problem
much
of
the
earlier
work
in
3d
rotation
tasks
focused
on
2d
mouse-based
techniques
for
the
desktop
including
the
virtual
sphere
chen
et
al
1988
and
the
arcball
shoemake
1992
unlike
object
translation
task
completion
time
is
generally
quite
high
in
3d
rotation
tasks
as
reported
by
ware
1990
and
zhai
et
al
1996
the
difficulties
of
these
indirect
rotation
techniques
can
be
mitigated
by
direct
manipulation
where
the
controlled
object
and
the
physical
hand
are
co-located
hinckley
et
al
1997
ware
and
rose
1999
poupyrev
et
al
2000
have
amplified
3d
rotation
control/display
ratio
less
than
1
and
concluded
that
amplifying
rotation
decreases
task
completion
time
without
significantly
effecting
accuracy
another
area
of
research
concerning
object
manipulation
focuses
on
selecting
and
interacting
with
objects
at
a
distance
some
of
the
more
common
techniques
include
ray-casting
scaled-world
grab
mine
et
al
1997
image
plane
interaction
pierce
et
al
1997
go-go
poupyrev
et
al
1996
homer
bowman
and
hodges
1997
and
world
in
miniature
wim
stoakley
et
al
1995
as
discussed
below
prism
can
be
used
in
conjunction
with
some
of
the
above
techniques
to
increase
precision
3
prism
interaction
direct
manipulation
of
virtual
objects
normally
consists
of
grabbing
an
object
moving
it
to
a
new
location
and/or
orientation
and
releasing
it
in
our
implementation
an
object
is
grabbed
when
the
user
places
the
tip
of
a
hand-held
stylus
within
a
virtual
object
and
holds
down
the
stylus
button
while
the
user
holds
the
button
the
position
and
orientation
of
the
virtual
object
directly
follows
the
movement
of
the
stylus
the
virtual
object
is
released
when
the
stylus
button
is
released
a
specific
design
challenge
for
a
direct
manipulation
interface
in
3d
is
to
provide
the
user
with
the
ability
to
perform
deliberate
precise
fine-grained
adjustments
to
the
position
and
orientation
of
objects
without
removing
the
ability
to
move
objects
quickly
from
one
general
position
to
another
hinckley
et
al
1994a
this
design
goal
implies
two
distinct
modes
of
interaction
one
where
constraints
are
supplied
to
aid
in
precision
manipulation
and
another
where
the
user
directly
controls
the
movement
of
the
object
free
of
any
artificial
constraints
in
response
to
this
prism
uses
the
hand
speed
of
the
user
to
gradually
switch
between
modes
by
altering
the
control/display
ratio
as
shown
in
figure
1
prism
uses
speed
thresholds--defined
by
thee
constants--to
determine
the
cd
ratio
that
controls
the
held
object
the
first
constant
is
a
minimum
speed
mins
below
which
a
user
is
unlikely
to
be
moving
purposefully
any
motion
below
this
speed
is
most
likely
tracking
error
or
inadvertent
drift
and
the
controlled
object
is
not
moved
the
second
and
most
significant
constant
is
the
scaling
constant
sc
if
the
user
is
moving
their
hand
slower
than
this
relatively
low
speed
they
are
likely
to
have
a
precise
goal
in
mind
while
in
scaled
manipulation
prism
sets
the
cd
ratio
inversely
proportional
to
the
hand
speed
for
example
if
the
user
is
moving
or
rotating
their
hand
at
a
slow
speed
close
to
the
mins
a
very
high
cd
ratio
is
used
and
the
object
would
move
very
little
if
the
hand
speed
is
closer
to
sc
the
cd
ratio
approaches
1
and
the
controlled
object
might
move
or
rotate
90
of
the
distance
the
hand
has
moved
during
the
most
recent
sampling
interval
any
time
the
user
is
moving
their
hand
at
a
speed
above
sc
they
are
transitioned
into
imprecise
direct
manipulation
mode
and
the
object
mimics
each
movement
and
rotation
of
the
hand
implicit
in
this
method
is
the
accumulation
of
an
offset
value
representing
either
the
positional
or
angular
displacement
between
the
hand
and
the
object
being
manipulated
each
time
movement
is
scaled
the
virtual
object
moves
or
rotates
a
fraction
of
the
distance
the
hand
does
although
there
are
implementation
differences
between
the
translation
and
rotation
techniques
in
both
cases
a
third
constant
maxs
triggers
automatic
offset
recovery
once
the
speed
of
the
hand
reaches
this
threshold
the
offset
is
automatically
reduced
causing
the
object
to
catch
up
to
the
hand
as
shown
in
figure
1
maxs
is
typically
larger
than
sc
in
order
to
provide
the
user
with
a
buffer
between
speeds
that
lead
to
direct
interaction
and
speeds
that
will
trigger
automatic
offset
recovery
there
exists
an
important
relationship
between
the
scaling
constant
sc
and
the
sensitivity
and
precision
via
scaling
the
user
experiences
the
lower
the
sc
value
the
less
scaling
will
take
place
making
the
controlled
object
more
sensitive
to
hand
movement
in
contrast
a
high
sc
value
provides
more
scaling
and
the
object
will
be
resistant
to
hand
motion
giving
the
user
the
ability
to
be
more
precise
some
users
naturally
move
their
hands
more
slowly
and
steadily
than
others
and
thus
feel
more
comfortable
using
a
lower
scaling
constant
on
the
other
hand
some
users
have
a
lower
degree
of
dexterity
and
have
particular
difficulty
keeping
their
hands
steady
users
in
this
category
require
the
object
to
be
less
sensitive
to
their
movements
thus
they
are
likely
to
benefit
from
a
higher
scaling
constant
the
ability
of
the
interface
designer
to
easily
change
the
behavior
and
feel
of
prism
by
adjusting
sc
not
only
allows
it
to
be
helpful
for
different
people
with
different
skills
it
also
allows
it
to
be
applied
in
many
different
situations
or
interaction
tasks
where
users
might
require
more
or
less
sensitivity
or
accuracy
3
1
prism
translation
implementation
details
prism
uses
hand
speed
to
govern
the
cd
ratio
for
object
translation
the
hand
speed
is
determined
by
taking
a
sample
of
the
hand
position
before
each
frame
and
determining
the
speed
using
the
current
location
and
the
location
of
the
hand
500
ms
in
the
past
note
that
this
is
a
somewhat
large
interval
and
that
small
movements
back
and
forth
within
the
last
500
ms
will
not
necessarily
count
towards
the
speed
value
by
using
a
smoothed
out
speed
a
small
movement
in
the
positive
direction
quickly
followed
by
a
movement
in
the
negative
direction
will
result
in
a
total
hand
speed
below
mins
and
the
cd
ratio
will
be
set
to
nullifying
hand
movement
this
is
crucial
since
small
frequent
movements
back
and
forth
are
often
indicative
of
tracking
jitter
or
hand
instability
that
ideally
should
not
cause
the
object
to
move
intentional
hand
movement
in
a
particular
direction
will
almost
always
last
more
than
500
ms--resulting
in
a
non-zero
speed
this
interval
was
chosen
through
observation
and
proved
to
be
a
reasonable
value
for
most
users
slightly
different
intervals
may
be
more
desirable
for
specific
individuals
this
is
a
simplistic
form
of
input
noise
reduction
compared
to
other
techniques
designed
to
combat
tracking
jitter
lian
et
al
1991
welch
and
bishop
1997
however
this
technique
has
proved
suitable
for
our
purposes
no
subjects
in
our
experiments
have
stated
that
the
latency
introduced
by
the
use
of
this
interval
was
perceptible
or
undesirable
in
any
way
to
move
the
object
itself
prism
determines
the
distance
and
direction
moved
between
the
last
two
frames
and
moves
the
object
some
proportion
of
that
distance
according
to
the
hand
speed
by
using
the
smoothed
out
speed
and
applying
it
to
the
instantaneous
movement
of
the
hand
prism
accurately
reflects
purposeful
and
sustained
movement
when
determining
the
cd
ratio
while
still
being
responsive
to
hand
movement
as
implied
in
1
once
the
hand
speed
reaches
the
scaling
constant
sc
the
cd
ratio
is
set
to
one
providing
direct
translation
dobject
is
the
distance
the
controlled
object
will
move
dhand
is
the
distance
the
hand
itself
moved
since
the
last
frame
shand
is
the
speed
of
the
hand
over
the
last
500
milliseconds
sc
is
scaling
constant
meters
per
second
in
our
trials
the
framerate
was
quite
constant
making
a
frame
based
offset
recovery
feasible
a
more
general
approach
would
be
to
reduce
offset
strictly
based
on
time
3
1
1
offset
recovery
whenever
the
user
is
in
scaled
mode
an
offset
accumulates
between
the
hand
and
the
controlled
object
this
offset
is
graphically
represented
by
a
white
line
between
the
virtual
hand
and
the
object
and
is
shown
in
figure
2
the
prism
translation
technique
uses
two
mechanisms
to
recover
this
offset
without
interrupting
the
user
s
interaction
the
first
mechanism
works
when
the
user
is
in
any
mode
has
accumulated
an
offset
in
a
particular
direction
and
then
changes
direction
moving
their
hand
back
towards
the
object
under
this
circumstance
the
object
is
not
moved
until
the
hand
crosses
back
through
the
object
reducing
the
offset
to
zero
this
mechanism
was
effectively
used
as
a
clutching/ratcheting
technique
by
users
and
also
guarded
against
unintended
hand
movements
the
second
automatic
offset
recovery
technique
activates
once
the
user
s
hand
speed
exceeds
the
maxs
threshold
at
this
speed
the
user
is
likely
trying
to
move
the
object
to
a
general
location
and
is
not
concerned
with
accuracy
starting
at
the
moment
the
speed
exceeds
maxs
prism
speeds
up
the
movement
of
the
object
so
it
catches
up
to
the
hand
once
the
object
and
hand
are
at
the
same
position
the
cd
ratio
returns
to
one
if
at
any
time
during
the
recovery
period
around
one
second
the
hand
slows
below
maxs
the
recovery
is
stopped
and
the
object
s
position
will
follow
the
hand
according
to
1
table
i
summarizes
this
offset
recovery
technique
3
1
2
axis
independent
scaling
prism
translation
operates
on
each
axis
x
y
and
z
in
world
coordinates
independently
for
instance
the
hand
speed
in
the
x
direction
only
affects
the
scaling
mode
and
movement
of
the
controlled
object
in
the
x
direction
this
allows
the
user
to
move
their
hand
rapidly
in
the
x
direction
and
retain
direct
control
while
simultaneously
being
more
precise
in
scaled
mode
in
the
y
and
z
direction
eliminating
inadvertent
drift
prism
could
have
been
implemented
using
the
euclidean
speed
of
the
hand
to
calculate
the
mode
and
degree
of
scaling
as
well
however
this
method
would
not
eliminate
drift
to
the
same
extent
the
biggest
advantage
of
independent
scaling
is
that
it
helps
users
move
the
object
in
a
straight
line
along
a
principal
axis
the
disadvantage
is
that
prism
scales
diagonal
movement
more
than
movement
along
a
principal
axis
scaling
against
the
principal
world
axis
was
a
fairly
arbitrary
design
choice
other
options
could
be
the
principal
axis
of
the
user
view
or
perhaps
another
object
s
coordinate
system
further
investigation
is
required
to
determine
which
scaling
method
is
most
suitable
for
different
situations
3
1
3
interaction
examples
figure
3
presents
several
typical
interaction
examples
for
simplicity
the
examples
are
illustrated
in
2d
in
figure
3
a
the
user
moves
a
virtual
sphere
to
the
right
at
a
speed
just
under
sc
the
hand
motion
in
the
horizontal
direction
is
scaled
by
a
small
amount
at
the
same
time
the
user
also
moved
their
hand
down
in
vertical
direction
at
a
very
slow
speed
indicative
of
an
inadvertent
drift
in
their
hand
position
since
the
scaling
values
are
calculated
independently
in
each
axis
this
vertical
movement
is
completely
filtered
out
without
affecting
the
movement
in
the
horizontal
direction
in
figure
3
b
the
user
moves
their
hand
in
the
same
manner
as
in
figure
3
a
however
this
time
the
hand
is
moved
quickly
in
both
the
horizontal
and
vertical
direction
this
quick
motion
faster
than
sc
results
in
direct
manipulation
in
both
directions
and
the
sphere
maintains
its
relationship
with
the
hand
in
figure
3
c
the
user
begins
with
an
offset
in
the
vertical
direction
from
some
previous
interaction
and
then
moves
slowly
up
towards
the
object
and
to
the
right
in
this
situation
the
vertical
offset
is
completely
recovered
and
none
of
the
upward
hand
movement
translates
into
vertical
movement
of
the
sphere
since
the
hand
also
moved
slowly
to
the
right
the
user
is
left
with
an
offset
in
the
horizontal
direction
in
figure
3
d
the
user
again
begins
with
an
offset
in
both
the
horizontal
and
vertical
directions
since
the
hand
motion
is
faster
than
maxs
the
offset
is
recovered
in
both
directions
as
the
controlled
object
catches
up
to
the
user
s
hand
3
2
prism
rotation
implementation
details
the
prism
rotation
technique
uses
the
angular
speed
of
the
hand
and
three
constants
to
determine
the
cd
ratio
of
the
interface
when
the
hand
is
rotating
slowly
below
a
mins
value
the
cd
ratio
will
be
and
the
object
will
not
rotate
as
the
rotational
speed
of
the
hand
increases
the
cd
decreases
and
the
object
is
rotated
by
an
angle
proportional
to
the
distance
the
hand
has
rotated
in
the
most
recent
sampling
interval
as
the
angular
speed
approaches
an
sc
value
the
object
rotates
directly
with
the
hand
the
third
constant
maxs
triggers
automatic
offset
recovery
and
is
discussed
in
section
3
2
1
working
with
3d
rotation
is
more
complex
than
the
euclidean
geometry
involved
in
translation
with
translation
prism
separates
the
movement
and
speed
of
the
hand
into
three
values
corresponding
to
the
x
y
and
z
directions
the
distance
covered
in
each
of
these
directions
can
be
easily
scaled
up
or
down
while
preserving
the
general
direction
of
movement
in
three
dimensions
although
a
similar
implementation
may
be
possible
using
euler
angles
pitch
yaw
and
roll
there
are
multiple
combinations
of
these
angles
that
describe
a
single
change
in
orientation
and
it
is
difficult
to
choose
one
that
is
consistent
with
the
user
s
model
of
the
change
along
with
gimbal
lock
these
problems
make
describing
3d
rotation
using
euler
angles
a
poor
approach
instead
prism
uses
a
quaternion
representation
for
the
orientation
of
the
hand
and
the
controlled
object
the
mathematics
of
quaternions
is
more
difficult
to
understand
however
concatenation
interpolation
and
scaling
of
3d
rotations
is
more
straightforward
a
simplified
view
of
a
quaternion
is
a
four-dimensional
vector
consisting
of
a
3d
vector
v
x
y
z
and
a
real
number
w
the
real
component
w
relates
to
the
angle
rotated
around
the
axis
defined
by
the
vector
v
w
is
cos
/2
where
is
the
angle
rotated
around
v
before
each
frame
a
quaternion
representation
of
the
hand
orientation
is
recorded
prism
rotation
calculates
the
rotational
speed
of
the
hand
before
each
frame
by
comparing
the
current
orientation
and
the
orientation
of
the
hand
200
ms
in
the
past
in
the
same
manner
as
in
prism
translation
this
sampling
interval
was
determined
through
observation
and
is
less
than
its
counterpart
in
the
translation
technique
because
a
continuous
rotation
normally
has
a
shorter
duration
than
a
translation
as
with
translation
users
did
not
report
negative
consequences
stemming
from
the
latency
introduced
by
using
this
interval
equation
3
calculates
the
rotational
difference
between
the
current
qt
and
last
orientation
qt-1
of
the
hand
in
the
form
of
qdiff
note
to
find
the
quaternion
needed
to
rotate
from
q1
to
q2
q2
is
divided
by
q1
equation
4
converts
the
angle
represented
by
qdiff
from
radians
to
degrees
and
eq
5
simply
divides
the
angle
by
200
ms
the
time
between
qt
and
qt-1
to
obtain
the
rotational
speed
of
the
hand
equation
6
is
used
to
determine
the
control
display
ratio
to
be
used
the
inverse
of
the
control
display
ratio
k
is
used
to
scale
rotation
in
eq
7
the
quaternion
representation
of
the
angle
the
hand
has
rotated
qdiff
is
scaled
by
raising
it
to
the
power
k
where
k
is
a
real
number
between
0
and
1
the
reader
is
referred
to
poupyrev
et
al
2000
and
shoemake
1985
for
details
on
how
the
quaternion
power
function
amplifies
and
scales
rotations
and
how
it
is
calculated
the
scaled
rotation
is
then
added
to
the
current
orientation
of
the
object
qobject
which
gives
the
new
orientation
of
the
object
qdiff
is
the
quaternion
representing
the
angle
the
hand
has
rotated
in
the
last
200
ms
qt
is
the
quaternion
representing
the
current
hand
orientation
qt-1
is
the
quaternion
representing
the
hand
orientation
200
ms
before
the
current
time
qnew
is
the
quaternion
representing
the
new
orientation
of
the
object
qobject
is
the
quaternion
representing
the
current
orientation
of
the
controlled
object
a
is
the
angle
in
degrees
the
hand
has
rotated
in
the
last
200
ms
rs
is
rotational
speed
of
the
hand
in
degrees/second
sc
is
scaling
constant
degrees
per
second
3
2
1
offset
recovery
when
scaling
the
rotations
of
the
hand
during
slower
movement
an
angular
offset
will
be
accumulated
between
the
hand
and
the
controlled
object
prism
helps
the
user
visualize
this
offset
by
drawing
two
sets
of
3d
axes
as
shown
in
figure
4
a
lower
axis
follows
the
rotation
of
the
hand
while
a
upper
axis
follows
the
orientation
of
the
controlled
object
when
the
axes
are
aligned
the
hand
and
the
object
are
at
the
same
base
orientation
as
they
diverge
the
user
can
see
that
scaling
is
taking
place
through
the
separation
of
these
axes
as
an
additional
aid
for
the
user
two
arcs
are
drawn
on
the
top
right
of
the
viewing
plane
the
lower
arc
is
always
the
same
length
half
circle
but
the
upper
arc
is
scaled
in
real
time
to
show
the
user
the
current
cd
ratio
when
the
upper
arc
is
very
small
the
object
will
resist
movement
high
cd
and
when
it
is
a
half
circle
it
will
follow
the
hand
more
directly
as
with
translation
prism
rotation
automatically
reduces
the
accumulated
angular
offset
once
the
user
rotates
their
hand
beyond
the
maxs
speed
threshold
while
prism
translation
reduces
this
offset
gradually
over
one
second
prism
rotation
eliminates
the
offset
immediately
which
causes
the
object
to
align
with
the
hand
we
use
immediate
offset
reduction
because
the
amount
of
rotation
that
can
be
performed
quickly
is
quite
limited
unlike
translation
rotation
is
bounded
in
that
one
can
only
rotate
an
object
360
degrees
before
returning
to
the
same
position
further
limiting
rotation
is
the
physical
limitations
of
the
wrist--normally
a
user
can
rotate
their
wrist
no
more
than
about
180
degrees
in
one
motion
without
putting
an
undesirable
amount
of
strain
on
their
elbow
thus
using
a
gradual
offset
reduction
accelerating
the
object
would
force
the
user
to
rotate
quickly
for
a
longer
period
of
time
than
necessary
and
would
be
undesirable
for
translation
prism
also
provides
offset
recovery
whenever
an
offset
is
accumulated
in
a
particular
direction
and
the
hand
is
then
moved
back
towards
the
object
translation
can
be
easily
broken
down
into
three
orthogonal
directions
simplifying
the
implementation
of
this
form
of
recovery
for
rotation
the
quaternion
representation
of
the
hand
and
object
rotation
cannot
be
broken
down
as
easily
it
is
difficult
to
determine
how
much
motion
is
actually
aimed
at
reducing
the
offset
although
a
similar
automatic
clutching
or
ratcheting
in
this
case
may
be
possible
for
3d
rotation
we
have
not
implemented
this
form
of
offset
recovery
3
2
2
interaction
examples
as
shown
in
figure
5
a
whenever
a
user
gains
control
of
an
object
the
cd
ratio
is
initialized
to
one
which
is
direct
manipulation
the
upper
arc
is
at
its
full
size
indicating
to
the
user
that
they
are
in
direct
control
of
the
object
and
the
object
and
hand
axes
are
aligned
indicating
that
there
is
no
angular
offset
present
in
figure
5
b
the
user
has
slowly
rotated
their
hand
clockwise
at
a
speed
of
around
sc/2
sc
was
30
degrees/second
in
our
implementation
here
the
upper
arc
is
at
half
size
indicating
that
the
object
will
only
rotate
about
half
the
distance
the
hand
rotates
since
the
object
rotation
has
been
scaled
there
is
an
angular
offset
between
the
hand
and
the
object
indicated
by
the
gap
between
the
upper
and
lower
axes
figure
5
c
shows
that
the
user
has
now
quickly
rotated
their
hand
counter
clockwise
all
offset
is
reduced
since
the
speed
of
rotation
was
above
sc
the
upper
arc
is
once
again
at
full
size
indicating
that
if
the
user
continues
to
rotate
at
this
speed
the
object
will
directly
follow
4
prism
applications
the
primary
goal
of
prism
is
to
enhance
the
precision
of
translational
and
rotational
input
coming
directly
from
the
user
s
hand
although
prism
was
designed
to
aid
in
direct
manipulation
of
virtual
objects
there
are
many
other
situations
where
the
position
and
orientation
of
the
hand
is
used
to
interact
with
the
virtual
world
or
the
interface
in
this
section
we
describe
several
situations
aside
from
direct
object
manipulation
where
prism
interaction
can
be
used
4
1
selection
by
increasing
the
precision
in
which
the
user
positions
and
orients
the
cursor
the
density
of
the
selectable
objects
within
a
confined
place
can
be
increased
without
introducing
selection
errors
one
of
the
most
fundamental
ways
of
selecting
objects
is
to
simply
move
the
cursor
controlled
by
the
user
s
hand
such
that
it
intersects
the
object
using
the
prism
translation
technique
the
position
of
the
cursor
can
be
specified
more
accurately
allowing
for
easier
selection
of
small
densely
packed
objects
this
increase
in
density
and
precision
could
improve
interaction
in
scientific
visualizations
and
surgical
training
applications
in
addition
prism
can
allow
designers
to
increase
the
density
or
decrease
the
size
of
input
widgets
such
as
the
command
control
cube
grosjean
and
coquillart
2001
without
decreasing
usability
the
command
control
cube
is
a
3d
cube
divided
into
27
equally
sized
cubes
3
3
3
in
which
each
cube
represents
a
specific
command
in
order
to
invoke
a
command
the
user
must
select
one
of
the
cubes
by
placing
the
cursor
inside
it
using
prism
each
individual
cube
could
be
smaller
allowing
for
either
more
commands
4
4
4
5
5
5
etc
within
the
same
dimensions
or
the
entire
interface
could
be
shrunk
uniformly
saving
screen
space
other
menu
interfaces
could
benefit
from
the
same
increase
in
density
as
well
there
are
many
situations
where
users
desire
to
select
objects
beyond
arms
reach
a
common
technique
for
this
task
is
ray
casting
mine
1995
were
the
user
points
his
or
her
hand
at
the
object
of
interest
and
a
ray
is
drawn
which
extends
outwards
from
the
user
s
hand/cursor
one
problem
with
this
approach
is
that
small
rotations
of
the
wrist
sweep
out
relatively
large
arcs
at
the
end
of
the
selection
ray
which
is
shown
in
figure
6
the
increased
sensitivity
to
slight
hand
rotations
makes
it
difficult
to
select
distant
objects
that
do
not
occupy
a
large
area
in
the
viewing
plane
a
problem
noted
by
poupyrev
et
al
1997
one
solution
to
this
is
to
snap
the
ray
to
the
nearest
intersecting
object
this
method
breaks
down
when
working
in
a
cluttered
environment
where
slight
movements
of
the
wrist
cause
the
ray
to
snap
between
several
proximate
objects
our
solution
to
improve
the
accuracy
of
ray
casting
employs
prism
rotation
to
control
the
angles
swept
out
by
the
ray
when
the
user
slows
the
rotation
of
their
wrist
the
orientation
changes
are
scaled
down
which
offsets
the
amplification
of
these
rotations
at
the
other
end
of
the
ray
section
5
presents
a
user
study
that
provides
more
detail
on
the
implementation
of
prism
enhanced
ray
casting
along
quantitative
performance
results
4
2
widget
control
most
3d
widgets
incorporate
some
form
of
direct
3d
translational
or
rotational
hand
input
the
world
in
miniature
wim
widget
stoakley
et
al
1995
provides
users
with
a
hand
held
miniaturized
model
of
the
virtual
world
complete
with
all
or
most
of
the
objects
within
it
the
user
directly
manipulates
the
miniaturized
version
of
the
objects
with
the
cursor
and
their
actions
are
mimicked
in
the
full-sized
world
one
problem
with
the
wim
is
that
in
order
to
interact
with
the
miniaturized
objects
one
must
scale
down
the
movements
of
the
cursor
as
well
moving
an
object
within
the
wim
one
centimeter
might
result
in
a
translation
of
one
meter
in
the
full-sized
world
if
the
user
wishes
to
move
an
object
by
a
centimeter
in
the
full-sized
world
it
would
be
nearly
impossible
to
do
so
using
the
wim
the
user
would
need
to
navigate
to
the
location
in
the
fullsized
world
and
manipulate
the
object
directly
a
wim
containing
a
cluttered
world
with
many
objects
poses
another
problem
in
that
many
small
miniaturized
objects
might
occupy
a
very
small
space
making
selection
increasingly
difficult
with
prism
users
would
be
able
to
move
the
cursor
quite
accurately
at
very
small
scales
allowing
them
to
select
small
closely
packed
objects
and
translate
them
at
scales
which
would
be
difficult
with
direct
manipulation
due
to
hand
instability
5
evaluation
we
have
conducted
four
separate
user
studies
examining
prism
before
each
experiment
users
were
provided
with
a
training
session
requiring
them
to
complete
practice
trials
using
both
direct
manipulation
and
prism
participants
were
not
permitted
to
participate
in
more
than
one
of
the
four
experiments
the
equipment
used
included
a
four-port
polhemus
3space
fastrak
electromagnetic
tracking
system
with
a
hand-held
stylus
polhemus
st8
approx
18
cm
long
1
3
cm
diameter
1
button
and
a
virtual
research
systems
v8
head
mounted
display
most
users
gripped
the
stylus
as
they
would
a
pencil
or
pen
using
the
index
finger
to
press
the
stylus
button
however
several
used
their
thumb
to
depress
the
button
we
used
the
simple
virtual
environment
toolkit
kessler
et
al
2000
to
implement
the
environment
the
system
rendered
the
test
environments
in
stereo
and
the
frame
rate
was
held
between
20
and
30
frames/sec
throughout
the
experiments
tracking
jitter
and
latency
were
within
expectable
limits
users
did
not
report
any
difficulties
in
each
experiment
participants
were
standing
and
free
to
walk
within
tracking
range
approximately
1m
radius
although
completion
of
the
experiments
did
not
require
a
large
degree
of
mobility
5
1
prism
translation
the
object
translation
task
required
the
user
to
pick
up
a
virtual
sphere
and
place
it
completely
inside
a
translucent
virtual
cube
such
that
no
part
of
the
sphere
protruded
any
side
of
the
cube
the
cube
changed
color
and
turned
opaque
each
time
the
user
positioned
the
sphere
completely
inside
the
target
and
released
the
stylus
button
to
drop
the
sphere
to
indicate
a
completion
a
short
time
later
the
sphere
re-appeared
in
its
original
starting
position
outside
the
cube
the
participants
were
asked
to
repeatedly
place
the
sphere
inside
the
cube
as
many
times
as
possible
during
the
3-minute
trial
in
each
trial
the
target
cube
became
smaller
after
each
completion
beginning
with
an
easy
difficulty
level
and
progressing
to
the
fourth
target
in
which
all
subsequent
cubes
were
very
difficult
this
variation
in
difficulty
level
was
included
to
increase
the
chance
that
all
participants
could
complete
at
least
one
target
in
a
trial
without
becoming
overly
frustrated
the
difficulty
levels
the
user
experienced
are
summarized
in
table
ii
since
all
trials
and
users
were
given
the
same
sequence
of
difficulty
levels
this
variation
does
not
affect
the
overall
performance
measure
number
of
completions
per
trial
the
starting
position
of
the
target
cube
and
mobile
sphere
were
the
same
for
each
participant
and
trial
the
target
cube
was
placed
at
shoulder
height
25
cm
below
the
top
of
the
hmd
near
the
center
of
the
virtual
world
each
sphere
was
placed
one
meter
away
from
the
target
in
order
to
eliminate
the
possibility
of
the
distance
from
the
target
affecting
the
difficulty
level
accot
and
zhai
1997
fitts
1954
the
spheres
were
also
placed
at
shoulder
height
the
distance
between
the
target
and
initial
position
of
the
sphere
ensured
that
movements
consisted
initially
of
arm/shoulder
movements
to
bring
the
sphere
close
to
the
target
and
then
fine-grained
hand
motion
to
precisely
align
the
objects
the
target
and
sphere
positions
relative
to
the
user
varied
since
the
user
was
free
to
move
around
as
they
wished
most
users
stood
at
a
position
where
the
target
was
directly
in
front
of
them
and
the
spheres
appeared
on
the
same
side
as
their
dominant
hand
the
training
session
prior
to
the
experiment
allowed
them
to
become
familiar
with
the
layout
of
the
world
we
conducted
a
within-subject
experiment
consisting
of
18
14
male
4
female
undergraduate
and
graduate
students
the
experiment
consisted
of
six
trials
and
included
two
factors
the
first
was
the
interaction
technique
at
three
levels
direct
prism
with
generic
scaling
and
prism
with
custom
scaling
the
custom
scaling
trials
used
a
scaling
constant
sc
chosen
by
the
participant
during
their
training
session
the
difference
between
generic
and
custom
prism
was
found
to
be
insignificant
and
will
not
be
discussed
in
detail
for
more
detail
on
the
custom
scaling
aspect
of
this
experiment
see
frees
and
kessler
2005
a
scaling
constant
of
0
15
m/sec
was
used
in
the
generic
trials
mins
and
maxs
were
0
01
m/sec
0
25
m/sec
respectively
for
all
trials
target
orientation
was
the
second
factor
target
cubes
were
either
axis-aligned
or
rotated
45
degrees
in
each
principal
axis
shown
in
figure
7
although
target
orientation
has
no
theoretical
effect
on
the
difficulty
of
the
task
the
sphere
still
needs
to
be
placed
at
the
centerpoint
of
the
target
users
have
a
tendency
to
move
the
sphere
diagonally
towards
the
rotated
targets
instead
of
along
a
principal
axis
we
compared
the
use
of
rotated
and
axis
aligned
targets
to
draw
conclusions
about
the
axis-independent
scaling
strategy
trial
orders
were
balanced
independently
for
interaction
type
and
target
orientation
5
1
1
results
the
number
of
completions
number
of
spheres
placed
inside
the
cube
in
each
trial
was
used
as
the
dependent
variable
in
the
experiment
the
fixed-effect
independent
factors
were
interaction
type
direct
generic
prism
and
custom
prism
and
target
orientation
axis-aligned
and
rotated
participants
were
treated
as
a
random
factor
figure
8
shows
the
mean
performance
under
each
of
the
conditions
table
iii
reports
the
results
of
the
anova
on
these
data
there
was
a
significant
effect
due
to
interaction
type
f
2
34
33
17
p
0
001
the
effect
of
target
orientation
was
not
significant
f
1
17
3
57
p
0
05
and
there
was
a
significant
interaction-type
target-orientation
interaction
f
2
34
3
50
p
0
05
a
visual
inspection
of
figure
8
indicates
that
there
were
more
completions
for
the
generic
prism
and
custom
prism
conditions
that
this
effect
was
greater
for
the
axis-aligned
conditions
and
that
there
was
no
difference
between
the
two
prism
conditions
this
last
observation
was
confirmed
by
an
insignificant
post-hoc
tukey
test
of
pair-wise
difference
between
the
two
prism
conditions
5
1
2
user
feedback
we
administered
an
exit
survey
in
which
users
responded
to
our
questions
by
giving
a
score
ranging
from
1
strongly
disagree
to
5
strongly
agree
when
asked
whether
they
were
able
to
adequately
learn
prism
during
the
training
period
14
participants
agreed
2
were
neutral
and
2
felt
they
did
not
nearly
all
participants
agreed
that
either
generic
or
custom
prism
was
better
than
direct
manipulation
13
preferred
generic
over
direct
16
preferred
custom
over
direct
there
was
no
consensus
as
to
which
set
of
targets
axis
aligned
or
rotated
were
more
challenging
7
preferred
axis
aligned
while
8
preferred
rotated
targets
5
1
3
discussion
prism
allowed
users
to
place
the
sphere
more
accurately
inside
targets
then
when
using
direct
manipulation
as
noted
above
difficulty
level
increased
within
each
trial
until
the
participant
reached
the
most
difficult
level
afterwards
each
target
remained
at
that
level
until
the
completion
of
the
3-minute
trial
this
meant
that
the
number
of
completed
targets
at
the
higher
difficulty
levels
varied
based
on
how
quickly
the
participant
completed
the
easier
ones
this
aspect
of
the
design
prevents
us
from
making
any
statistical
claims
as
to
how
prism
or
direct
manipulation
was
affected
by
the
difficulty
level
however
we
observed
a
clear
trend
on
the
first
two
targets
in
each
trial
the
largest
cubes
participants
did
not
have
trouble
using
prism
or
direct
manipulation
when
using
direct
manipulation
performance
dropped
off
dramatically
around
the
third
or
fourth
cube
when
using
prism
performance
did
not
noticeably
drop
off
until
the
targets
reached
their
smallest
size
and
even
then
performance
remained
acceptable
the
common
strategy
when
using
direct
manipulation
on
the
smaller
targets
was
to
rapidly
and
repeatedly
pick
up
and
drop
the
sphere--with
the
hope
that
it
would
eventually
be
dropped
in
the
correct
position
this
strategy
suggests
that
users
did
not
have
confidence
in
their
ability
to
purposefully
move
the
sphere
to
its
proper
position
this
lack
of
confidence
or
control
was
not
observed
during
the
prism
trials
although
performance
was
better
with
prism
for
both
types
of
targets
improvement
over
direct
was
reduced
when
using
rotated
targets
we
were
unable
to
determine
the
true
cause
of
this
and
none
of
the
participants
expressed
concerns
about
how
prism
was
responding
to
their
movements
most
participants
cited
difficulties
with
getting
a
good
view
of
the
target
when
asked
why
they
favored
one
set
of
targets
over
another
as
discussed
in
the
future
work
section
below
the
most
obvious
reason
performance
suffered
is
prism
s
axis-independent
scaling
which
slows
diagonal
motion
more
than
movement
along
a
principal
axis
5
2
prism
rotation
the
rotation
task
required
the
user
to
rotate
a
virtual
object
seen
in
figure
9
such
that
it
was
completely
inside
a
translucent
target
object
that
was
the
same
shape
but
slightly
larger
the
centers
of
the
object
were
always
placed
at
the
same
location
in
the
world
at
shoulder
height
and
translation
of
the
objects
was
disabled
leading
to
a
purely
rotational
task
participants
were
free
to
walk
about
the
world
as
they
wished
once
the
user
rotated
the
object
and
released
the
stylus
button
such
that
it
was
completely
inside
the
target
the
target
changed
color
and
became
opaque
to
indicate
a
completion
a
few
seconds
later
the
object
re-appeared
with
an
orientation
offset
from
the
target
the
target
orientation
was
held
constant
the
starting
orientation
of
the
mobile
object
was
randomly
selected
at
runtime
from
a
set
of
four
orientations
to
prevent
the
participant
from
being
able
to
perform
the
same
exact
rotation
repeatedly
which
could
lead
to
significant
learning
effects
each
of
the
four
orientations
required
between
50
and
80
degrees
of
rotation
about
an
arbitrary
axis
in
order
to
align
with
the
target
both
objects
were
made
up
of
nine
individual
cubes
of
equal
size
numbered
1
to
9
in
figure
10
in
order
to
record
an
alignment
the
user
needed
to
rotate
the
mobile
object
such
that
the
displacement
between
the
centers
of
each
set
of
corresponding
cubes
cube
1
in
both
the
mobile
and
target
object
were
within
a
maximum
allowed
tolerance
note
because
of
the
3d
nature
of
the
objects
it
is
impossible
to
align
the
objects
without
aligning
each
corresponding
cube
alignments
could
have
been
measured
using
the
pure
orientation
of
the
objects
however
determining
alignments
based
on
the
displacement
between
each
set
of
corresponding
cubes
is
more
straightforward
for
situations
when
translation
and
rotation
are
involved
as
is
the
case
in
section
5
3
this
experiment
included
two
factors
the
first
factor
was
the
interaction
technique
direct
and
prism
the
second
factor
was
target
size
or
error
tolerance
which
was
at
three
levels
described
in
table
iv
a
scaling
constant
of
30
degrees/sec
was
used
which
was
determined
from
pilot
studies
for
each
prism
trial
mins
was
set
to
1
degree/sec
and
maxs
was
at
35
degrees/sec
there
were
six
trials
each
lasting
2
minutes
users
were
asked
to
get
as
many
completions
as
possible
within
the
trial
each
user
performed
the
task
with
each
interaction
type/error
tolerance
combination
we
conducted
this
experiment
with
15
14
male
1
female
undergraduate
and
graduate
students
for
each
subject
the
trial
ordering
was
randomly
determined
note
that
the
error
tolerances
used
in
this
experiment
are
less
than
what
was
used
in
the
translation
task
more
accuracy
is
needed
based
on
pilot
studies
we
felt
users
would
be
able
to
achieve
a
higher
level
of
precision
in
a
rotation
only
task
than
they
were
able
to
achieve
with
translation
in
addition
rotation
error
propagates
when
larger
objects
are
being
used--which
is
not
the
case
with
translation
for
instance
when
rotating
an
object
2
m
long
an
error
of
just
1
degree
will
result
in
a
1
75
cm
displacement
of
the
object
at
its
end
points
the
size
of
the
target
was
not
adjusted
to
represent
the
error
tolerance
during
pilot
studies
several
users
complained
that
occasionally
the
system
did
not
register
an
alignment
even
when
the
mobile
object
appeared
to
be
completely
within
the
target
this
was
extremely
confusing
to
the
user
since
it
gave
no
indication
as
to
the
nature
of
the
misalignment
this
was
most
likely
a
visual
artifact
caused
by
the
graphics
system
the
target
object
remained
slightly
larger
than
the
mobile
object
only
so
the
geometries
of
the
objects
could
never
completely
overlap
which
caused
an
awkward
blending
of
the
objects
participants
were
told
to
align
the
two
objects
as
closely
as
possible
and
that
depending
on
the
error
tolerance
the
alignment
would
be
accepted
when
they
released
the
object
each
error
tolerance
allowed
a
larger
displacement
than
the
size
difference
between
the
target
and
mobile
object
this
ensured
any
alignment
that
appeared
to
be
completely
within
the
target
would
always
be
accepted--thus
ensuring
that
the
user
always
had
a
visual
indication
of
the
misalignment
a
wire
frame
visualization
of
the
error
tolerance
could
have
been
used
it
is
questionable
whether
users
would
have
been
able
to
effectively
use
that
type
of
feedback
however
as
it
would
be
challenging
to
identify
where
the
solid
object
was
protruding
out
of
a
wire
frame
object
by
a
small
amount
5
2
1
results
in
this
analysis
interaction
type
and
difficulty
were
the
fixed
independent
variables
we
also
included
participant
number
as
a
random
independent
variable
the
dependent
variable
was
the
number
of
completions
per
trial
the
anova
table
for
this
experiment
is
shown
in
table
v
and
the
mean
number
of
completions
per
trial
are
displayed
in
figure
11
there
were
more
completions
per
trial
in
the
prism
condition
than
in
the
direct
condition
f
1
14
5
28
p
0
05
as
the
difficulty
of
the
target
increased
the
number
of
completions
per
trial
decreased
f
2
28
20
67
p
0
0001
the
interactiontype
difficulty
interaction
was
not
significant
f
2
28
2
27
p
0
05
5
2
2
user
feedback
feedback
from
a
post-experiment
survey
was
similar
to
the
results
we
received
for
prism
translation
eleven
out
of
15
participants
preferred
prism
over
direct
rotation
three
preferred
direct
rotation
and
one
participant
had
no
preference
as
for
prism
being
easy
to
learn
prism
received
a
score
of
3
8
on
a
5-point
scale
where
5
was
very
easy
and
1
was
very
difficult
5
2
3
discussion
the
results
show
a
clear
increase
in
performance
when
using
prism
however
throughout
the
experiment
we
noticed
that
many
participants
had
difficulty
determining
which
way
to
rotate
the
object
in
order
to
fit
it
inside
the
target
often
the
user
would
see
the
object
protruding
out
of
a
section
of
the
target
but
could
not
determine
which
axis
to
rotate
the
object
around
in
order
to
get
the
desired
effect
these
types
of
problems
with
understanding
3d
rotation
along
with
poor
performance
in
3d
rotation
tasks
have
been
recognized
in
the
literature
parsons
1995
when
using
direct
rotation
this
was
not
as
problematic
since
accidental
rotations
of
the
hand
resulted
in
object
rotation
this
accidental
rotation
occasionally
caused
the
object
to
go
inside
the
target
on
the
easier
difficulty
levels
or
at
least
cued
the
user
as
to
which
direction
they
needed
to
rotate
the
object
on
the
more
difficult
targets
this
confusion
was
a
more
serious
problem
when
using
prism
since
prism
filters
out
much
of
the
accidental
and
unintentional
movement
the
user
needs
to
thoroughly
understand
which
direction
and
around
which
axis
they
need
to
rotate
the
object
from
observation
it
was
clear
that
once
the
user
knew
which
rotation
would
result
in
success
they
were
able
to
execute
that
rotation
far
more
effectively
with
prism
in
short
prism
enhanced
their
ability
to
rotate
objects
purposefully
but
it
was
a
detriment
if
the
user
did
not
know
which
rotation
was
needed
in
the
first
place
this
seems
to
be
a
general
problem
with
3d
rotation
rather
than
a
problem
with
prism
one
solution
might
be
to
administer
spatial
ability
tests
and
lower
the
scaling
constant
which
would
allow
for
more
unintentional
movement
for
those
who
score
poorly
we
also
suspect
that
providing
a
wire-frame
representation
showing
the
user
how
the
object
would
rotate
using
direct
manipulation
would
alleviate
some
of
this
confusion
when
using
prism
this
technique
has
been
successfully
employed
by
ruddle
et
al
2002
in
cluttered
environments
5
3
six
dof
with
prism
this
experiment
was
quite
similar
to
the
rotation-only
experiment
however
the
object
being
manipulated
was
not
constrained
to
a
fixed
position
the
mobile
object
started
approximately
0
5
meters
away
from
the
target
and
needed
to
be
translated
and
rotated
to
fit
inside
the
target
the
layout
of
the
world
mimicked
that
of
the
translation
experiment--the
target
object
and
the
starting
position
of
the
mobile
object
were
always
at
the
same
location
in
the
world
just
as
in
the
translation
experiment
the
distance
between
the
target
and
initial
position
of
the
mobile
object
ensured
that
the
task
required
both
arm/shoulder
movement
and
fine-grained
hand
motion
this
task
required
more
effort
and
skill
than
the
first
two
tasks
the
user
needed
to
accurately
control
the
position
and
orientation
of
the
object
this
is
more
difficult
because
hand
rotation
often
changes
the
hand
position
and
vice-versa
bowman
et
al
2001
due
to
the
added
difficulty
we
increased
the
time
for
each
trial
to
3
minutes
and
only
examined
two
difficulty
levels
which
are
shown
in
table
vi
the
experiment
consisted
of
four
trials
each
participant
performed
the
task
with
direct
manipulation
and
prism
with
both
tolerance
levels
in
each
trial
once
the
user
rotated
and
translated
the
object
and
released
the
stylus
button
such
that
it
was
completely
inside
the
target
the
target
changed
color
and
became
opaque
a
few
seconds
later
the
object
reappeared
at
the
original
starting
position
and
with
a
random
orientation
chosen
from
the
same
set
used
in
the
rotation-only
experiment
sixteen
undergraduate
and
graduate
students
12
males
4
females
participated
in
this
experiment
a
scaling
constant
of
0
15
m/sec
was
used
for
translation
and
30
degrees/sec
was
used
for
rotation
for
each
subject
the
trial
ordering
was
randomly
determined
we
recorded
the
number
of
completions
made
in
each
trial
5
3
1
results
the
results
of
this
experiment
are
displayed
in
figure
12
and
table
vii
below
as
expected
there
were
fewer
completions
for
the
hard
target
than
for
the
easy
target
f
1
15
54
00
p
0
0001
the
interaction-type
difficulty
interaction
was
significant
f
1
15
6
24
p
0
05
there
were
fewer
completions
for
the
prism
condition
with
easy
targets
and
more
with
the
hard
targets
we
observed
that
when
using
direct
manipulation
with
the
smaller
error
tolerances
the
participants
often
resorted
to
guessing
where
they
quickly
picked
up
and
dropped
the
object
repeatedly
in
hope
that
it
would
fall
into
place
to
look
at
this
more
closely
we
also
used
the
number
of
button
clicks
per
completion
as
a
dependent
variable
participants
did
not
seem
to
have
much
trouble
on
the
easier
targets
however
for
the
more
difficult
targets
participants
clearly
had
more
trouble
using
direct
as
evidenced
by
a
nearly
100
increase
in
the
number
of
times
the
user
picked
up
and
dropped
the
object
this
is
further
evidenced
by
a
significant
interaction-type
difficulty
interaction
f
1
15
14
09
p
0
01
5
3
2
user
feedback
overall
participants
found
the
trials
with
the
higher
error
tolerance
to
be
quite
easy
and
some
told
us
that
prism
slowed
them
down
on
these
trials
on
the
other
hand
most
participants
felt
prism
was
a
large
improvement
over
direct
manipulation
for
the
more
difficult
trials
overall
75
of
the
participants
thought
prism
was
more
preferable
and
more
effective
than
direct
manipulation
most
users
felt
prism
was
relatively
easy
to
learn
with
an
average
score
of
3
44
out
of
5
5
being
very
easy
to
learn
5
3
3
discussion
we
saw
a
number
of
outside
factors
contributing
to
performance
when
using
prism
first
it
was
quite
obvious
that
the
participants
were
having
difficulty
determining
which
direction
they
needed
to
turn
the
object
to
move
it
inside
the
target
just
as
we
saw
in
the
rotation-only
experiment
for
some
participants
there
was
even
more
confusion
due
to
the
translation
component
of
the
task
in
situations
where
a
small
rotation
would
have
moved
the
object
inside
the
target
many
participants
instead
tried
to
translate
it
repeatedly
once
again
this
type
of
confusion
favored
direct
manipulation
accidental
movement
of
the
object
at
least
gave
the
user
the
chance
of
moving
it
in
the
correct
direction
when
using
prism
this
accidental
movement
was
filtered
out
despite
this
issue
prism
outperformed
direct
manipulation
for
the
more
difficult
smaller
targets
and
we
believe
that
the
gap
between
prism
and
direct
manipulation
s
effectiveness
is
larger
than
the
results
suggest
the
increase
in
button
clicks
guessing
indicates
to
us
that
the
user
did
not
feel
confident
in
their
control
over
the
object
when
using
direct
manipulation
this
is
precisely
the
type
of
behavior
prism
is
trying
to
remedy
5
4
ray
casting
with
prism
rotation
we
implemented
ray
casting
by
drawing
a
white
line
extending
out
from
a
small
red
cone
attached
to
the
user
s
stylus
to
aid
in
targeting
a
red
sphere
was
placed
at
the
far
end
of
the
ray
far
beyond
the
targets
when
the
ray
intersected
the
target
the
far
end
of
the
ray
was
attached
to
the
object
and
the
sphere
in
the
distance
turned
green
to
indicate
that
the
target
could
be
selected
with
a
button
press
when
using
direct
manipulation
the
red
cone
attached
to
the
stylus
always
pointed
in
the
same
direction
as
the
stylus
when
using
prism
the
rotation
of
the
red
cone
was
scaled
proportionally
to
the
rotational
speed
of
the
stylus
which
can
be
seen
in
figure
13
in
pilot
studies
it
was
determined
that
the
scaling
constant
did
not
need
to
be
as
high
as
it
was
for
direct
object
rotation--the
prism
implementation
of
ray
casting
used
a
scaling
constant
of
20
degrees/second
this
experiment
required
users
to
select
small
far
away
objects
as
quickly
and
as
accurately
as
possible
each
trial
presented
the
user
with
a
series
of
15
targets
that
exploded
when
selected
users
were
given
a
starting
value
of
3000
points
their
points
decreased
at
a
rate
of
10
points
per
second
in
addition
each
time
the
user
pressed
the
stylus
button
but
missed
the
target
they
lost
200
points
the
point
system
was
explained
to
the
user
prior
to
the
experiment
and
they
were
instructed
that
in
order
to
maximize
their
points
for
each
trial
a
trial
was
complete
once
all
15
targets
were
selected
they
needed
to
value
accuracy
over
speed
each
participant
completed
four
trials
during
the
experiment
two
trials
using
prism
ray
casting
and
two
with
directly
controlled
ray
casting
the
participants
alternated
between
direct
and
prism
there
were
a
total
of
12
participants
1
female
half
of
the
participants
began
with
direct
half
with
prism
each
target
was
a
30
cm
cube
and
was
positioned
within
a
10
10
25
meter
x
y
z
region
centered
at
0
0
-37
5
in
world
coordinates
as
shown
in
figure
14
the
participants
began
the
experiment
at
world
coordinate
0
0
0
looking
down
the
negative
z-axis
they
were
told
in
advance
where
the
target
region
was
relative
to
their
position
and
the
training
session
allowed
them
to
become
accustomed
to
the
layout
of
the
environment
the
target
sizes
and
the
set
of
positions
within
the
target
region
remained
constant
for
each
trial
and
for
each
participant
to
avoid
search
time
becoming
a
factor
the
user
was
presented
with
one
target
at
a
time
each
time
a
target
was
shown
it
first
became
visible
directly
in
front
of
the
user
and
then
was
slowly
moved
to
its
final
position
once
the
target
reached
its
position
a
sound
played
indicating
the
user
was
permitted
to
select
it
upon
selection
the
next
target
would
become
visible
and
the
procedure
was
repeated
the
backdrop
of
the
virtual
environment
was
a
star
field
the
white
spots
in
figure
13
are
the
stars
which
helped
increase
the
users
situational
awareness
and
depth
perception
5
4
1
results
we
performed
an
anova
table
viii
on
the
data
with
completion
time
and
error
rate
misses/total
button
clicks
as
the
dependent
variable
the
independent
variables
included
interaction
type
direct
or
prism
in
addition
we
also
looked
at
the
trial
number
corresponding
to
the
technique
used
1
if
it
was
the
first
trial
using
direct
or
prism
2
if
it
was
the
second
trial
using
the
respective
technique
overall
our
results
show
that
prism
was
extremely
effective
in
increasing
the
accuracy
and
decreasing
completion
time
when
using
ray
casting
with
distant
targets
interaction
type
was
highly
significant
for
both
completion
time
f
1
11
28
89
p
0
001
and
error
rate
f
1
11
187
23
p
0
001
and
there
was
no
evidence
of
learning
effects
between
trials
these
results
are
summarized
in
figure
15
5
4
2
user
feedback
most
participants
thought
prism
was
more
effective
than
the
direct
form
of
ray
casting
with
11
of
12
participants
favoring
prism
prism
ray
casting
also
received
3
75
out
of
5
when
asked
how
easy
it
was
to
learn
a
common
problem
with
ray
casting
and
precision
is
the
slight
movement
of
the
wrist
when
the
user
attempts
to
press
the
stylus
button--an
example
of
the
heisenberg
effect
described
in
bowman
et
al
2001
this
problem
has
also
been
recognized
by
gerber
and
bechmann
2005
as
a
factor
limiting
the
number
of
menu
items
that
can
be
placed
in
their
spin
menu
and
was
echoed
by
our
participants
many
participants
told
us
that
whenever
they
pressed
the
button
their
hand
would
move
slightly
and
cause
them
to
miss
when
using
direct
ray
casting
these
participants
told
us
that
this
problem
was
almost
completely
eliminated
during
the
prism
trials
and
that
they
felt
like
the
ray
locked
into
place
5
4
3
discussion
the
results
of
this
experiment
show
that
using
prism
with
ray
casting
is
extremely
effective
prism
solves
a
fundamental
problem
with
ray
casting
the
inability
to
select
small
distant
objects
carrying
this
further
it
is
likely
to
solve
the
equally
important
problem
of
not
being
able
to
accurately
move
objects
from
a
distance
the
problems
with
ray
casting
are
caused
by
the
fact
that
small
rotations
of
the
wrist
result
in
large
translations
at
the
end
of
the
ray
where
the
cursor
or
an
object
is
being
moved
this
worsens
as
the
length
of
the
ray
increases
prism
works
to
counteract
this
by
scaling
down
the
rotation
of
the
hand
or
source
object
the
red
cone
in
our
implementation
we
also
noticed
that
using
prism
for
ray
casting
encouraged
more
comfortable
hand
positions
and
movements
we
observed
that
when
users
had
difficulty
targeting
objects
with
the
directly
controlled
ray
casting
they
tended
to
point
outwards
with
their
arms
sometimes
using
two
hands
much
like
they
would
when
aiming
a
pistol
a
more
experienced
user
would
normally
shoot
from
the
hip
instead
which
is
much
less
fatiguing
prism
encouraged
the
user
to
hold
their
hand
in
a
comfortable
position
and
move
the
ray
only
by
rotating
their
wrist
this
was
partly
due
to
the
increased
ability
to
target
objects
however
prism
was
also
easier
to
use
when
the
wrist
was
the
primary
joint
involved
sweeping
out
arcs
with
the
shoulder
or
elbow
is
in
general
a
much
slower
movement
than
a
wrist
rotation
and
this
slower
rotation
forces
prism
into
scaling
mode
once
users
realized
they
could
take
advantage
of
prism
s
modal
switching
more
effectively
when
rotating
their
wrist
quickly
to
cover
larger
angular
distance
they
became
much
more
effective
of
course
when
designing
a
technique
that
requires
the
use
of
the
shoulder
or
elbow
to
rotate
something
the
scaling
constant
could
be
decreased
to
provide
direct
control
when
moving
more
slowly
however
for
traditional
ray
casting
the
scaling
constant
used
in
this
experiment
was
a
good
choice
6
future
directions
a
short-term
issue
we
would
like
to
further
investigate
is
prism
translation
specifically
the
axis-independent
scaling
technique
axis-independent
scaling
offers
a
distinct
advantage
over
uniform
scaling
it
helps
the
user
move
an
object
in
a
straight
line
or
along
a
plane
by
eliminating
drift
along
the
axis
of
movement
in
which
movement
is
slowest
unfortunately
performance
is
slowed
for
diagonal
movement
seen
in
the
tasks
with
rotated
target
cubes
in
our
user
study
in
the
future
we
plan
to
implement
a
version
of
prism
that
scales
translation
uniformly
in
all
directions
by
using
the
absolute
speed
of
the
hand
to
determine
one
true
cd
ratio
in
the
long
term
we
would
like
to
further
investigate
prism
s
applicability
to
other
tasks
and
with
a
number
of
user
interface
widgets
as
described
in
our
user
study
prism
rotation
can
enhance
selection
with
ray
casting
significantly
to
follow
this
up
we
would
like
to
work
with
user
interface
controls
such
as
menus
and
buttons
we
believe
that
by
using
a
prism
controlled
cursor
we
can
increase
touch
selection
accuracy
and
thus
decrease
the
size
of
the
typical
widgets
used
in
vr
in
addition
the
usability
of
a
world
in
miniature
wim
may
be
significantly
enhanced
through
prism
manipulation
and
we
plan
on
running
experiments
focused
on
this
7
conclusions
in
the
physical
world
we
move
and
rotate
objects
quite
accurately
by
using
friction
and
the
object
s
inertia
to
help
steady
our
movements
in
the
virtual
world
none
of
these
physical
properties
normally
exist
and
object
manipulation
is
negatively
affected
by
hand
instability
we
have
shown
in
a
series
of
user
studies
that
prism
increases
precision
and
allow
users
to
make
purposeful
and
fine-grained
adjustments
to
the
position
and
orientation
of
an
object
in
a
timely
manner
when
examining
translation
and
rotation
separately
there
was
a
statistically
significant
increase
in
performance
when
using
prism
over
direct
manipulation
with
prism
rotation
and
translation
combined
6dof
task
results
were
less
clear
however
prism
continued
to
out
perform
direct
manipulation
for
trials
requiring
very
high
precision
accuracy
was
increased
in
these
tasks
because
prism
successfully
filters
out
a
significant
amount
of
hand
instability
however
in
situations
where
there
is
absolutely
no
tolerance
for
error
other
techniques
might
be
more
appropriate
unlike
many
other
techniques
aimed
at
increasing
precision
prism
does
not
place
any
limits
on
the
granularity
in
which
the
user
can
work
with
objects
and
does
not
overconstrain
interaction
when
precision
is
not
required
by
utilizing
the
principle
of
fitts
law
prism
dynamically
adjusts
the
control/display
ratio
without
explicit
user
intervention
to
provide
extra
precision
when
moving
slowly
and
direct
unconstrained
manipulation
when
the
hand
is
moving
quickly
we
believe
that
the
natural
control
metaphor
used
by
prism
is
also
what
makes
it
so
easy
to
learn
we
have
shown
that
prism
can
also
apply
to
the
control
of
a
3d
cursor
in
particular
we
have
shown
that
a
prism-enhanced
version
of
ray
casting
can
significantly
increase
the
speed
in
which
objects
or
buttons
and
controls
can
be
selected
and
dramatically
decreases
miss
rates
prism
is
applicable
across
a
wide
variety
of
tasks
in
immersive
virtual
environments
wherever
the
hand
directly
controls
the
position
and
orientation
of
the
cursor
it
is
our
hope
that
this
increase
in
precision
will
allow
designers
to
increase
the
complexity
of
their
virtual
worlds
without
suffering
from
a
manipulation
and
interaction
bottleneck
frequency-based
identification
of
correct
translation
equivalents
fite
obtained
through
transformation
rules
ari
pirkola
jarmo
toivonen
heikki
keskustalo
and
kalervo
jarvelin
university
of
tampere
we
devised
a
novel
statistical
technique
for
the
identification
of
the
translation
equivalents
of
source
words
obtained
by
transformation
rule
based
translation
trt
the
effectiveness
of
the
technique
called
frequency-based
identification
of
translation
equivalents
fite
was
tested
using
biological
and
medical
cross-lingual
spelling
variants
and
out-of-vocabulary
oov
words
in
spanish-english
and
finnish-english
trt
the
results
showed
that
depending
on
the
source
language
and
frequency
corpus
fite-trt
the
identification
of
translation
equivalents
from
trt
s
translation
set
by
means
of
the
fite
technique
may
achieve
high
translation
recall
in
the
case
of
the
web
as
the
frequency
corpus
translation
recall
was
89
2
91
0
for
spanish-english
fite-trt
for
both
language
pairs
fite-trt
achieved
high
translation
precision
95
0
98
8
the
technique
also
reliably
identified
native
source
language
words
source
words
that
cannot
be
correctly
translated
by
trt
dictionary-based
clir
augmented
with
fite-trt
performed
substantially
better
than
basic
dictionary-based
clir
where
oov
keys
were
kept
intact
fite-trt
with
web
document
frequencies
was
the
best
technique
among
several
fuzzy
translation/matching
approaches
tested
in
cross-language
retrieval
experiments
we
also
discuss
the
application
of
fite-trt
in
the
automatic
construction
of
multilingual
dictionaries
1
introduction
out-of-vocabulary
oov
words
constitute
a
major
problem
in
cross-language
information
retrieval
clir
and
machine
translation
mt
in
those
cases
where
equivalent
terms
in
different
languages
are
etymologically
related
technical
terms
cross-lingual
spelling
variants--as
german
konstruktion
and
english
construction
it
is
possible
to
use
a
transliteration
type
of
translation
to
recognize
the
target
language
equivalents
of
the
source
language
words
in
pirkola
et
al
2003
we
generated
automatically
large
collections
of
character
correspondences
in
several
language
pairs
for
the
translation
of
cross-lingual
spelling
variants
equivalent
term
pairs
in
two
languages
were
first
extracted
automatically
from
translation
dictionaries
and
then
regular
character
correspondences
between
the
words
in
the
two
languages
were
identified
using
an
edit
distance
measure
large
sets
of
transformation
rules
augmented
with
statistical
information
were
generated
for
automatic
translation
of
spelling
variants
we
call
the
translation
technique
based
on
the
generated
rules
transformation
rule
based
translation
trt
trt
is
similar
to
transliteration
except
that
no
phonetic
elements
are
involved
in
it
the
term
fuzzy
translation
is
used
in
connection
with
trt
it
refers
to
the
fact
that
trt
often
gives
many
possible
equivalents
for
a
source
word
not
one
equivalent
or
several
alternatives
like
regular
translation
in
toivonen
et
al
2005
we
showed
that
high
translation
recall--the
proportion
of
source
words
for
which
trt
yields
equivalents
among
all
source
words--may
be
achieved
when
most
of
the
rules
available
for
a
source
word
are
used
in
trt
however
high
translation
recall
is
associated
with
low
translation
precision
the
proportion
of
equivalents
among
all
word
forms
yielded
by
trt
in
other
words
the
translation
set
containing
the
target
language
word
forms
often
includes
the
correct
translation
equivalent
of
a
source
word
and
a
large
number
of
other
word
forms
it
is
obvious
that
a
technique
where
words
not
found
in
a
dictionary
are
translated
by
transformation
rules
would
be
useful
in
many
information
systems
where
automatic
translation
is
part
of
the
system
however
in
many
cases
the
trt
technique
may
be
useless
if
it
just
indicates
a
set
of
possible
translations
for
a
source
word
but
is
not
able
to
indicate
the
one
correct
equivalent
which
was
the
case
in
pirkola
et
al
2003
as
well
as
in
toivonen
et
al
2005
in
the
present
research
we
attack
this
problem
moving
trt
from
fuzzy
translation
towards
dictionary-like
translation
where
for
each
source
word
either
one
translation
equivalent
is
indicated
or
the
source
word
is
indicated
not
to
be
translatable
by
means
of
trt
for
this
we
developed
a
novel
statistical
equivalent
identification
technique
called
frequency-based
identification
of
translation
equivalents
fite
the
identification
of
equivalents
is
based
on
regular
frequency
patterns
associated
with
the
target
word
forms
obtained
by
trt
in
this
paper
we
also
present
a
novel
feature
of
trt
translation
through
indirect
translation
routes
if
a
direct
translation
from
a
source
language
into
a
target
language
fails
to
find
an
equivalent
the
source
word
is
retranslated
into
a
target
language
through
intermediate
pivot
languages
as
in
the
case
of
direct
translation
the
equivalents
are
identified
from
trt
s
translation
set
by
means
of
the
novel
fite
technique
transitive
translation
through
a
pivot
language
is
a
well-known
technique
in
clir
used
to
address
the
problem
of
limited
availability
of
translation
resources
ballesteros
2000
gollins
and
sanderson
2001
lehtokangas
et
al
2004
indirect
translation
could
be
used
in
trt
in
cases
where
direct
translation
is
not
possible
due
to
the
lack
of
translation
resources
transformation
rules
in
this
study
however
we
investigate
whether
indirect
translation
improves
fite-trt
effectiveness
it
may
compensate
the
failures
of
direct
translation
and
thereby
increase
translation
recall
we
explore
the
effectiveness
of
fite
in
spanish-english
and
finnishenglish
trt
german
and
french
serve
as
intermediate
languages
for
both
language
pairs
as
test
words
we
use
terms
in
the
domains
of
biology
and
medicine
the
terms
were
selected
from
texts
and
real
information
requests
of
biomedical
researchers
fite-trt
is
also
applied
as
part
of
an
actual
clir
system
the
effectiveness
of
dictionary-based
clir
augmented
with
fite-trt
is
compared
to
the
effectiveness
of
dictionary-based
clir
augmented
with
plain
trt
and
skipgram
keskustalo
et
al
2003
oov
word
methods
we
also
run
dictionary-translationonly
no
oov
word
technique
is
applied
and
monolingual
english
queries
as
baselines
in
pirkola
et
al
2006
we
presented
the
main
features
of
fite-trt
and
the
first
results
on
fite-trt
effectiveness
and
the
effectiveness
of
clir
augmented
with
fite-trt
in
this
article
we
describe
the
fite-trt
technique
in
more
detail
present
the
find-equivalent
algorithm
and
extend
the
first
study
by
using
large
word
frequency
lists
mined
from
the
web
as
fite-trt
s
frequency
source
and
by
comparing
fite-trt
to
other
oov
word
methods
in
cross-language
retrieval
experiments
the
novel
fite-trt
technique
is
fundamentally
different
from
other
oov
word
methods/systems
presented
in
the
literature
for
instance
cheng
et
al
2004
and
zhang
and
vines
2004
both
developed
a
web-based
translation
method
for
chinese-english
oov
words
where
the
oov
words
were
extracted
from
bilingual
chinese-english
texts
found
in
chinese
web
pages
using
word
co-occurrence
statistics
and
syntactic
structures
meng
et
al
2000
employed
a
trt
type
rule-based
approach
to
the
oov
word
problem
phonetic
mappings
were
derived
from
english
and
chinese
mandarin
pronunciation
rules
for
english-chinese
spoken
document
retrieval
the
researchers
also
considered
chinese
name
variation
an
english
proper
name
may
have
several
character
sequence
variants
and
pronunciations
in
chinese
to
combat
this
problem
the
transliteration
approach
should
involve
approximate
matches
between
the
english
and
chinese
pronunciations
fujii
and
ishikawa
2001
used
characterbased
rules
to
establish
mapping
between
english
characters
and
romanized
japanese
katakana
characters
they
also
utilized
probabilistic
character-based
language
models
which
can
be
seen
as
a
variation
of
fuzzy
matching
the
technique
is
different
from
fite-trt
but
bears
some
resemblance
to
the
fuzzy
translation
reported
in
pirkola
et
al
2003
however
focusing
on
languages
with
different
orthographies
thus
having
a
different
focus
the
skipgram
fuzzy
matching
approach
to
oov
words
by
keskustalo
et
al
2003
is
discussed
in
section
5
2
1
the
rest
of
this
article
is
organized
as
follows
section
2
presents
the
trt
technique
its
background
research
the
transformation
rule
collections
and
the
dictionary
data
that
was
used
in
the
rule
generation
in
section
3
we
define
the
terms
cross-lingual
spelling
variant
and
native
word
and
present
the
research
problems
and
evaluation
measures
used
in
the
experiments
the
novel
fite
technique
is
described
in
section
4
section
5
presents
the
methods
and
data
used
in
the
experiments
and
the
findings
section
6
contains
discussion
and
conclusions
2
trt
transformation
rules
and
background
research
the
idea
of
trt
and
the
automatic
method
to
generate
transformation
rules
is
described
in
pirkola
et
al
2003
a
transformation
rule
contains
source
and
target
language
characters
that
are
transformed
and
their
context
characters
in
addition
there
are
two
important
numerical
factors
associated
with
a
rule
frequency
and
confidence
factor
which
may
be
used
as
thresholds
to
select
the
most
common
and
reliable
rules
for
trt
frequency
refers
to
the
number
of
the
occurrences
of
the
rule
in
the
dictionary
data
that
was
used
for
the
rule
generation
confidence
factor
cf
is
defined
as
the
frequency
of
a
rule
divided
by
the
number
of
source
words
where
the
source
substring
of
the
rule
occurs
below
we
present
an
example
of
a
german-english
rule
ekt
ect
middle
191
214
89
25
the
rule
is
read
as
follows
the
letter
k
prior
to
t
and
after
e
is
transformed
into
the
letter
c
in
the
middle
of
words
with
the
confidence
factor
being
89
25
100
191/214
examples
of
target
word
forms
obtained
in
trt
are
shown
in
sections
4
2
and
4
3
in
pirkola
et
al
2003
we
studied
trt
in
combination
with
fuzzy
matching--
digram
and
trigram
matching
we
investigated
five
source
languages
with
english
being
a
target
language
for
all
the
source
languages
the
results
showed
that
for
finnish
german
and
spanish
the
combined
technique
performed
better
than
digrams
and
trigrams
alone
for
french
and
swedish
performance
changes
were
slight
in
toivonen
et
al
2005
we
studied
how
effective
trt
is
without
fuzzy
matching
we
found
that
translation
recall
was
high
when
low
frequency
and
confidence
factor
were
used
as
thresholds
to
select
the
rules
for
trt
however
at
low
confidence
factor
and
frequency
levels
translation
precision
was
low
the
fite-trt
technique
addresses
this
problem
and
as
we
will
show
in
this
article
it
achieves
both
high
recall
and
precision
the
transformation
rules
used
in
trt
in
this
study
were
generated
using
the
rule
generation
method
based
on
the
use
of
dictionary
data
described
in
pirkola
et
al
2003
the
dictionary
data
consisted
of
a
multilingual
medical
dictionary
by
andre
fairchild
http
//members
interfold
com/translator/
for
the
language
pairs
of
spanish-english
spanish-german
spanish-french
german-english
and
french-english
for
finnish-english
the
data
for
rule
generation
was
obtained
by
translating
1
a
list
of
finnish
medical
terms
into
english
using
a
medical
dictionary
by
kielikone
inc
and
2
a
list
of
finnish
terms
in
various
domains
into
english
using
kielikone
s
general-purpose
dictionary
thus
for
finnish-english
we
constructed
two
collections
the
second
collection
was
constructed
because
the
first
collection
missed
many
important
rules
table
i
shows
the
number
of
entries
and
the
average
number
of
translations
per
an
entry
for
each
dictionary
used
in
the
rule
generation
columns
2
and
3
table
i
also
shows
the
total
number
of
rules
in
the
generated
rule
collections
as
well
the
number
of
rules
at
or
above
the
thresholds
of
cf
4
0
and
frequency
2
applied
in
this
study
columns
4
and
5
we
applied
the
confidence
factor
and
frequency
thresholds
because
trt
may
give
very
large
translation
sets
and
at
the
present
stage
of
development
the
trt
program
is
not
efficient
enough
to
process
very
large
word
sets
due
to
the
efficiency
issues
we
also
applied
a
limit
of
40
word
forms
if
there
were
more
than
40
word
forms
in
a
translation
set
of
an
intermediate
language
the
source
word
was
retranslated
by
trt
with
a
confidence
factor
of
10
0
and
frequency
of
10
to
yield
a
smaller
translation
set
as
can
be
seen
in
table
i
each
rule
collection
contains
a
high
number
of
rules
which
suggests
that
the
rule
generation
method
effectively
captured
spelling
variation
between
the
language
pairs
3
research
problems
and
evaluation
measures
we
distinguish
between
two
kinds
of
words
in
a
language
with
respect
to
the
words
in
another
language
cross-lingual
spelling
variants
and
native
words
cross-lingual
spelling
variants
are
etymologically
related
words
and
therefore
similar
in
the
two
languages
differing
only
slightly
in
spelling
a
native
word
and
its
target
language
equivalent
are
not
related
to
each
other
morphologically
even
if
they
share
the
same
meaning
the
words
have
different
origins
and
etymologies
in
the
history
of
the
respective
languages
the
words
do
not
have
morphological
or
phonetic
resemblance--or
if
there
is
some
it
is
purely
accidental
as
examples
consider
the
english
words
computer
and
chemotherapy
and
their
finnish
equivalents
tietokone
and
kemoterapia
the
first
pair
computertietokone
do
not
have
morphological
or
phonetic
resemblance
computer
originating
from
latin
computare
to
calculate
and
tietokone
being
a
compound
of
knowledge
and
machine
the
finnish
words
tieto
and
kone
are
old
words
in
the
language
in
the
second
pair
chemotherapy-kemoterapia
both
words
originate
from
greek
chemeia
therapeia
and
albeit
having
been
modified
to
fit
the
style
of
their
present
languages
still
have
not
lost
their
morphological
or
phonetic
resemblance
trt
is
intended
to
translate
spelling
variants
and
fite
is
intended
to
identify
the
translation
equivalents
of
spelling
variants
and
to
indicate
the
native
source
language
words--the
source
words
that
cannot
be
correctly
translated
by
trt
using
test
word
sets
containing
both
types
of
words
we
examine
the
following
research
questions
r
in
the
case
of
spelling
variants
how
to
effectively
identify
the
correct
equivalent
of
a
source
word
among
the
many
word
forms
produced
by
trt
when
most
of
the
transformation
rules
available
for
a
language
pair
are
used
in
trt
r
how
to
reliably
identify
native
source
language
words
r
are
word
frequency
lists
mined
from
the
web
competitive
with
the
web
as
a
collection
of
documents
as
fite-trt
s
frequency
source
r
what
are
the
translation
recall
and
precision
and
indication
precision
see
the
definitions
below
of
the
proposed
fite-trt
method
r
what
is
the
contribution
of
each
step
in
the
fite-trt
process
to
its
overall
effectiveness
r
what
is
the
effectiveness
of
a
standard
clir
system
boosted
by
the
use
of
fite-trt
in
comparison
to
a
clir
system
augmented
with
trt
and
fuzzy
matching
oov
word
methods
and
in
comparison
to
dictionary-translationonly
clir
and
monolingual
baselines
the
effectiveness
of
fite-trt
was
evaluated
by
using
the
measures
of
translation
recall
translation
precision
and
indication
precision
for
spelling
variants
translation
recall
is
defined
as
the
proportion
of
source
words
for
which
fite
identifies
correct
equivalents
among
all
source
words
for
example
if
there
are
10
source
words
and
trt
gives
for
these
100
target
language
word
forms
among
which
there
are
correct
equivalents
for
8
source
words
then
translation
recall
is
8/10
80
translation
precision
is
defined
as
the
proportion
of
correct
equivalents
among
all
words
that
are
indicated
as
equivalents
for
example
if
fite
identifies
10
translation
equivalents
of
which
9
are
correct
equivalents
translation
precision
is
9/10
90
for
native
words
the
question
of
what
share
of
them
is
translated
by
trt
is
an
irrelevant
question
and
naturally
recall
is
not
measured
for
them
for
native
source
words
indication
precision
is
defined
as
the
proportion
of
words
correctly
indicated
to
be
untranslatable
by
trt
for
example
if
there
are
5
native
source
words
and
fite
indicates
that
none
of
these
translation
equivalents
are
contained
in
the
translation
sets
indication
precision
is
5/5
100
retrieval
effectiveness
was
evaluated
using
the
measures
of
mean
average
precision
map
and
precision
at
20
documents
map
is
a
standard
evaluation
measure
used
in
trec
http
//trec
nist
gov
and
it
refers
to
the
average
of
the
precision
values
obtained
after
each
relevant
document
is
retrieved
map
is
a
system-oriented
measure
while
precision
at
20
documents
is
important
from
the
practical
ir
standpoint
the
probability
of
a
searcher
scanning
further
down
a
ranked
result
list
decreases
as
s
he
scans
down
and
we
use
a
document
cutoff
value
of
20
as
a
rule
of
thumb
for
the
stopping
point
of
scan
4
the
fite
technique
4
1
frequency
data
fite
identifies
the
correct
translation
equivalents
among
the
trt
generated
word
forms
by
their
frequency
distribution
in
some
corpus
frequencies
for
fite
were
taken
from
the
web
and
word
frequency
lists
in
the
case
of
the
web
we
consider
document
frequency
df
and
in
the
case
of
frequency
lists
word
frequency
wf
df
statistics
were
collected
using
the
altavista
search
engine
and
its
language
selection
feature
a
research
assistant
fed
the
word
forms
into
the
search
engine
which
reported
for
each
word
form
the
number
of
documents
containing
the
word
form
the
word
frequency
lists
were
mined
from
the
web
using
a
web
mining
technique
which
is
described
next
in
the
first
step
of
the
web
mining
process
a
query
script
based
on
the
use
of
text-based
web
browser
lynx
was
run
to
fetch
medical
and
biological
documents
in
a
desired
language
from
the
google
search
engine
the
query
script
described
in
zhang
and
vines
2004
was
modified
for
this
purpose
we
used
the
following
parameters
and
parameter
values
in
the
script
language
english/finnish/german/spanish
the
number
of
documents
to
fetch
700
query
keys
the
words
medicine
biology
and
disease
conjuncted
by
the
and-operator
and
the
corresponding
words
in
finnish
german
and
spanish
the
use
of
these
keys
directed
the
actual
web
mining
towards
medical
and
biological
sub-webs
in
the
second
step
urls
were
extracted
from
the
fetched
documents
and
were
saved
in
a
file
in
the
third
step
the
url
file
was
cleaned
by
removing
duplicates
so
that
only
urls
with
unique
domain
names
were
kept
in
the
file
the
url
file
served
as
an
input
for
the
fourth
step
the
actual
web
mining
stage
where
documents
were
downloaded
from
each
web
site
represented
in
the
url
file
using
a
wget
program
http
//www
gnu
org/software/wget/
wget
s
parameter
directory
depth
was
set
at
3--on
each
web
site
documents
at
directory
depths
1
3
were
downloaded
in
the
fifth
step
of
the
process
all
downloaded
documents
were
combined
into
one
large
file
in
the
sixth
step
word
frequency
lists
were
constructed
from
the
combined
document
file
the
number
of
documents
downloaded
from
the
web
varied
depending
on
the
language
for
example
for
german
35
000
documents
were
downloaded
the
total
size
of
these
documents
was
2
26
gb
the
numbers
of
unique
words
contained
in
the
frequency
lists
are
as
follows
english
762
000
words
finnish
886
000
words
german
470
000
words
spanish
386
000
words
intermediate
language
french
was
small
it
was
not
considered
in
the
frequency
list
experiments
and
we
did
not
construct
a
word
frequency
list
for
french
its
minor
contribution
was
probably
due
to
the
fact
that
it
was
used
as
the
second
intermediate
language
rather
than
due
to
its
linguistic
features
in
statistical
mt
the
choice
between
translation
alternatives
depends
on
the
translation
probabilities
of
the
alternatives
and
their
context
al-onaizan
et
al
1999
brown
et
al
1990
translation
probabilities
are
computed
on
a
basis
of
aligned
corpora
in
contrast
to
this
the
source
and
target
language
corpora
used
by
fite
are
independent
of
each
other
in
statistical
mt
bigram
and
trigram
language
models
are
typically
used
to
capture
the
context
fite
is
based
on
a
unigram
language
model
no
context
dependence
of
translations
is
assumed
4
2
frequency
pattern
in
order
to
avoid
several
long
function
definitions
not
precisely
in
the
focus
of
our
article
we
introduce
some
notational
conventions
used
in
the
definition
of
the
fite
method
notational
convention
1
let
sl
be
a
source
language
and
tl
a
target
language
and
sw
be
some
source
language
word
in
the
source
language
collection
s
we
denote
this
by
sw
sl
we
denote
the
word
set
produced
by
our
trt
translation
by
trt
s
l-
t
l
sw
using
the
transformation
rules
for
sl-
tl
translation
the
result
is
a
set
its
elements
tw
hold
the
relationship
tw
trt
s
l-
t
l
sw
if
the
trt
translation
is
performed
using
a
strict
confidence
factor
10
and
a
strict
rule
frequency
10
see
section
2
we
denote
this
by
trt
s
l-
t
l
strict
sw
notational
convention
2
let
sw
be
some
word
in
source
language
sl
sw
sl
we
denote
its
document
frequency
in
the
source
language
collection
s
by
df
s
sw
note
that
if
sw
does
not
appear
in
any
documents
of
s
then
df
s
sw
0
if
s
is
a
source
language
wordlist
containing
word
frequencies
we
denote
the
frequency
of
sw
in
s
by
wf
s
sw
notational
convention
3
let
tw
be
some
word
of
the
target
language
tl
in
the
target
language
document
collection
t
tw
tl
we
denote
its
document
frequency
in
t
by
dft
tw
it
refers
to
the
frequency
of
target
language
documents
that
contain
the
word
tw
note
that
if
tw
does
not
appear
in
any
documents
of
t
then
dft
tw
0
if
t
is
a
target
language
wordlist
containing
word
frequencies
we
denote
the
frequency
of
tw
in
t
by
wft
tw
notational
convention
4
let
sw
be
some
source
language
sl
word
sw
sl
tl
a
target
language
tws
trt
s
l-
t
l
sw
the
word
set
produced
by
our
trt
translation
and
t
a
target
language
document
collection
or
word
list
the
frequency-ranked
list
of
words
of
tws
in
t
is
denoted
by
r
trt-frank
tws
t
table
ii
is
an
example
of
such
a
list
with
the
frequency
data
added
for
a
given
source
language
word
sw
we
obtain
this
list
by
trt-frank
trt
s
l-
t
l
sw
t
the
elements
of
this
list
are
denoted
by
the
usual
notation
for
example
trt-frank
tws
t
3
gives
its
third
component
as
an
example
in
the
case
of
table
ii
trt-frank
trtspa-
eng
biosintesis
engweb
3
biosyntesis
its
frequency
is
dfengweb
trt-frank
trtspa-
eng
biosintesis
engweb
3
dfengweb
biosyntesis
634
the
core
of
fite
is
that
except
for
the
translation
equivalents
the
word
forms
yielded
by
trt
are
malformed
rather
than
real
words
or
they
are
rare
words
for
example
foreign
language
words
in
the
target
language
text
the
equivalents
belong
to
a
language
s
basic
lexicon
and
are
much
more
common
in
the
language
than
the
other
word
forms
this
regular
frequency
pattern
allows
the
identification
of
the
equivalents
the
example
in
table
ii
shows
the
document
frequency
pattern
associated
with
the
word
forms
obtained
by
trt
for
the
spanish
word
biosintesis
in
spanish-english
trt
in
the
english
sub-web
the
word
forms
are
sorted
by
document
frequency
by
trt-frank
trtspa-
eng
biosintesis
engweb
we
can
see
that
the
df
of
biosynthesis
the
equivalent
of
biosintesis
is
remarkably
higher
than
the
dfs
of
the
other
the
word
forms
this
type
of
frequency
distribution
is
very
common
for
word
forms
within
a
translation
set
of
trt
given
a
target
word
form
ranking
r
trt-frank
tws
t
the
magnitude
of
difference
between
the
document
frequency
of
the
first
word
form
dft
r
1
and
the
document
frequency
of
the
second
word
form
dft
r
2
or
the
frequencies
between
r
2
and
r
3
see
section
4
5
forms
the
basis
of
the
equivalent
identification
we
used
the
coefficient
value
the
magnitude
of
difference
of
10
for
the
identification
of
equivalents
both
for
web
and
word
frequency
lists
the
same
pattern
holds
for
the
web
and
word
frequency
lists
with
the
main
differences
being
in
that
in
the
case
of
frequency
lists
word
frequencies
instead
of
document
frequencies
are
considered
and
in
that
web
gives
more
malformed
words
than
the
frequency
lists
the
following
definition
of
the
function
freqpattern-ok
checks
whether
the
frequencies
of
two
target
language
words
twi
and
tw
j
have
the
required
pattern
definition
1
let
twi
and
tw
j
be
two
candidate
word
forms
in
the
target
language
sub-
web
document
collection
or
word
frequency
list
t
as
given
by
trt
let
dft
twi
and
dft
tw
j
be
their
frequencies
in
t
let
be
a
corpus
dependent
normalizing
factor
1
the
boolean
function
freq-pattern-ok
gives
the
value
true
if
the
frequency
of
twi
in
t
is
at
least
times
the
frequency
of
tw
j
in
t
freq-pattern-ok
twi
tw
j
t
true
if
dft
twi
dft
tw
j
false
otherwise
typically
the
function
freq-pattern-ok
is
applied
on
two
consecutive
words
in
a
frequency-ranked
order
for
example
freq-pattern-ok
biosynthesis
biosintesis
10
engweb
yields
the
value
true
table
ii
in
the
tests
the
coefficient
was
experimentally
set
at
10
using
the
training
data
section
5
1
1
4
3
relative
frequency
there
are
situations
where
the
highest
df
wf
is
possessed
by
a
word
that
is
not
the
correct
equivalent
for
example
the
source
word
may
occur
frequently
in
a
target
language
collection
if
trt
fails
to
translate
the
source
word
it
may
appear
at
the
first
position
in
a
translation
set
a
source
word
is
always
included
in
trt
s
translation
set
because
source
and
target
language
words
may
be
identical
also
in
the
case
of
web
as
a
document
collection
there
are
mixed
language
pages
some
of
which
a
search
engine
may
consider
target
language
pages
which
wrongly
increases
the
target
df
of
a
source
word
found
in
the
mixed
language
pages
trt
may
also
accidentally
give
high
df
words
that
are
not
correct
equivalents
as
a
solution
for
this
problem
we
compute
relative
document
frequency
rel-df
and
relative
word
frequency
rel-wf
defined
as
follows
definition
2
let
sw
be
a
source
language
word
in
the
source
language
collection
s
and
tw
a
target
language
word
form
in
the
target
language
sub-
web
document
collection
t
as
given
by
trt
let
df
s
sw
be
the
frequency
of
sw
in
s
and
dft
tw
the
frequency
of
the
target
language
word
tw
in
t
let
be
a
corpus
dependent
normalizing
factor
0
the
function
rel-df
gives
the
relative
document
frequency
for
tw
in
t
definition
3
let
sw
be
a
source
language
word
in
the
source
language
word
list
s
and
tw
a
target
language
word
form
in
the
target
language
word
list
t
as
given
by
trt
let
wf
s
sw
be
the
frequency
of
sw
in
s
and
wft
tw
the
frequency
of
the
target
language
word
tw
in
t
let
be
a
corpus
dependent
normalizing
factor
0
the
function
rel-wf
gives
the
relative
word
frequency
for
tw
in
t
rel-wf
tw
sw
t
s
wft
tw
/
xwf
s
sw
the
coefficient
is
a
corpus
dependent
normalizing
factor
it
is
assigned
such
a
value
that
rel-df
and
rel-wf
1
indicate
that
the
target
word
form
is
an
equivalent
and
rel-df
and
rel-wf
1
indicate
the
equivalent
is
not
found
in
the
translation
set
the
coefficient
values
reflect
the
relative
sizes
of
the
sub-webs/word
frequency
lists
in
relation
to
each
other
in
our
case
2
was
used
in
all
test
conditions
the
value
2
was
determined
experimentally
the
values
of
from
1
to
2
are
appropriate
for
the
conditions
where
the
target
corpus
is
much
larger
than
the
source
corpus
which
was
the
case
in
our
experiments
the
finnish
word
frequency
list
contains
more
words
than
the
english
list
section
4
1
however
the
sum
frequency
over
all
words
is
substantially
higher
in
the
english
than
finnish
list
this
allows
the
use
of
2
in
the
rel-wf
formula
also
for
finnish-english
the
example
in
table
iii
illustrates
the
case
where
the
word
with
the
highest
df
is
not
the
correct
equivalent
the
translation
set
contains
the
word
forms
and
the
associated
frequencies
of
english
web
pages
for
a
spanish
source
word
fraccionamiento
a
typical
frequency
pattern
is
found
however
fraccionamiento
the
word
with
the
highest
df
is
the
spanish
source
word
not
translated
into
english
its
df
in
the
spanish
portion
of
web
is
416
000
it
is
not
accepted
as
an
equivalent
since
rel-df
fraccionamiento
fraccionamiento
2
engweb
spaweb
1
we
considered
the
two
highest
ranked
word
forms
and
naturally
also
for
the
second
form
fraccionamento
rel-df
1
4
4
length
factor
cross-lingual
spelling
variants
are
close
to
each
other
in
word
length
a
great
difference
between
the
length
of
a
target
word
form
and
the
source
word
is
an
indication
of
a
wrong
equivalent
the
length
factor
is
taken
into
account
as
fite
identifies
equivalents
the
length
criteria
for
accepting
an
equivalent
candidate
as
an
equivalent
are
shown
in
table
iv
it
can
be
seen
for
example
that
when
a
source
word
contains
7
characters
the
target
word
form
has
to
have
from
5
to
9
characters
in
order
to
be
accepted
as
an
equivalent
definition
4
let
sw
be
a
source
language
word
and
len
sw
its
length
in
characters
likewise
let
tw
be
a
target
language
word
and
len
tw
its
length
in
characters
the
boolean
function
tw-len-ok
gives
the
value
true
if
the
length
of
the
target
word
tw
is
within
the
range
defined
in
table
iv
tw-len-ok
tw
sw
true
if
4
len
tw
7
and
len
sw
5
true
if
5
len
tw
8
and
len
sw
6
true
if
len
tw
-
len
sw
2
and
7
len
sw
10
true
if
len
tw
-
len
sw
3
and
len
sw
10
false
otherwise
4
5
the
application
of
fite
in
the
empirical
experiments
the
source
test
words
section
5
1
1
were
translated
into
english
by
the
trt
translation
program
the
applied
thresholds
were
described
in
section
2
the
equivalents
were
searched
for
from
the
translation
sets
using
the
fite
technique
as
described
in
sections
4
2
4
4
the
main
criteria
of
equivalent
identification
of
fite
are
the
following
1
the
frequency
patterns
of
the
top
word
forms
tested
by
the
function
freq-pattern-ok
2
the
relative
frequency
criterion
tested
by
the
rel-df/rel-wf
functions
and
3
length
criterion
tested
by
the
function
tw-len-ok
the
basic
idea
is
to
apply
the
criteria
1
3
in
three
steps
a
c
first
in
step
a
direct
translation
is
tried
and
then
in
steps
b
and
c
the
pivot
language
translations
one
after
the
other
the
criteria
are
first
applied
to
the
highest
ranking
target
word
candidate
as
given
by
the
function
trt-frank
if
these
steps
do
not
yield
a
solution
then
basically
the
same
steps
are
repeated
with
the
second
highest
ranking
target
word
candidate
this
process
is
specified
as
algorithm
find-equivalent
which
is
presented
in
the
appendix
the
algorithm
is
for
the
case
of
word
frequency
lists
s
and
t
for
the
source
and
target
languages
in
the
case
of
web
document
collections
the
word
frequency
lists
are
replaced
by
a
function
that
gives
the
web
document
frequency
for
a
given
source
word
the
trt
rule
bases
for
the
source
pivot
and
target
languages
as
described
above
need
to
be
available
but
are
not
precisely
defined
see
notational
conventions
in
section
4
2
we
use
the
notations
and
functions
of
preceding
sections
in
the
definition
of
the
algorithm
the
algorithm
find-equivalent
first
tries
the
first
candidates
produced
by
the
trt
direct
or
pivoted
processes
it
calls
the
procedure
direct-trans
to
produce
the
frequency-based
ranking
of
the
direct
trt
candidates
the
procedure
generates
them
as
the
list
r
and
then
the
first
component
of
r
is
tested
for
the
criteria
1
3
by
the
procedure
test-cand
if
the
first
component
passes
the
test
it
is
given
as
the
equivalent
if
not
the
algorithm
find-equivalent
then
uses
the
first
and
finally
the
second
pivot
language
translation
given
by
the
procedure
pivot-trans
which
first
checks
the
number
of
pivot
language
word
forms
obtained
the
trt
rules
are
used
liberally
the
thresholds
of
cf
4
0
and
frequency
2
are
used
see
section
2
if
there
are
at
most
40
candidates
and
otherwise
strictly
the
thresholds
of
cf
10
0
and
frequency
10
are
used
either
way
produces
a
target
language
word
candidate
list
tws
which
then
is
ranked
by
frequency
and
the
first
component
tested
for
the
criteria
1
3
if
the
first
pass
focusing
on
the
first-ranked
components
is
not
successful
then
the
algorithm
find-equivalent
tries
the
second
equivalent
candidate
produced
by
the
trt
processes
the
first
component
is
still
selected
as
the
equivalent
if
the
three
criteria
are
fulfilled
as
follows
the
second
component
passes
the
frequency
pattern
and
relative
frequency
criteria
and
the
first
one
the
length
criterion
the
second
word
form
is
selected
as
the
equivalent
only
if
the
first
word
form
does
not
meet
the
length
criterion
and
the
second
form
meets
all
the
three
criteria
otherwise
the
source
word
is
indicated
to
be
untranslatable
by
means
of
trt--the
string
nil
is
returned
we
found
empirically
the
need
to
compare
the
second
candidate
word
form
to
the
third
form
to
find
out
if
there
are
more
than
one
correct
target
language
words
high
frequency
word
forms
in
the
translation
set
if
there
are
exactly
two
acceptable
words
the
first
word
rather
than
the
second
is
selected
as
the
equivalent
based
on
our
observations
that
in
these
cases
the
equivalent
tends
to
be
at
the
first
position
the
second
word
form
is
accepted
as
the
equivalent
only
if
the
first
form
does
not
meet
the
length
criterion
in
the
actual
experiments
described
in
section
5
the
algorithm
findequivalent
was
applied/modified
as
follows
in
the
case
of
frequency
lists
the
second
pivot
language
french
was
not
considered
finnish-english
experiments
differed
from
the
spanish-english
experiments
in
that
there
were
two
direct
translation
routes
thanks
to
two
finnish-english
rule
collections
the
order
of
the
use
of
the
translation
routes
for
finnish-english
was
as
follows
finnish-english/collection
1
finnish-english/collection
2
finnish-germanenglish
and
finnish-french-english
all
the
source
words
were
in
base
form
and
only
base
form
equivalents
were
accepted
as
correct
equivalents
thus
equivalents
in
plural
form
and
the
derivatives
of
the
actual
equivalents
were
not
accepted
as
correct
equivalents
this
is
because
our
aim
is
to
develop
a
dictionary-like
rule-based
translation
method
which
indicates
the
precise
equivalents
of
source
words
we
conclude
this
section
by
summarizing
in
figure
1
the
fite-trt
process
the
left
side
of
the
figure
describes
the
production
of
transformation
rules
and
the
translation
of
oov
words
by
means
of
trt
the
fite
technique--the
focus
of
the
present
article--is
the
grey
shaded
area
fite-trt
effectiveness
was
evaluated
using
the
measures
of
translation
recall
and
precision
and
indication
precision
5
experiments
and
findings
in
this
section
we
present
the
methods
and
data
used
in
the
fite-trt
and
clir
effectiveness
experiments
and
the
experimental
results
subsection
5
1
presents
the
training
and
test
words
describes
how
the
words
were
translated
by
means
of
trt
and
presents
the
findings
of
the
fite-trt
effectiveness
experiments
the
clir
experiments
are
dealt
with
in
subsection
5
2
5
1
fite-trt
effectiveness
5
1
1
training
and
test
word
sets
and
translation
by
trt
fite-trt
is
intended
to
handle
both
spelling
variants
and
native
source
language
words
the
training
and
test
word
sets
contained
both
types
of
words
next
we
describe
the
selection
of
training
and
test
words
then
we
characterize
quantitatively
the
difference
between
cross-lingual
spelling
variants
and
native
words
we
used
a
training
word
set
for
the
development
of
the
fite
technique
the
set
contained
the
title
words
n
75
of
the
spanish
clef
topics
numbered
91
to
109
in
addition
to
native
spanish
words
the
titles
contain
spanish-english
spelling
variants
native
english
words
and
english
acronyms
the
effectiveness
of
fite-trt
was
evaluated
using
four
sets
of
test
words
for
each
source
language
word
set
there
was
a
corresponding
english
word
set
that
contained
the
equivalents
of
the
source
words
for
the
first
two
test
word
sets
a
list
of
english
biological
and
medical
terms
was
gathered
manually
from
the
index
of
clef
s
peters
2005
la
times
collection
the
english
terms
were
translated
into
spanish
and
finnish
by
means
of
translation
dictionaries
and
monolingual
spanish
and
finnish
medical
dictionaries
from
these
words
we
selected
spanish-english
and
finnish-english
spelling
variants
for
our
tests
the
identification
of
spelling
variants
was
done
based
on
the
similarity
of
the
spanish-english
and
finnish-english
word
pairs
judged
by
a
researcher
the
similarity
feature
used
as
a
selection
criterion
is
discussed
in
section
3
and
later
in
this
section
the
spanish
terms
formed
the
first
and
the
finnish
terms
the
second
test
word
set
both
contained
the
same
terms
n
89
albeit
in
different
languages
these
terms
are
called
bio-terms
for
the
third
and
fourth
test
word
sets
trec
genomics
track
2004
topics
in
spanish
and
finnish
section
5
2
1
were
translated
into
english
using
the
utaclir
system
an
automatic
dictionary-based
query
translation
and
construction
system
developed
in
the
information
retrieval
laboratory
at
the
university
of
tampere
hedlund
et
al
2004
the
oov
keys
of
the
utaclir
runs
were
used
as
test
words
among
the
oov
keys
there
were
in
addition
to
spelling
variants
native
spanish/finnish
words
as
well
as
english
words
and
english
acronyms
the
spanish
word
set
contained
98
and
the
finnish
set
53
oov
keys
after
the
removal
of
short
words
the
difference
in
the
number
of
the
oov
keys
reflects
the
different
sizes
of
utaclir
s
spanish-english
and
finnish-english
dictionaries
these
test
key
sets
are
here
called
oov-utaclir-spa-eng
and
oov-utaclir-fin-eng
words
containing
four
or
less
letters
were
not
translated
by
trt
this
restriction
was
set
because
the
short
words
were
english
acronyms
and
they
need
not
be
translated
generally
acronyms
cannot
be
translated
by
means
of
trt
which
only
handles
spelling
variants
on
the
other
hand
cross-lingual
spelling
variants
are
not
very
short
words
within
the
four
test
word
sets
there
were
two
short
4-letter
spelling
variants
which
were
removed
from
the
sets
according
to
the
short
word
restriction
the
total
number
of
unique
source
words
translated
by
trt
was
89
98
spanish
89
53
finnish
329
words
to
characterize
quantitatively
the
difference
between
cross-lingual
spelling
variants
and
native
words
we
computed
for
both
types
of
source
language
test
words
their
degree
of
similarity
with
respect
to
their
english
equivalents
using
a
simple
measure
of
longest
common
subsequence
divided
by
the
mean
length
of
source
and
target
words
lcs/mwl
lcs
is
defined
as
the
length
of
the
longest
subsequence
of
characters
shared
by
two
words
the
closer
to
1
0
lcs/
mwl
is
the
more
similar
the
words
are
as
an
example
for
the
spanish
bioterm
omnivoro
lcs/mwl
7/
8
8
/2
0
875
with
respect
to
the
english
equivalent
omnivore
for
the
native
spanish
oov
word
vinculante
lcs/mwl
0
353
with
respect
to
the
equivalent
binding
table
v
shows
the
results
of
lcs/mwl
calculations
from
the
viewpoint
of
trt
the
target
language
english
words
as
source
words
are
similar
cases
as
spelling
variants
and
they
were
thus
regarded
as
spelling
variants
in
the
sets
oov-utaclir-spa-eng
and
oov-utaclir-fin-eng
as
mentioned
in
section
4
3
a
source
word
is
included
in
trt
s
translation
set
because
source
and
target
language
words
may
be
identical
in
cases
where
native
spanish
and
finnish
words
had
multiple
meanings
in
english
the
meaning
that
appeared
in
the
genomics
track
topic
was
selected
for
lcs/mwl
calculation
both
the
spanish-english
and
finnish-english
oov
word
sets
contained
five
native
words
it
can
be
seen
in
table
v
that
for
bio-terms
and
spelling
variant
oov
words
the
average
lcs/mwls
are
0
839
and
0
911
spanish-english
and
0
784
and
0
853
finnish-english
for
native
spanish
and
finnish
words
average
lcs/mwls
are
much
lower
0
339
for
spanish
and
0
361
for
finnish
low
standard
deviation
figures
show
that
the
lcs/mwl
values
are
clustered
around
the
average
values
in
the
experiments
the
source
words
were
translated
by
the
trt
program
through
direct
and
indirect
translation
routes
into
english
using
the
confidence
factor
and
rule
frequency
thresholds
section
2
the
equivalents
of
source
words
were
identified
from
trt
s
english
translation
sets
by
means
of
the
fite
technique
as
described
in
section
4
5
like
target
language
translation
sets
the
intermediate
translation
sets
are
often
large
and
only
five
top
german
and
french
forms
in
a
frequency-ranked
translation
set
were
further
translated
into
english
different
english
translation
sets
corresponding
to
the
same
finnish/spanish
source
word
were
combined
5
1
2
findings
table
vi
reports
the
translation
recall
and
precision
results
for
bio-terms
and
table
vii
the
contribution
of
different
translation
routes
to
the
recall
for
bio-terms
table
viii
presents
the
translation
recall
and
precision
and
indication
precision
results
for
oov-utaclir-spa-eng
and
oovutaclir-fin-eng
words
table
vi
shows
that
spanish-english
fite-trt
reaches
91
0
recall
in
the
case
of
web
document
frequencies
and
82
0
recall
in
the
case
of
word
frequency
lists
finnish-english
fite-trt
reaches
71
9
web
and
67
4
frequency
lists
recall
while
spanish-english
fite-trt
achieves
higher
recall
precision
is
approximately
the
same
and
it
is
remarkably
high
for
both
language
pairs
97
0
98
8
the
same
trends
hold
for
the
oov
words
table
viii
for
spanish-english
recall
is
higher
than
for
finnish-english
and
for
both
language
pairs
precision
is
very
high
95
0
97
6
table
vii
shows
that
the
contribution
of
direct
translation
to
the
recall
is
substantial
for
both
language
pairs
for
finnish-english
fite-trt
the
contribution
of
the
second
direct
route
collection
2
to
the
recall
is
high
indirect
translation
adds
recall
only
for
spanish-english
in
the
case
of
web
as
a
frequency
corpus
the
first
pivot
language
adds
recall
by
6
7
while
the
second
one
adds
recall
only
by
2
2
for
the
native
words
indication
precision
is
100
in
all
test
situations
table
viii
there
were
only
10
native
spanish
and
finnish
words
in
all
however
the
results
are
reasonable
since
the
cases
where
trt
accidentally
gives
correct
words
are
not
common
5
2
clir
effectiveness
5
2
1
methods
and
data
fite-trt
was
applied
as
part
of
an
actual
clir
system
as
test
data
we
used
trec
genomics
track
2004
data
hersh
et
al
2005
the
data
consisted
of
50
test
topics
a
subset
of
the
medline
collection
containing
around
4
5
million
documents
and
relevance
judgments
queries
were
formulated
on
the
basis
of
the
title
and
need
fields
of
the
topics
the
data
are
well
suited
for
investigating
fite-trt
since
the
topics
are
rich
in
technical
mainly
biological
and
medical
terms
the
topics
were
translated
manually
into
spanish
and
finnish
by
a
researcher
the
final
spanish
topics
were
formulated
by
a
knowledgeable
spanish
speaker
a
university
teacher
of
spanish
the
researcher
is
a
native
finnish
speaker
and
has
expertise
in
medical
informatics
the
test
system
was
the
inquery
retrieval
system
allan
et
al
2000
larkey
and
connell
2005
inquery
is
a
probabilistic
retrieval
system
based
on
the
bayesian
inference
network
model
queries
can
be
presented
as
a
bag
of
word
queries
or
they
can
be
structured
using
a
variety
of
query
operators
in
this
study
the
translated
queries
were
structured
using
inquery
s
syn-operator
as
described
in
pirkola
1998
and
sperer
and
oard
2000
the
keys
within
the
syn-operator
are
treated
as
instances
of
one
key
the
spanish
and
finnish
topics
were
translated
back
into
english
using
the
utaclir
system
and
the
queries
were
then
run
on
the
genomics
track
test
collection
utaclir
s
output
without
any
oov
word
technique
provides
crosslingual
baseline
for
the
fite-trt
queries
for
which
utaclir
s
oov
words
were
translated
by
means
of
trt
and
equivalents
were
identified
using
fite
the
original
english
queries
were
also
run
to
show
the
performance
level
of
the
translated
queries
we
also
compared
the
effectiveness
of
fite-trt
queries
to
the
effectiveness
of
plain
trt
and
skipgram
queries
plain
trt
is
the
trt
part
of
fite-trt
and
for
plain
trt
queries
utaclir
s
oov
words
were
translated
by
trt
with
cf
4
and
frequency
2
all
translations
of
a
source
word
were
included
in
a
query
and
were
wrapped
in
the
syn-operator
in
skipgram
queries
the
oov
words
were
translated
using
a
skipgram
fuzzy
matching
technique
keskustalo
et
al
2003
this
string
matching
technique
is
a
generalization
of
the
n-gram
technique
where
words
are
split
into
digrams
on
the
basis
of
both
consecutive
as
well
as
nonconsecutive
characters
see
below
in
these
comparison
experiments
only
direct
translation/matching
was
examined
since
it
is
not
sensible
to
study
indirect
fuzzy
matching
also
indirect
trt
without
frequency-based
selection
of
intermediate
word
forms
for
further
translation
would
give
very
long
queries
that
would
be
hard
to
manage
the
skipgram
fuzzy
matching
technique
constructs
digrams
of
both
consecutive
and
nonconsecutive
characters
of
words
keskustalo
et
al
2003
the
generated
digrams
are
put
into
comparable
categories
based
on
the
number
of
skipped
characters
as
digrams
are
constructed
the
character
combination
index
cci
indicates
the
number
of
skipped
characters
as
well
as
the
comparable
categories
here
we
used
cci
0
1
2
this
means
that
digrams
formed
of
consecutive
characters
form
one
comparable
category
and
digrams
with
one
and
two
skipped
characters
the
other
cci
0
1
2
was
very
effective
in
the
study
conducted
by
keskustalo
et
al
2003
who
explored
the
same
general
problem
as
we
do
in
this
article
the
identification
of
translation
equivalents
of
cross-lingual
spelling
variants
skipgrams
with
cci
0
1
2
outperformed
conventional
digrams
formed
of
consecutive
characters
of
words
in
the
skipgram
experiments
each
oov
word
was
matched
against
each
string
in
the
index
of
the
trec
collection
two
types
of
queries
were
constructed
for
each
oov
word
1
two
best
matches
and
2
five
best
matches
were
selected
for
a
query
in
the
query
the
best
matches
were
linked
to
each
other
with
inquery
s
syn-operator
all
inflected
query
words
were
rendered
into
base
form
for
a
dictionary
lookup
for
finnish
utaclir
s
morphological
analyzer
gave
base
forms
for
most
inflected
words
and
those
that
the
analyzer
was
not
able
to
handle
were
lemmatized
manually
all
spanish
inflected
words
were
lemmatized
manually
manual
lemmatization
of
the
inflected
keys
was
necessary
because
at
this
stage
of
development
trt
only
translates
lemmas
thus
the
results
show
clir
performance
when
a
searcher
gives
query
keys
in
base
form
the
results
were
tested
for
significance
by
the
wilcoxon
signed
rank
test
conover
1980
the
wilcoxon
test
takes
into
account
both
the
direction
and
magnitude
of
change
between
each
comparable
result
of
a
query
in
summary
we
run
the
following
queries
in
spanish-english
and
finnishenglish
clir
experiments
r
original
english
queries
r
utaclir
baseline
with
oov
words
passed
through
unchanged
r
utaclir
fite-trt
web
r
utaclir
fite-trt
frequency
lists
r
utaclir
trt
r
utaclir
skipgrams
with
two
best
matches
r
utaclir
skipgrams
with
five
best
matches
5
2
2
findings
the
results
of
the
retrieval
experiments
are
presented
in
tables
ix
xi
the
statistical
significance
of
the
test
queries
was
tested
against
the
utaclir
baseline
tables
ix
x
and
against
utaclir
fite-trt/web
table
xi
in
the
tables
the
statistical
significance
is
indicated
by
asterisks
as
expected
the
queries
where
oov
keys
are
translated
by
fite-trt
perform
better
than
the
baseline
queries
where
oov
keys
are
retained
untranslatable
tables
ix
and
x
in
spanish-english
clir
map
improvement
percentages
are
40
3
web
and
35
2
frequency
lists
precision
at
20
documents
is
improved
by
50
5
web
and
49
2
frequency
lists
also
finnishenglish
fite-trt
queries
remarkably
outperform
the
clir
baseline
although
performance
improvements
are
smaller
than
in
the
case
of
spanish-english
table
x
all
spanish-english
and
finnish-english
results
are
statistically
significant
at
p
0
001
these
findings
are
in
agreement
with
the
high
number
of
oov
keys
in
the
utaclir
runs
and
fite-trt
s
high
translation
recall
and
precision
it
should
be
noted
that
some
oov
words
may
only
be
marginally
topical
in
which
case
correct
fite-trt
identification
may
result
in
a
performance
drop
therefore
fite-trt
performance
does
not
always
correlate
linearly
with
clir
fite-trt
performance
spanish-english
fite-trt
queries
perform
better
than
finnish-english
fite-trt
queries
which
were
much
longer
and
more
ambiguous
than
spanishenglish
queries
due
to
the
larger
coverage
of
utaclir
s
finnish-english
dictionary
the
higher
degree
of
translation
ambiguity
and
fite-trt
s
lower
performance
resulted
in
lower
clir
performance
the
higher
performance
of
english
queries
with
respect
to
the
performance
of
spanish-english
and
in
particular
finnish-english
queries
is
mostly
caused
by
translation
ambiguity
table
xi
shows
the
performance
of
fite-trt
trt
and
skipgram
queries
the
results
are
ranked
based
on
map
values
it
can
be
seen
that
for
both
language
pairs
the
best
oov
word
method
is
fite-trt
with
web
document
frequencies
however
it
shows
significantly
better
results
only
against
the
cases
of
spanish-english/skipgram/2
and
finnish-english/trt
and
the
results
are
significant
only
at
p
0
05
in
the
latter
case
the
difference
in
map
is
small
0
2447
0
2393
but
systematic
and
hence
significant
in
comparison
to
utaclir
baselines
tables
ix
and
x
all
queries
perform
well
it
was
expected
that
plain
trt
queries
yield
good
results
since
trt
with
cf
4
and
frequency
2
very
often
gives
a
source
word
s
correct
equivalent
while
the
other
translations
typically
are
malformed
word
forms
not
occurring
in
the
database
index
and
having
no
effects
whatsoever
on
retrieval
results
in
many
applications
outside
cross-language
document
retrieval
it
is
however
important
to
avoid
the
ambiguity
of
trt
and
obtain
one
reliable
translation
as
given
by
fite-trt
6
discussion
and
conclusions
in
this
study
we
first
examined
the
following
two
basic
questions
regarding
spelling
variants
how
to
effectively
identify
the
correct
equivalent
of
a
source
word
among
the
many
word
forms
produced
by
trt
when
most
of
the
transformation
rules
available
for
a
language
pair
are
used
in
trt
how
to
reliably
identify
native
source
language
words
source
words
that
cannot
be
correctly
translated
by
trt
we
devised
the
fite-trt
technique--a
novel
oov
word
translation
technique
its
effectiveness
was
tested
for
spanish-english
and
finnish-english
spelling
variants
and
actual
oov
words
in
the
domains
of
biology
and
medicine
here
the
research
questions
were
as
follows
what
are
the
translation
recall
and
precision
and
indication
precision
of
the
proposed
fite-trt
method
are
word
frequency
lists
mined
from
the
web
competitive
with
the
web
as
a
collection
of
documents
as
fite-trt
s
frequency
source
what
is
the
contribution
of
each
step
direct
and
indirect
translation
routes
in
the
fite-trt
process
to
its
overall
effectiveness
we
found
that
depending
on
the
source
language
and
frequency
source
fitetrt
may
achieve
high
translation
recall
when
equivalents
were
identified
on
the
basis
of
web
document
frequencies
spanish-english
fite-trt
achieved
89
2
91
0
recall
for
finnish-english
and
for
frequency
lists
mined
from
the
web
recall
was
lower
but
still
substantial
the
lowest
recall
67
4
was
obtained
for
finnish-english/frequency
lists
the
results
indicated
that
fitetrt
achieves
high
precision
fite
indicates
precisely
the
equivalents
of
source
words
as
well
as
the
native
words
for
spanish-english
and
finnish-english
test
words
translation
precision
was
95
0
98
8
all
native
oov
words
were
correctly
indicated
to
be
untranslatable
by
trt
the
test
requests
only
contained
10
native
oov
words
the
results
reported
here
need
to
be
corroborated
using
a
larger
set
of
native
words
the
contribution
of
direct
translation
to
the
overall
recall
was
substantial
for
spanish-english
bio-terms
direct
translation
achieved
82
0
recall
while
the
overall
recall
was
91
0
we
expected
that
finnish-english
fite-trt
through
pivot
languages
would
have
compensated
failures
of
direct
translation
but
that
did
not
happen
indirect
translation
did
not
help
at
all
these
findings
suggest
that
that
the
costs
of
indirect
translation
against
its
benefits
are
high
because
a
pivot
language
increases
fite-trt
complexity
it
does
not
seem
sensible
to
use
two
pivot
languages
as
part
of
a
fite-trt
system
last
we
examined
the
effectiveness
of
a
standard
clir
system
boosted
by
the
use
of
fite-trt
in
comparison
to
a
clir
system
augmented
with
trt
and
fuzzy
matching
oov
word
methods
and
in
comparison
to
dictionarytranslation-only
clir
and
monolingual
baselines
it
was
shown
that
fitetrt
with
web
document
frequencies
was
the
best
technique
among
several
approaches
to
handling
oov
words
tested
in
the
experiments
dictionarybased
clir
augmented
with
fite-trt
performed
substantially
better
than
the
baseline
where
oov
keys
were
kept
intact
in
spanish-english
clir
map
improvement
percentages
were
40
3
web
and
35
2
frequency
lists
also
finnish-english
fite-trt
queries
remarkably
outperformed
the
clir
baseline
although
performance
improvements
were
smaller
than
in
the
case
of
spanish-english
about
26
for
both
web
and
frequency
lists
the
trt
program
we
used
in
this
study
was
not
able
to
process
a
high
number
of
word
forms
in
a
reasonable
time
frame
and
we
had
to
apply
cf
and
frequency
thresholds
in
the
preliminary
tests
we
translated
without
using
any
thresholds
and
for
long
words
we
had
to
end
the
program
because
it
was
not
able
to
complete
the
translation
process
within
a
day
we
observed
that
for
many
source
words
equivalents
were
not
found
in
translation
sets
because
the
cfs
and/or
frequencies
of
the
relevant
rules
were
below
the
thresholds
we
therefore
expect
that
recall
values
can
still
be
improved
by
using
a
more
sophisticated
trt
program
that
allows
efficient
trt
even
without
the
use
of
thresholds
for
example
in
the
case
of
spanish/bio-terms/web
there
were
81
correct
identifications
one
wrong
identification
and
seven
words
for
which
trt
did
not
identify
equivalents
for
five
of
the
seven
words
equivalents
were
not
contained
in
the
translation
sets
because
of
low
cf
or
frequency
rules
the
five
words
and
the
rules
are
shown
in
table
xii
also
deficiencies
in
the
finnish-english
rule
collections
and
word
frequency
lists
caused
recall
errors
fite-trt
effectiveness
was
better
for
spanishenglish
than
finnish-english
the
better
effectiveness
can
be
attributed
to
the
higher
quality
of
spanish-english
rule
collection
deficiencies
in
the
finnishenglish
transformation
rules
can
be
overcome
by
using
more
data
in
rule
generation
the
frequency
lists
we
constructed
using
web
mining
turned
out
to
be
good
frequency
data
for
fite
however
for
some
source
word/equivalent
pairs
frequencies
were
too
low
for
rel-wf
formula
which
resulted
in
a
decrease
in
recall
performance
this
deficiency
can
be
overcome
by
adding
more
data
to
the
existing
lists
the
main
advantage
of
using
frequency
lists
is
that
after
the
lists
have
been
constructed
fite-trt
is
independent
of
the
web
this
is
important
when
a
practical
fite-trt
implementation
is
developed
the
frequency
lists
we
used
contain
biological
and
medical
terms
in
accordance
with
the
test
terminology
used
in
the
study
the
lists
are
large
and
contain
terms
in
various
domains
the
application
of
fite-trt
in
the
other
domains
may
require
lists
with
different
types
of
terms
however
for
each
domain
the
lists
are
concise
enough
to
be
held
in
main
memory
for
efficient
implementation
overall
the
percentage
of
wrong
equivalents
indicated
by
fite
was
small
the
identification
of
derivatives
and
plural
forms
of
the
correct
equivalents
was
the
primary
cause
of
precision
errors
as
an
example
for
the
finnish
word
leukosyytti
and
the
spanish
word
bacteria
the
correct
equivalents
are
leucocyte
and
bacterium
while
fite
gave
the
words
leucocytic
and
bacteria
many
of
these
types
of
cases
could
be
solved
by
augmenting
the
transformation
rules
with
word
class
information
for
example
that
a
rule
is
likely
to
refer
an
adjective
rather
than
a
noun
information
on
oov
word
s
word
class
is
achieved
when
the
sentential
context
of
the
oov
word
is
known
at
present
trt
is
only
intended
to
translate
singular
base
forms
word
class
information
is
needed
if
fite-trt
will
be
applied
to
running
texts
containing
inflectional
word
forms
this
would
imply
the
supplementation
of
rule
collections
with
word
class
information
the
next
main
challenge
in
the
fite-trt
research
is
to
improve
the
rules
such
that
fite-trt
can
handle
words
in
a
running
text
the
clir
experiments
showed
that
the
best
fuzzy
translation/matchingbased
query
was
fite-trt
with
web
document
frequencies
the
fite
component
of
fite-trt
was
the
focus
of
this
article
and
here
an
important
issue
is
its
contribution
in
clir
the
results
showed
that
in
the
case
of
finnish-english
fite-trt
was
significantly
better
than
plain
trt
but
only
at
p
0
05
in
the
case
of
spanish-english
fite-trt
did
not
show
significantly
better
results
overall
the
results
are
inconclusive
and
the
issue
needs
to
be
investigated
more
thoroughly
in
future
research
such
factors
as
query
structure
and
other
than
oov
keys
affect
the
effectiveness
of
fite-trt
and
trt
queries
also
efficiency
needs
to
be
taken
into
account
in
future
research
trt
queries
are
much
longer
than
fite-trt
queries
and
thus
require
more
processing
power
on
the
other
hand
the
fite
component
of
fite-trt
increases
computational
expense
in
other
information
systems
than
retrieval
systems
in
particular
in
mt
fuzzy
translation
trt
does
not
come
into
question
the
good
quality
of
translations
achieved
through
fite-trt
suggests
that
it
can
contribute
to
better
mt
performance
there
is
still
one
clir-related
application
of
fite-trt
that
is
worth
mentioning
an
automatic
construction
of
multilingual
dictionaries
of
technical
terms
and
proper
names
by
means
of
fite-trt
the
dictionary
construction
process
could
be
designed
to
be
largely
automatic
thanks
to
fite-trt
s
high
degree
of
effectiveness
given
a
list
of
words
and
a
set
of
transformation
rule
collections
the
process
would
automatically
yield
the
translation
equivalents
of
the
words
in
different
languages--the
result
would
essentially
be
a
multilingual
dictionary
the
construction
of
dictionaries
is
a
non-time-critical
task
given
enough
time
it
would
be
possible
to
construct
large
multilingual
dictionaries
the
cost
benefits
of
an
automatic
method
are
obvious
it
can
easily
be
seen
that
there
is
a
difference
in
the
cost
of
automatic
as
opposed
to
manual
construction
of
a
dictionary
of
say
10
languages
and
50
000
dictionary
entries
a
formal
model
of
annotations
of
digital
content
maristella
agosti
and
nicola
ferro
university
of
padua
this
article
is
a
study
of
the
themes
and
issues
concerning
the
annotation
of
digital
contents
such
as
textual
documents
images
and
multimedia
documents
in
general
these
digital
contents
are
automatically
managed
by
different
kinds
of
digital
library
management
systems
and
more
generally
by
different
kinds
of
information
management
systems
even
though
this
topic
has
already
been
partially
studied
by
other
researchers
the
previous
research
work
on
annotations
has
left
many
open
issues
these
issues
concern
the
lack
of
clarity
about
what
an
annotation
is
what
its
features
are
and
how
it
is
used
these
issues
are
mainly
due
to
the
fact
that
models
and
systems
for
annotations
have
only
been
developed
for
specific
purposes
as
a
result
there
is
only
a
fragmentary
picture
of
the
annotation
and
its
management
and
this
is
tied
to
specific
contexts
of
use
and
lacks-general
validity
the
aim
of
the
article
is
to
provide
a
unified
and
integrated
picture
of
the
annotation
ranging
from
defining
what
an
annotation
is
to
providing
a
formal
model
the
key
ideas
of
the
model
are
the
distinction
between
the
meaning
and
the
sign
of
the
annotation
which
represent
the
semantics
and
the
materialization
of
an
annotation
respectively
the
clear
formalization
of
the
temporal
dimension
involved
with
annotations
and
the
introduction
of
a
distributed
hypertext
between
digital
contents
and
annotations
therefore
the
proposed
formal
model
captures
both
syntactic
and
semantic
aspects
of
the
annotations
furthermore
it
is
built
on
previously
existing
models
and
may
be
seen
as
an
extension
of
them
1
introduction
digital
library
management
systems
dlms
are
currently
in
a
state
of
evolution
today
they
are
simply
places
where
information
resources
can
be
stored
and
made
available
to
end
users
whereas
tomorrow
they
will
increasingly
become
an
integrated
part
of
the
way
the
user
works
for
example
instead
of
simply
downloading
a
paper
and
then
working
on
a
printed
version
a
user
will
be
able
to
work
directly
with
the
paper
by
means
of
the
tools
provided
by
the
dlms
and
share
their
work
with
colleagues
by
doing
this
the
user
s
intellectual
work
and
the
information
resources
provided
by
the
dlms
can
be
merged
to
form
a
single
working
context
the
dlms
therefore
is
no
longer
perceived
as
something
external
to
the
intellectual
production
process
nor
is
it
seen
as
a
mere
consulting
tool
instead
it
becomes
an
intrinsic
and
active
part
of
the
intellectual
production
process
as
pointed
out
in
agosti
and
ferro
2005a
and
candela
et
al
2006
this
turning
point
in
dlms
also
clearly
emerges
from
the
outcomes
of
the
third
brainstorming
meeting
organized
by
delos1
the
european
network
of
excellence
on
digital
libraries
funded
by
the
eu
s
6th
framework
programme
the
main
conclusions
were
the
following
first
digital
libraries
need
to
become
more
user-centered
second
digital
libraries
should
not
simply
be
passive
repositories
rather
they
should
provide
users
with
tools
for
more
active
cooperation
and
communication
and
third
there
is
an
increasing
need
for
generalized
digital
library
management
systems
delos
2004
annotations
are
an
effective
means
to
enable
the
interaction
between
users
and
the
dlms
we
envision
since
their
use
is
a
diffuse
and
very
well-established
practice
annotations
are
not
only
a
way
of
explaining
and
enriching
an
information
resource
with
personal
observations
but
also
a
means
of
transmitting
and
sharing
ideas
to
improve
collaborative
work
practices
furthermore
annotations
allow
users
to
naturally
merge
and
link
personal
contents
with
the
information
resources
provided
by
the
dlms
so
that
a
common
context
unifying
all
of
these
contents
can
be
created
furthermore
annotations
cover
a
very
broad
spectrum
because
they
range
from
explaining
and
enriching
an
information
resource
with
personal
observations
to
transmitting
and
sharing
ideas
and
knowledge
on
a
subject
therefore
annotations
can
be
geared
not
only
to
the
individual
s
way
of
working
and
to
a
given
method
of
study
but
also
to
a
way
of
doing
research
moreover
they
may
cover
different
scopes
and
have
different
kinds
of
annotative
context
they
can
be
private
shared
or
public
according
to
the
type
of
intellectual
work
that
is
being
carried
out
in
addition
the
boundaries
among
these
scopes
are
not
fixed
rather
they
may
vary
and
evolve
with
time
finally
annotations
call
for
active
involvement
the
degree
of
which
varies
according
to
the
aim
of
the
annotation
private
annotations
require
the
involvement
of
the
authors
whereas
shared
or
public
annotations
involve
the
participation
of
a
whole
community
therefore
annotations
are
suitable
for
improving
collaboration
and
cooperation
among
users
as
pointed
out
by
ioannidis
et
al
2005
this
turning
point
of
digital
library
dl
requires
that
dl
development
must
move
from
an
art
to
a
science
and
it
needs
unifying
and
comprehensive
theories
and
frameworks
across
the
lifecycle
of
dl
information
ioannidis
et
al
2005
p
266
the
streams
structures
spaces
scenarios
societies
5s
model
proposed
by
goncalves
et
al
2004a
is
an
example
of
such
a
framework
and
was
one
of
the
first
efforts
in
this
direction
more
recently
the
reference
model
for
dlms
agosti
et
al
2006a
aims
at
laying
the
foundations
and
identifying
the
cornerstone
concepts
within
the
universe
of
digital
libraries
thus
facilitating
the
integration
of
research
and
proposing
better
ways
of
developing
appropriate
systems
the
notion
of
annotation
has
been
explicitly
introduced
in
this
model
as
a
first
class
concept
for
the
universe
of
dls
the
aim
of
this
article
is
to
contribute
to
the
development
of
such
unifying
frameworks
by
proposing
a
formal
model
for
the
annotation
of
digital
contents
the
motivations
of
this
proposal
lie
in
the
previous
presentation
dls
are
moving
towards
more
mature
dlms
which
are
supported
by
well
defined
formal
frameworks
and
annotations
are
also
headed
in
that
broad
direction
furthermore
to
date
there
has
been
little
agreement
about
what
an
annotation
is
nor
has
a
comprehensive
and
formal
model
of
the
annotation
been
proposed
with
respect
to
this
last
point
buneman
et
al
2002
p
150
state
that
view
annotation2
is
becoming
an
increasingly
useful
method
of
communicating
meta-data
among
users
of
shared
scientific
data
sets
and
to
our
knowledge
there
has
been
no
formal
study
of
this
problem
and
bottoni
et
al
2003
p
216
point
out
that
strangely
enough
there
is
not
an
agreement
yet
on
the
definition
of
digital
annotation
or
on
how
to
distinguish
it
from
other
digital
entities
e
g
hyperlinks
metadata
newsgroup
messages
furthermore
an
analysis
of
the
basic
operations
to
be
enabled
by
a
digital
annotation
system
seems
to
be
lacking
the
aim
of
the
formal
model
we
propose
is
to
formalize
the
main
concepts
concerning
annotations
and
to
define
the
relationships
between
annotations
and
annotated
information
resources
therefore
the
proposed
formal
model
captures
both
syntactic
and
semantic
aspects
of
the
annotations
as
well
as
building
on
previously
existing
models
such
as
the
streams
structures
spaces
scenarios
societies
model
this
new
model
thus
becomes
as
compatible
as
possible
with
the
previous
ones
and
may
be
seen
as
an
extension
of
them
the
rest
of
this
article
is
organized
as
follows
section
2
provides
an
overview
of
annotations
and
their
use
in
different
contexts
thus
presenting
the
reader
with
some
background
knowledge
and
the
main
issues
concerning
annotations
section
3
highlights
the
key
points
about
annotations
that
have
to
be
taken
into
consideration
when
modeling
them
and
introduces
the
different
areas
covered
by
the
proposed
formal
model
sections
4
to
8
explain
and
formalize
the
various
concepts
of
the
formal
model
according
to
what
has
been
anticipated
in
section
3
section
9
capitalizes
on
the
definitions
introduced
in
the
previous
sections
and
proposes
a
comprehensive
definition
of
annotation
section
10
shows
how
the
notion
of
hypertext
between
annotated
objects
and
annotations
follows
from
the
proposed
definition
of
annotation
finally
section
11
draws
some
conclusions
and
discusses
possible
directions
for
future
work
2
background
on
annotations
over
the
years
a
lot
of
research
has
been
done
on
annotations
the
main
focus
of
this
work
has
been
on
the
employment
of
ad
hoc
devices
or
handheld
devices
that
enable
reading
appliances
with
annotation
capabilities
marshall
et
al
1999
2001a
marshall
and
ruotolo
2002
schilit
et
al
1998
and
the
design
and
development
of
document
models
and
systems
that
support
annotations
agosti
et
al
2007a
phelps
and
wilensky
1996
1997
2000a
2000b
2001
bottoni
et
al
2003
in
specific
management
systems
in
particular
--
in
the
web
handschuh
and
staab
2003
bottoni
et
al
2004
2005a
2005b
brush
et
al
2001
2002
davis
and
huttenlocher
1995
nagao
2003
w3c
2005a
2005b
2007
--
in
digital
libraries
agosti
et
al
2003
2005a
agosti
and
ferro
2005b
agosti
et
al
2005b
agosti
and
ferro
2003a
agosti
et
al
2004
constantopoulos
et
al
2004
rigaux
and
spyratos
2004
gueye
et
al
2004
frommholz
et
al
2003
2004
neuhold
et
al
2004
thiel
et
al
2004
and
--
in
databases
stein
et
al
2002
bhagwat
et
al
2004
buneman
et
al
2001
2002
2004
tan
2004
the
aim
of
this
section
is
to
introduce
the
two
main
approaches
that
have
been
adopted
for
dealing
with
annotations
we
can
consider
them
either
metadata
which
is
discussed
in
section
2
1
or
content
which
is
presented
in
section
2
2
this
broad
distinction
in
the
viewpoints
about
annotations
is
also
pointed
out
by
marshall
1998
when
she
distinguishes
between
formal
versus
informal
annotations
where
the
former
are
metadata
and
the
latter
are
marginalia
of
the
sort
that
we
write
to
ourselves
as
we
read
a
journal
article
marshall
1998
p
41
which
correspond
to
the
content
this
section
does
not
aim
at
providing
a
full
and
exhaustive
survey
of
all
the
systems
offering
annotation
capabilities
that
have
been
developed
so
far
for
this
please
refer
to
agosti
et
al
2007a
handschuh
and
staab
2003
and
nagao
2003
instead
the
goal
here
is
to
describe
some
relevant
uses
and
features
of
annotations
so
that
in
section
3
these
features
can
be
used
to
gain
some
insights
which
will
prove
useful
when
developing
a
formal
model
for
annotations
2
1
annotations
as
metadata
annotations
can
be
considered
metadata
that
is
additional
data
which
relate
to
an
existing
content
and
clarify
the
properties
annotated
content
with
this
aim
in
mind
annotations
have
to
conform
to
some
specifications
that
define
the
structure
the
semantics
the
syntax
and
even
the
values
that
annotations
can
assume
the
recipients
of
this
kind
of
annotation
are
both
people
and
computing
devices
on
the
one
hand
metadata
can
benefit
people
because
if
they
are
expressed
in
a
human-readable
form
and
their
format
and
fields
are
known
they
can
be
read
by
people
and
used
to
obtain
useful
and
well-structured
information
about
an
existing
content
on
the
other
hand
metadata
offer
computing
devices
the
means
for
automatically
processing
the
annotated
contents
consider
for
example
the
case
of
the
machine
readable
cataloging
marc
3
records
not
only
they
are
a
useful
information
source
for
the
users
of
online
public
access
catalogs
opac
which
often
make
marc
records
accessible
but
they
also
act
as
a
standard
for
the
representation
and
communication
of
bibliographic
and
related
information
in
machinereadable
form
and
hence
for
the
automatic
processing
of
this
information
by
computing
devices
note
that
the
examples
presented
below
have
the
dual
use
just
described
they
can
be
considered
useful
for
both
people
and
computing
devices
a
relevant
example
of
this
use
of
annotations
is
the
mpeg-7
standard
formally
named
multimedia
content
description
interface
which
is
a
standard
for
annotating
and
describing
multimedia
content
data
iso
2004
to
a
certain
degree
mpeg-7
supports
the
interpretation
of
the
information
meaning
which
can
be
passed
onto
or
accessed
by
a
device
or
a
computer
code
mpeg-7
is
not
aimed
at
any
application
in
particular
rather
the
elements
that
mpeg-7
standardizes
can
support
many
broad
ranges
of
applications
in
this
case
annotating
a
multimedia
document
means
filling
in
the
various
fields
provided
by
the
mpeg-7
standard
in
order
to
describe
the
features
of
the
object
at
hand
similar
uses
of
annotations
can
be
found
in
the
natural
language
processing
field
for
example
part
of
speech
tagging
consists
of
annotating
each
word
in
a
sentence
with
a
tag
that
describes
its
appropriate
part
of
speech
so
as
to
decide
whether
a
word
is
a
noun
a
verb
an
adjective
and
so
on
jurafsky
and
martin
2000
manning
and
schutze
2001
in
both
cases
annotations
are
usually
embedded
in
the
annotated
digital
objects
a
broader
example
of
the
use
of
annotations
as
metadata
is
provided
by
the
semantic
web
w3c
2007
initiative
promoted
by
the
world
wide
web
consortium
w3c
which
aims
at
enhancing
human-understandable
data
namely
web
pages
with
computer-understandable
data
namely
metadata
so
that
information
is
given
well-defined
meaning
better
enabling
computing
devices
and
people
to
work
in
cooperation
berners-lee
et
al
2001
the
process
of
adding
metadata
to
web
pages
is
called
semantic
annotation
because
it
involves
the
decoration
of
existing
data
such
as
a
piece
of
text
whose
content
is
understandable
only
to
people
with
semantic
metadata
that
describe
that
piece
of
text
so
that
computers
can
process
it
and
thus
in
turn
offer
automation
integration
and
reuse
of
data
across
various
applications
handschuh
and
staab
2003
the
semantic
web
makes
use
of
the
resource
description
framework
rdf
4
as
a
syntax
for
describing
and
exchanging
metadata
in
this
context
the
annotea
project
developed
by
the
w3c
kahan
and
koivunen
2001
w3c
2005a
sees
annotations
as
metadata
and
interprets
them
as
the
first
step
in
creating
an
infrastructure
for
handling
and
associating
metadata
with
content
thus
leading
to
the
semantic
web
annotea
predefines
annotation
types
from
a
list
that
contains
some
of
the
following
comments
notes
explanations
or
other
types
of
external
remarks
that
can
be
attached
to
any
web
document
or
a
selected
part
of
the
document
without
modifying
the
document
annotea
uses
rdf
and
extensible
markup
language
xml
5
for
describing
annotations
as
metadata
and
xpointer6
for
locating
the
annotations
in
the
annotated
document
annotea
employs
a
client-server
architecture
where
annotations
reside
in
dedicated
servers
and
a
specialized
browser
is
capable
of
retrieving
them
upon
request
when
visiting
a
web
page
koivunen
and
swick
2001
and
koivunen
et
al
2003
go
one
step
further
and
employ
annotations
as
an
extension
of
bookmarks
to
improve
cooperation
among
users
the
additional
data
provided
by
annotations
are
exploited
to
describe
organize
categorize
share
and
search
for
the
bookmarks
in
the
scholnet7
dlms
constantopoulos
et
al
2004
use
annotations
as
metadata
to
support
communication
and
interaction
within
scholarly
communities
and
employ
the
sis-telos8
knowledge
representation
language
to
express
them
they
introduce
a
semantic
annotation
model
where
annotations
are
treated
as
documents
themselves
the
semantics
of
which
is
captured
by
a
controlled
vocabulary
of
annotation
types
furthermore
annotations
can
be
translated
into
records
compliant
with
a
subset
of
dublin
core
metadata
initiative
dcmi
9
specification
to
improve
interoperability
constantopoulos
et
al
2004
developed
a
service
for
the
scholnet
system
which
offers
storage
retrieval
and
deletion
of
annotations
in
a
similar
context
imaginum
patavinae
scientiae
archivum
ipsa
agosti
et
al
2003
2005a
2006c
supports
the
annotation
and
personalization
of
image
digital
archives
the
final
goal
is
to
provide
end
users
with
tools
for
performing
scientific
research
on
images
taken
from
illuminated
manuscripts
canova
1988
one
of
the
most
important
aims
of
the
research
on
illuminated
manuscripts
is
the
unveiling
of
hidden
connections
among
illustrations
belonging
to
different
manuscripts
the
use
of
annotations
has
been
proposed
as
a
useful
way
of
accessing
a
digital
archive
and
sharing
knowledge
in
a
cooperative
environment
in
ipsa
annotations
are
links
that
connect
one
image
to
another
image
related
to
it
because
illustrations
were
copied
from
images
in
other
manuscripts
or
they
were
merely
inspired
by
previous
works
or
they
were
directly
inspired
by
nature
ipsa
utilizes
annotations
as
metadata
because
they
are
drawn
from
a
link
taxonomy
which
comprises
two
broad
classes
the
first
deals
with
hierarchical
relationships
between
two
images
where
one
image
somehow
depends
on
an
earlier
one
the
second
concerns
relatedness
relationships
between
two
images
where
they
both
share
similar
properties
even
though
they
were
created
independently
both
classes
contain
more
specialized
annotation/link
types
ipsa
manages
and
stores
annotations
in
the
same
archive
where
the
illuminated
manuscripts
are
stored
even
if
annotations
are
separated
from
the
annotated
digital
objects
and
they
do
not
modify
them
finally
rigaux
and
spyratos
2004
and
gueye
et
al
2004
propose
a
data
model
for
the
composition
and
metadata
management
of
documents
in
a
distributed
setting
such
as
a
dlms
the
model
enables
the
creation
of
composite
documents
which
are
made
up
of
either
composite
documents
or
atomic
documents
which
can
be
any
piece
of
uniquely
identifiable
material
a
set
of
annotations
is
associated
to
each
composite
document
where
they
interpret
annotations
as
terms
taken
from
a
controlled
vocabulary
or
taxonomy
to
which
all
authors
adhere
the
model
provides
algorithms
to
automatically
compute
the
annotations
of
composite
documents
starting
from
the
annotations
of
its
composing
atomic
documents
by
means
of
a
subsumption
relation
defined
within
the
taxonomy
mentioned
previously
annotations
are
also
used
in
the
context
of
database
management
systems
dbms
and
in
particular
in
the
case
of
curated
databases
and
scientific
databases
swiss-prot10
is
a
curated
protein
sequence
database
which
strives
to
provide
a
high
level
of
annotation
such
as
the
description
of
the
function
of
a
protein
its
domain
structure
and
so
on
in
this
case
the
annotations
are
embedded
in
the
database
and
merged
with
the
annotated
content
biodas11
provides
a
distributed
annotation
system
which
is
a
system
based
on
web
servers
for
sharing
lists
of
annotations
across
a
certain
segment
of
the
genome
in
this
case
annotations
are
not
mixed
together
with
the
content
they
annotate
they
are
instead
separated
from
it
annotations
have
types
methods
and
categories
the
annotation
type
is
selected
from
a
list
of
types
that
have
biological
significance
the
annotation
method
is
intended
to
describe
how
the
annotated
feature
was
discovered
and
may
include
a
reference
to
a
program
the
annotation
category
is
a
broad
functional
category
that
can
be
used
to
filter
group
and
sort
annotations
stein
et
al
2002
buneman
et
al
2001
2002
investigate
the
use
of
annotations
with
respect
to
the
data
provenance
problem
sometimes
also
referred
to
as
data
lineage
or
data
pedigree
data
provenance
which
is
the
description
of
the
origins
of
a
piece
of
data
and
the
process
by
which
it
arrived
in
a
database
is
undoubtedly
an
open
and
challenging
research
issue
in
the
field
of
dbms
as
abiteboul
et
al
2005
point
out
buneman
et
al
2001
distinguish
between
why-provenance
which
explains
why
a
given
piece
of
data
is
in
the
database
and
where-provenance
which
explains
where
a
given
piece
of
data
comes
from
the
distinguishing
feature
of
scientific
databases
is
that
data
needs
to
be
tracked
for
example
to
know
which
instruments
were
used
to
gather
the
data
and
what
their
settings
were
moreover
scientists
and
experts
continuously
correct
and
annotate
the
original
source
data
and
this
too
is
a
way
of
carrying
out
their
research
work
bhagwat
et
al
2004
carry
on
the
research
into
provenance
and
propose
an
extension
to
a
relational
dbms
and
to
structured
query
language
sql
called
propagate
sql
which
provides
a
clause
for
propagating
annotations
to
tuples
through
queries
they
see
annotations
as
information
about
data
such
as
provenance
comments
or
other
types
of
metadata
they
envisage
the
following
applications
of
annotations
in
dbms
tracing
the
provenance
and
flow
of
data
reporting
errors
or
remarks
about
a
piece
of
data
and
describing
the
quality
or
the
security
level
of
a
piece
of
data
as
a
final
example
of
the
use
of
annotations
as
metadata
once
more
in
the
context
of
scientific
databases
buneman
et
al
2004
propose
an
archiving
technique
for
managing
and
archiving
different
versions
of
scientific
databases
over
time
they
exploit
the
hierarchical
structure
of
scientific
data
to
represent
the
content
and
the
different
versions
of
the
database
with
a
tree
structure
they
attach
annotations
to
the
nodes
of
the
tree
annotations
that
contain
time-stamp
and
key
information
about
the
underlying
data
structure
therefore
these
annotations
are
metadata
about
the
annotations
contained
in
the
database
itself
in
a
sense
we
could
say
that
these
annotations
are
meta-metadata
as
we
can
see
they
differ
from
the
annotations
contained
in
the
database
in
that
they
are
metadata
about
the
modifications
to
the
contents
of
the
database
over
time
while
the
latter
are
metadata
about
genome
sequences
on
the
whole
this
annotated
tree
structure
provides
an
additional
data
layer
that
enables
the
development
of
efficient
algorithms
for
archiving
and
searching
for
the
different
versions
of
the
database
2
2
annotations
as
content
annotations
are
regarded
as
additional
content
that
relates
to
an
existing
content
meaning
that
they
increase
the
existing
content
by
providing
an
additional
layer
of
elucidation
and
explanation
however
this
elucidation
does
not
happen
as
in
the
case
of
annotations
as
metadata
by
means
of
some
kind
of
constrained
or
formal
description
of
the
semantics
of
the
annotated
object
on
the
contrary
the
explanation
itself
takes
the
shape
of
an
additional
content
that
can
help
people
understand
the
annotated
content
however
the
semantics
of
the
additional
content
may
be
no
more
explicit
for
a
computing
device
than
the
semantics
of
the
annotated
content
this
view
of
annotations
is
comparable
to
the
activity
of
reading
a
document
and
adding
notes
to
it
explanation
and
clarification
of
words
or
passages
of
the
document
by
expounding
on
it
providing
a
commentary
on
it
and
finally
completing
it
with
personal
observations
and
ideas
therefore
the
final
recipients
of
this
kind
of
annotation
are
people
because
a
content
annotation
does
not
make
the
annotated
object
more
readily
processable
by
a
computer
than
the
same
object
without
annotations
in
fact
from
the
point
of
view
of
a
computer
the
semantics
of
content
annotations
needs
to
be
in
some
way
processed
for
example
indexed
before
it
can
be
used
to
deal
with
the
semantics
of
the
annotated
object
this
is
quite
different
from
the
case
of
metadata
annotations
which
are
pieces
of
information
ready
to
be
used
for
interpreting
the
semantics
of
the
annotated
object
in
contrast
the
additional
semantics
provided
by
content
annotations
can
offer
people
useful
interpretations
and
comments
for
the
annotated
object
making
it
easier
to
understand
its
hidden
facets
this
view
of
annotations
entails
an
intrinsic
dualism
between
annotation
as
content
enrichment
and
annotation
as
stand-alone
document
the
former
considers
annotations
as
mere
additional
content
regarding
an
existing
document
and
as
a
result
they
are
not
autonomous
entities
but
in
fact
rely
on
previously
existing
information
resources
to
justify
their
existence
the
latter
regards
annotations
as
real
documents
and
autonomous
entities
that
maintain
some
sort
of
connection
with
an
existing
document
this
twofold
nature
of
the
annotation
can
be
made
clearer
by
considering
how
we
study
a
document
first
of
all
we
can
annotate
some
passages
that
require
a
further
looking
into
we
can
consider
this
as
a
sort
of
annotation
as
content
enrichment
second
we
can
reconsider
and
collect
our
annotations
and
we
can
use
them
as
a
starting
point
for
a
new
document
this
is
an
example
of
annotation
as
a
stand-alone
document
in
this
case
the
annotation
process
can
be
seen
as
an
informal
unstructured
elaboration
that
could
lead
to
a
rethinking
of
the
annotated
document
and
to
the
creation
of
a
new
one
note
that
both
kinds
of
content
annotation
are
valuable
and
the
boundaries
between
them
may
fade
into
one
another
with
the
passing
of
time
as
a
consequence
both
kinds
of
content
annotations
need
to
be
considered
as
first-class
digital
objects
notecards
halasz
et
al
1987
halasz
1988
influenced
the
successive
research
in
the
field
of
hypermedia/hypertext
systems
notecards
is
a
hypermedia
system
designed
for
helping
people
to
work
with
ideas
authors
researchers
and
intellectual
work
practitioners
can
analyze
information
construct
models
formulate
topics
and
elaborate
ideas
by
using
a
network
of
electronic
notecards
interconnected
by
typed
links
one
of
the
famous
seven
issues
mentioned
by
halasz
1988
concerns
support
for
collaborative
work
he
highlighted
how
annotations
are
part
of
the
activities
that
form
the
basis
of
any
collaboration
effort
halasz
1988
p
848
moving
forward
in
the
context
of
the
web
the
conote
davis
and
huttenlocher
1995
is
a
cooperative
system
for
supporting
communications
within
groups
of
users
by
using
shared
annotations
on
a
set
of
documents
conote
offers
plain
text
or
hypertext
markup
language
html
w3c
1999
annotations
on
web
pages
and
pays
particular
attention
in
structuring
annotations
on
the
same
part
of
a
document
as
a
tree
in
order
to
ease
the
discussion
among
the
users
by
supporting
replies
to
previously
inserted
annotations
conote
stores
annotations
separately
from
the
original
web
page
and
displays
them
on
request
by
using
a
standard
browser
a
similar
approach
is
also
adopted
by
the
webann
system
brush
et
al
2002
which
is
aimed
at
supporting
interaction
between
students
and
instructors
by
using
annotations
with
respect
to
conote
it
offers
users
the
possibility
of
more
finely
tuned
annotations
a
recent
example
of
this
kind
of
annotation
system
in
the
web
is
multimedia
annotation
of
digital
content
over
the
web
madcow
bottoni
et
al
2004
2005b
this
system
enables
multimedia
annotation
on
web
pages
and
is
based
on
a
client-server
architecture
servers
are
repositories
of
annotations
to
which
different
clients
can
connect
while
the
client
is
a
plug-in
for
a
standard
web
browser
madcow
uses
hypertext
transfer
protocol
http
fielding
et
al
1999
as
the
communication
protocol
between
the
annotation
servers
and
the
browser
plugin
moreover
it
assumes
that
pages
are
written
in
html
in
order
to
annotate
web
pages
it
supports
both
private
and
public
annotations
and
allows
different
pre-established
types
of
annotations
such
as
explanation
comment
question
solution
summary
and
so
on
which
are
defined
according
to
the
rhetorical
structure
theory
rst
mann
and
thompson
1987
note
that
once
annotations
have
been
created
they
are
assigned
a
uniform
resource
locator
url
berners-lee
1994a
and
are
treated
as
any
other
html
document
therefore
annotations
can
be
made
on
other
annotations
annotations
as
content
also
find
a
natural
application
in
the
context
of
a
dl
a
dl
is
not
only
the
digital
version
of
traditional
libraries
and
archives
but
also
offers
instruments
and
services
that
can
go
beyond
the
mere
presentation
of
content
stored
in
digital
repositories
lesk
2005
witten
and
bainbridge
2003
candela
et
al
2006
annotations
can
be
considered
one
such
instrument
moreover
as
introduced
in
section
1
both
dls
and
their
management
systems
the
dlms
can
greatly
benefit
from
annotations
for
actively
involving
users
and
promoting
interaction
among
them
ioannidis
et
al
2005
also
agree
with
this
point
when
they
state
that
dlms
should
provide
the
means
for
creating
annotations
and
should
support
the
storage
selective
sharing
and
configurable
presentation
of
annotations
different
layers
of
annotations
can
coexist
for
the
same
document
a
private
layer
of
annotations
accessible
only
by
authors
of
the
annotations
a
collective
layer
of
annotations
shared
by
a
team
of
people
and
finally
a
public
layer
of
annotations
accessible
to
all
the
users
of
the
digital
library
in
this
way
user
communities
can
benefit
from
different
views
of
the
information
resources
managed
by
the
dl
marshall
1997
marshall
and
brush
2002
2004
a
dl
can
encourage
cooperative
work
practices
enabling
the
sharing
of
documents
and
annotations
also
with
the
aid
of
special
devices
such
as
xlibris
schilit
et
al
1998
finally
as
suggested
in
marshall
et
al
2001b
marshall
and
ruotolo
2002
searching
reading
and
annotating
the
information
resources
of
a
dl
can
be
done
together
with
other
activities
for
example
working
with
colleagues
this
may
also
occur
in
a
mobile
context
where
merging
content
and
wireless
communications
can
foster
ubiquitous
access
to
dlms
improving
well
established
cooperative
practices
of
work
and
exploiting
physical
and
digital
resources
the
wireless
context
and
the
small
form
factor
of
handheld
devices
challenge
our
technical
horizons
for
information
management
and
access
specialized
solutions
are
required
to
overcome
the
constraints
imposed
by
such
kinds
of
devices
as
reported
in
agosti
and
ferro
2003b
an
example
of
this
use
of
annotations
in
dlms
is
collaboratory
for
annotation
indexing
and
retrieval
of
digitized
historical
archive
material
collate
frommholz
et
al
2003
thiel
et
al
2004
which
supports
the
collaboration
among
film
scientists
and
archivists
who
are
annotating
historical
film
documentation
dealing
with
digitized
versions
of
documents
about
european
films
from
the
1920s
and
1930s
such
documents
are
censorship
documents
newspaper
articles
posters
advertisement
material
registration
cards
and
photos
annotations
support
user
communities
in
accessing
the
information
resources
provided
by
the
dl
in
a
personalized
and
customized
way
they
are
dialog
acts
part
of
a
discourse
about
film
documentation
and
constitute
the
document
context
intended
as
the
context
of
the
collaborative
discourse
in
which
the
document
is
placed
this
collaborative
discourse
is
carried
out
by
allowing
annotations
to
annotate
other
annotations
note
that
collate
offers
different
predefined
types
of
dialog
acts
such
as
elaboration
comparison
argumentation
counterargument
and
so
on
in
this
respect
collate
opts
for
a
solution
similar
to
the
one
adopted
by
madcow
flexible
annotation
service
tool
fast
fast
agosti
and
ferro
2003a
2004
2005b
2005a
2006
ferro
2004
2005
is
a
flexible
system
designed
to
support
two
different
things
various
architectural
paradigms
such
as
peer-topeer
p2p
or
web
services
ws
architectures
a
wide
range
of
different
dlms
the
flexibility
of
fast
and
its
independence
from
any
particular
dlms
is
a
key
feature
for
providing
users
with
a
uniform
means
of
interaction
with
annotation
functionalities
without
the
need
for
changing
their
annotative
practices
only
because
a
user
works
with
different
dlms
fast
supports
both
users
and
groups
of
users
with
different
access
permission
on
annotations
and
offers
three
different
scopes
for
the
annotations
private
shared
and
public
furthermore
annotations
in
fast
allow
users
to
merge
their
personal
content
with
the
information
resources
managed
by
diverse
dlms
annotations
can
span
and
cross
the
boundaries
of
a
single
dlms
annotating
digital
objects
that
are
part
of
different
dls
if
users
so
desire
finally
this
use
of
annotations
gives
the
users
the
possibility
of
linking
digital
objects
that
otherwise
would
have
remained
separated
because
they
were
managed
by
different
dlms
ioannidis
et
al
2005
recently
noted
this
as
an
advantage
for
users
and
a
challenge
for
the
next
generation
dlms
fast
also
constitutes
the
underlying
infrastructure
of
the
digital
library
annotation
service
dilas
project
agosti
et
al
2005c
2006d
2006b
which
is
an
ongoing
project
in
the
framework
of
delos
the
european
network
of
excellence
on
digital
libraries
the
goal
of
dilas
is
to
design
and
develop
a
generic
annotation
service
that
can
be
easily
used
in
different
dlms
dilas
aims
at
defining
a
set
of
application
program
interfaces
to
enable
both
access
to
this
service
from
different
dlms
and
the
creation
of
different
annotation
clients
and
user
interfaces
embedded
in
various
dlms
the
annotation
service
will
be
evaluated
as
a
new
way
of
interacting
with
a
dl
and
cooperating
among
dl
users
and
stakeholders
with
respect
to
this
last
issue
the
dilas
project
defines
the
overall
aim
of
a
formative
evaluation
about
how
the
present
design
of
decentralized
annotation
services
complies
with
the
needs
of
the
prospective
users
the
overall
goal
of
the
evaluation
is
to
investigate
the
extent
to
which
the
annotation
system
complies
with
the
characteristics
activities
tasks
and
environments
of
users
in
order
to
inspire
future
developments
of
annotation
services
and
tools
for
dls
3
modeling
annotations
3
1
key
points
table
i
summarizes
the
discussion
introduced
in
section
2
and
presents
systems
along
two
dimensions
one
is
the
degree
of
structure
of
the
content
and
the
other
is
the
degree
of
structure
of
the
annotation
the
structure
of
the
content
can
range
from
loosely
structured
documents
as
in
the
case
of
the
web
to
highly
structured
data
as
in
the
case
of
a
database
similarly
the
structure
of
the
annotation
can
vary
from
unstructured
or
loosely
structured
annotations
as
in
the
case
of
content
annotations
to
very
structured
annotations
as
in
the
case
of
metadata
annotations
note
that
we
have
put
the
notecards
system
under
the
web
box
even
though
it
was
developed
before
the
web
because
a
hypermedia
system
is
much
more
closer
to
the
web
than
to
dbms
or
dlms
we
can
point
out
that
across
the
different
systems
there
is
a
very
wide
range
of
uses
of
annotations
as
a
powerful
tool
for
uncovering
and
clarifying
the
semantics
of
the
annotated
objects
the
final
recipients
of
annotations
can
be
computing
devices
or
people
the
former
is
mainly
the
case
of
metadata
annotations
which
allow
annotated
objects
to
be
automatically
processed
integrated
and
reused
in
different
applications
even
though
these
metadata
annotations
can
be
understandable
and
useful
for
people
too
the
latter
is
mainly
the
case
of
content
annotations
which
elucidate
and
expound
on
an
annotated
object
note
that
also
in
this
latter
case
a
computing
device
can
become
the
recipient
of
such
annotations
provided
that
some
further
step
of
processing
is
performed
for
example
indexing
however
in
both
cases
the
semantics
of
the
annotation
itself
needs
to
be
taken
into
consideration
and
modeled
this
can
happen
formally
and
precisely
by
agreeing
on
metadata
standards
that
describe
how
annotations
should
to
be
interpreted
and
used
alternatively
support
can
be
provided
for
identifying
different
predefined
annotation
types
perhaps
with
varying
levels
of
detail
the
medium
of
the
annotation
can
vary
a
lot
it
can
range
from
textual
annotations
to
image
audio
and
video
annotations
in
a
general
setting
we
may
need
to
deal
with
multimedia
rich
annotations
composed
of
different
parts
each
with
its
own
medium
all
of
these
different
kinds
of
media
have
to
be
considered
and
properly
modeled
in
a
uniform
way
where
possible
both
annotations
and
annotated
objects
need
to
be
uniquely
identified
moreover
annotations
comprise
a
temporal
dimension
that
is
often
not
explicit
but
which
limits
the
creation
of
the
annotation
to
the
existence
of
another
object
this
temporal
relationship
between
the
annotation
and
the
annotated
object
does
not
mean
that
the
annotation
cannot
be
considered
a
stand-alone
intellectual
work
but
it
does
impose
a
temporal
ordering
between
the
existence
of
an
annotated
object
and
the
annotation
annotating
it
which
cannot
be
overlooked
in
addition
once
we
have
identified
both
the
annotation
and
the
annotated
object
we
need
to
link
and
anchor
the
annotation
to
the
part
of
the
annotated
object
in
question
this
can
happen
in
a
way
that
mainly
depends
on
the
medium
of
the
annotated
object
on
the
whole
we
need
to
model
how
annotations
and
annotated
objects
are
uniquely
identified
and
linked
together
maybe
with
a
varying
degree
of
granularity
in
the
anchoring
paying
particular
attention
to
the
temporal
dimension
that
regulates
the
relationships
between
annotations
and
annotated
objects
as
far
as
co
operation
is
concerned
almost
all
of
the
analyzed
systems
show
that
annotations
have
great
potential
for
supporting
and
improving
interaction
among
users
and
even
among
computing
devices
therefore
there
is
a
need
for
modeling
and
offering
different
scopes
of
annotations
for
example
private
shared
or
public
and
managing
the
access
rights
of
various
groups
of
users
finally
a
relevant
aspect
of
annotations
is
that
they
can
take
the
part
of
a
hypertext
agosti
et
al
2004
halasz
1988
marshall
1998
since
they
enable
the
creation
of
new
relationships
among
existing
objects
by
means
of
links
that
connect
annotations
together
with
existing
objects
as
we
will
see
later
in
more
detail
the
hypertext
viewpoint
about
annotations
is
common
to
different
systems
such
as
annotea
madcow
and
notecards
in
the
hypermedia/web
context
or
dilas
fast
and
ipsa
in
the
dlms
context
halasz
1988
points
out
that
annotations
are
one
of
the
activities
that
form
the
basis
of
any
collaborative
effort
and
for
which
hypermedia
systems
are
ideally
suited
while
marshall
1998
considers
annotations
a
natural
way
of
creating
and
growing
hypertexts
that
connect
information
resources
by
actively
engaging
users
in
addition
the
hypertext
between
annotations
and
annotated
objects
can
be
exploited
not
only
for
providing
alternative
navigation
and
browsing
capabilities
but
also
for
offering
advanced
search
functionalities
able
to
retrieve
more
and
better
ranked
objects
in
response
to
a
user
query
and
also
by
exploiting
the
annotations
linked
to
them
agosti
and
ferro
2005b
moreover
dlms
usually
offer
some
basic
hypertext
and
browsing
capabilities
based
on
the
available
structured
data
such
as
authors
or
references
on
the
other
hand
dlms
do
not
normally
provide
users
with
advanced
hypertext
functionalities
where
the
information
resources
are
linked
on
the
basis
of
the
semantics
of
their
content
and
hypertext
information
retrieval
hir
functionalities
are
available
as
in
agosti
et
al
1991
therefore
annotations
can
turn
out
to
be
an
effective
way
of
associating
this
kind
of
hypertext
to
a
dlms
to
enable
the
active
and
dynamic
use
of
information
resources
in
addition
this
hypertext
can
span
and
cross
the
boundaries
of
the
single
dlms
if
users
need
to
interact
with
the
information
resources
managed
by
diverse
dlms
agosti
and
ferro
2004
2005a
this
latter
possibility
is
quite
innovative
because
it
offers
the
means
for
interconnecting
various
dlms
in
a
personalized
and
meaningful
way
for
the
end-user
and
as
ioannidis
et
al
2005
point
out
this
is
a
major
challenge
for
the
next
generation
dlms
in
conclusion
this
hypertext
has
to
be
explicitly
modeled
and
taken
into
consideration
when
dealing
with
annotations
3
2
modeling
approach
the
previous
discussion
clearly
demonstrates
that
annotation
is
quite
a
complex
concept
comprising
a
number
of
different
aspects
therefore
when
we
attempt
to
model
annotations
we
have
to
work
through
this
complexity
first
to
identify
the
main
macro-areas
of
this
concept
and
second
to
provide
clear
definitions
of
the
concepts
within
each
macro-area
and
of
the
relationships
among
these
concepts
figure
1
provides
both
an
overview
of
the
areas
covered
and
the
detail
of
the
definitions
introduced
within
each
area
the
figure
clearly
shows
how
these
areas
correspond
to
the
very
basic
issues
that
emerge
when
we
think
about
annotations
we
need
to
identify
annotations
and
annotated
objects
in
order
to
link
them
together
perhaps
providing
facilities
for
supporting
cooperation
and
we
have
to
deal
with
both
the
actual
contents
of
an
annotation
and
the
semantics
expressed
by
those
contents
therefore
the
objective
of
the
formal
model
is
to
delimit
the
boundaries
of
each
area
and
to
clearly
define
the
concepts
contained
in
each
area
together
with
the
relationships
among
them
we
can
then
exploit
these
definitions
to
provide
a
comprehensive
definition
of
annotation
and
to
derive
from
this
definition
the
notion
of
hypertext
between
annotated
objects
and
annotations
to
this
end
we
adopt
a
modeling
approach
based
on
set
theory
halmos
1974
and
graph
theory
bollobas
1998
diestel
2000
in
the
remainder
of
this
section
we
will
briefly
introduce
the
areas
shown
in
figure
1
which
can
be
used
as
a
map
of
the
concepts
dealt
with
in
the
formal
model
note
that
figure
1
illustrates
the
main
areas
from
bottom
to
top
in
the
same
order
in
which
they
were
discussed
in
section
3
1
which
represents
the
natural
way
in
which
they
occur
when
we
reason
about
annotations
in
contrast
the
areas
are
presented
below
in
the
order
in
which
they
will
be
discussed
in
detail
in
sections
4
to
8
this
order
derives
from
the
need
to
introduce
concepts
and
definitions
in
a
linear
fashion
thus
avoiding
back
and
forth
references
between
them
finally
section
9
will
define
the
annotation
itself
and
section
10
will
discuss
the
hypertext
between
annotated
objects
and
annotations
which
can
also
be
thought
of
as
a
kind
of
a
concise
view
of
the
main
relationships
between
annotations
and
annotated
objects
identification
is
the
problem
of
uniquely
identifying
both
the
annotation
and
the
annotated
objects
highlighting
the
temporal
constraints
between
them
this
area
is
built
around
the
concept
of
handle
which
is
defined
as
a
unique
identifier
of
both
digital
objects
and
annotations
and
the
proposed
notation
for
dealing
with
the
time
dimension
involved
by
annotations
cooperation
is
about
annotations
as
a
cooperation
tool
among
users
it
introduces
the
definitions
of
user
and
group
of
users
together
with
the
associated
concept
of
scope
of
annotation
and
access
permission
which
regulate
the
access
policies
for
a
given
annotation
linking
deals
with
the
allowed
linking
patterns
between
digital
objects
and
annotations
and
the
problem
of
correctly
anchoring
annotations
to
digital
objects
it
defines
the
concepts
of
link
type
which
is
defined
as
the
allowed
methods
of
linking
annotations
to
annotated
objects
stream
which
abstracts
the
notion
of
content
of
a
digital
object
and
segment
which
represents
a
given
portion
of
a
stream
useful
for
anchoring
an
annotation
to
a
digital
object
semantics
concerns
the
meaning
of
the
annotation
and
what
it
stands
for
trying
to
make
explicit
the
semantics
of
the
different
parts
of
the
content
of
an
annotation
it
introduces
the
notions
of
meaning
of
annotation
which
is
part
of
the
semantics
of
the
whole
annotation
and
meanings
graph
that
is
a
graph
allows
for
interoperability
between
the
different
meanings
materialization
deals
with
the
way
in
which
the
semantics
carried
by
an
annotation
can
take
shape
that
is
the
actual
content
of
the
annotation
perceived
by
the
user
it
describes
the
sign
of
annotation
which
is
a
particular
type
of
stream
representing
part
of
the
content
of
an
annotation
the
proposed
formal
model
was
first
introduced
in
ferro
2004
and
a
reduced
version
of
it
was
adapted
to
the
context
of
illuminated
manuscripts
in
agosti
et
al
2006c
4
identification
in
order
to
uniquely
identify
both
the
annotation
and
the
annotated
objects
we
need
to
proceed
as
follows
first
we
need
to
define
the
objects
we
deal
with
as
described
in
section
4
1
then
we
also
have
to
be
able
to
deal
with
objects
whose
relationships
are
constrained
by
a
temporal
dimension
as
explained
in
section
4
2
finally
a
suitable
identification
mechanism
has
to
be
provided
as
introduced
in
section
4
3
4
1
document
annotation
and
digital
object
sets
according
to
widely
accepted
terminology
we
adopt
the
term
digital
object
to
refer
to
information
resources
managed
by
an
information
management
system
ims
indeed
paskin
2006
p
6
defines
the
digital
object
as
a
data
structure
whose
principal
components
are
digital
material
or
data
plus
a
unique
identifier
for
this
material
goncalves
et
al
2004a
p
292
say
that
information
in
digital
libraries
is
manifest
in
terms
of
digital
objects
which
can
contain
textual
or
multimedia
content
e
g
images
audio
video
and
metadata
and
they
define
a
digital
object
as
a
tuple
constituted
by
a
unique
handle
structured
contents
and
structural
metadata
goncalves
et
al
2004a
p
294
finally
bottoni
et
al
2003
p
217
define
the
digital
object
as
a
typed
tuple
of
attribute-value
pairs
with
at
least
two
mandatory
attributes
a
unique
identifier
and
the
actual
content
of
the
digital
object
furthermore
they
consider
annotations
to
be
digital
objects
with
specific
attributes
that
is
annotations
are
specialized
digital
objects
in
the
following
we
need
terminology
to
distinguish
between
two
kinds
of
digital
objects
the
generic
ones
managed
by
the
ims
which
we
call
documents
and
the
ones
that
are
annotations
therefore
when
we
use
the
generic
term
digital
object
we
mean
a
digital
object
that
can
be
either
a
document
or
an
annotation
note
that
the
term
document
is
used
here
in
a
broad
sense
since
it
indicates
a
generic
multimedia
possibly
compound
digital
object
examples
of
such
a
broad
use
of
the
term
document
can
be
found
in
castelli
and
pagano
2002a
who
define
the
document
as
a
structured
multilingual
and
multimedia
information
object
or
in
the
document
object
model
dom
w3c
1998
where
the
term
document
indicates
many
different
kinds
of
information
that
may
be
stored
in
diverse
systems
and
much
of
this
would
traditionally
be
seen
as
data
rather
than
as
documents
finally
note
that
when
we
talk
about
annotations
we
mean
both
metadata
annotations
introduced
in
section
2
1
and
content
annotations
explained
in
section
2
2
therefore
we
consider
both
of
these
types
of
annotations
as
kinds
of
digital
objects
the
following
definition
introduces
the
different
sets
of
digital
objects
we
will
need
to
deal
with
definition
4
1
let
us
define
the
following
sets
--
d
is
a
set
of
documents
and
d
d
is
a
generic
document
u
d
is
a
universe
set
of
documents
which
is
the
set
of
all
the
possible
documents
so
that
d
u
d
--
a
is
a
set
of
annotations
and
a
a
is
a
generic
annotation
u
a
is
a
universe
set
of
annotations
which
is
the
set
of
all
the
possible
annotations
so
that
a
u
a
--do
d
a
is
a
set
of
digital
objects
and
do
do
is
either
a
document
or
an
annotation
udo
u
a
u
d
is
a
universe
set
of
digital
objects
so
that
do
udo
4
2
expressing
the
temporal
dimension
involved
by
annotations
the
universe
sets
u
d
u
a
and
udo
are
abstract
sets
since
they
contain
all
the
possible
needed
objects
whether
they
actually
exist
or
not
in
any
given
moment
on
the
other
hand
the
sets
d
a
and
do
are
tangible
sets
that
contain
the
objects
that
already
exist
in
a
given
moment
if
we
pick
out
an
element
from
d
a
or
do
we
are
dealing
with
a
digital
object
that
has
been
created
even
before
we
start
working
on
it
in
other
words
the
element
already
exists
the
d
a
and
do
sets
are
a
sort
of
time-variant
sets
since
we
can
add
delete
or
modify
elements
of
these
sets
over
time
on
the
other
hand
the
u
d
u
a
and
udo
sets
are
a
sort
of
time-invariant
sets
since
they
already
contain
every
possibile
object
we
may
need
to
deal
with
the
annotation
is
the
result
of
an
intellectual
task
performed
on
an
existing
digital
object
and
it
follows
an
already
existing
digital
object
therefore
the
annotation
comprises
a
temporal
dimension
which
is
often
not
explicit
but
that
limits
the
creation
of
the
annotation
to
the
existence
of
another
digital
object
this
temporal
relationship
between
the
annotation
and
the
annotated
digital
object
does
not
mean
that
the
annotation
cannot
be
considered
a
stand-alone
intellectual
task
but
it
does
impose
a
temporal
ordering
between
the
existence
of
an
annotated
digital
object
and
its
annotation
that
cannot
be
overlooked
in
conclusion
we
need
some
mechanism
for
rendering
the
time
dimension
explicit
if
necessary
we
will
illustrate
this
mechanism
by
means
of
some
examples
which
show
some
interesting
cases
please
note
that
they
do
not
aim
to
be
exhaustive
although
these
examples
do
make
use
of
the
set
do
they
have
a
more
general
validity
creation
of
a
new
digital
object
consists
of
the
following
events
1
we
start
with
the
set
of
digital
objects
at
time
k
do
k
2
we
create
a
new
digital
object
we
pick
out
an
element
from
the
universe
set
of
digital
objects
that
does
not
belong
to
do
k
d
o
do
k
udo
3
we
end
up
with
a
new
set
of
digital
objects
at
time
k
1
which
contains
the
newly
created
digital
object
do
k
1
do
k
do
2udo
therefore
we
have
the
following
temporal
ordering
both
events
1
and
2
happen
at
time
k
but
at
that
time
the
newly
created
digital
object
does
not
yet
belong
to
the
set
do
k
of
digital
objects
at
time
k
event
3
happens
at
time
k
1
and
represents
the
new
set
of
digital
objects
that
now
also
contains
the
newly
created
and
existing
digital
object
deletion
of
an
existing
digital
object
consists
of
the
following
events
1
we
start
with
the
set
of
digital
objects
at
time
k
do
k
2
we
choose
an
existing
digital
object
in
the
set
of
digital
objects
at
time
k
do
do
k
3
we
end
up
with
a
new
set
of
digital
objects
at
time
k
1
which
no
longer
contains
the
previously
chosen
digital
object
do
k
1
do
k
do
2udo
therefore
we
have
the
following
temporal
ordering
both
events
1
and
2
happen
at
time
k
event
3
happens
at
time
k
1
and
represents
the
new
set
of
digital
objects
which
does
not
contain
the
previously
existing
digital
object
modification
of
an
existing
digital
object
consists
of
the
following
events
--
we
start
with
the
set
of
digital
objects
at
time
k
do
k
--
we
choose
an
existing
digital
object
in
the
set
of
digital
objects
at
time
k
do
do
k
--
we
choose
a
new
digital
object
which
is
the
modified
version
of
the
previously
chosen
digital
object
and
does
not
belong
to
do
k
do
do
k
udo
--
we
end
up
with
a
new
set
of
digital
objects
at
time
k
1
which
contains
the
modified
version
of
the
digital
object
therefore
we
have
the
following
temporal
ordering
events
1
2
and
3
happen
at
time
k
but
at
that
time
the
modified
digital
object
does
not
yet
belong
to
the
set
do
k
of
digital
objects
at
time
k
event
4
happens
at
time
k
1
and
represents
the
new
set
of
digital
objects
which
now
contains
the
modified
digital
object
these
three
basic
examples
reveal
our
strategy
for
addressing
the
time
dimension
--
time
k
identify
an
initial
set
do
k
to
work
with
--
time
k
identify
the
digital
objects
to
work
with
which
may
belong
to
do
k
or
not
if
the
identified
digital
objects
belong
to
do
k
then
they
already
exist
on
the
other
hand
if
the
identified
digital
objects
do
not
belong
to
do
k
then
they
do
not
yet
exist
and
therefore
this
step
represents
their
creation
--
time
k
1
identify
the
new
set
do
k
1
which
results
from
performing
the
appropriate
operations
on
the
set
and
the
digital
objects
previously
identified
in
all
of
the
cases
both
do
k
and
do
k
1
contain
only
digital
objects
that
already
exist
this
mechanism
allows
us
to
unambiguously
state
which
objects
we
are
dealing
with
in
any
given
moment
and
the
moment
when
they
are
able
to
be
utilized
do
k
and
do
k
1
unambiguously
identify
the
digital
objects
we
are
dealing
with
which
are
given
by
do
k
do
k
1
in
particular
the
deleted
digital
objects
are
given
by
do
k
do
k
1
and
the
newly
created
digital
objects
are
given
by
do
k
1
do
k
therefore
we
can
talk
about
the
digital
objects
identified
by
the
transition
from
do
k
to
do
k
1
we
assume
that
the
operations
previously
shown
are
atomic
no
operation
can
occur
during
the
execution
of
another
operation
so
as
to
avoid
concurrency
issues
in
conclusion
this
mechanism
provides
us
with
a
means
to
clearly
identify
which
objects
are
involved
in
a
given
operation
when
they
can
be
utilized
and
the
ordering
among
the
different
events
involved
by
an
operation
the
definitions
of
collection
c
and
repository
provided
by
goncalves
et
al
2004a
p
295
to
a
certain
extent
resemble
the
proposed
mechanism
indeed
they
model
the
storing
of
a
digital
object
in
the
repository
as
the
transition
from
the
original
repository
to
a
repository
which
manages
a
collection
augmented
by
the
newly
inserted
digital
object
similarly
the
deletion
of
a
digital
object
is
modeled
as
the
transition
from
the
original
repository
to
a
repository
which
manages
a
smaller
collection
without
the
deleted
digital
object
on
the
other
hand
the
definitions
introduced
by
goncalves
et
al
2004a
do
not
explicitly
take
into
account
and
model
the
temporal
dimension
involved
with
these
operations
in
this
sense
we
can
consider
our
mechanism
an
extension
of
the
proposal
made
by
goncalves
et
al
2004a
furthermore
our
mechanism
extends
formalizes
and
makes
more
explicit
what
rigaux
and
spyratos
2004
p
421
left
implicit
when
they
said
in
order
to
define
a
document
formally
we
assume
the
existence
of
a
countably
infinite
set
d
whose
elements
are
used
by
all
authors
for
identifying
the
created
documents
in
fact
we
assume
that
the
creation
of
a
document
is
tantamount
to
choosing
a
new
element
from
d
indeed
the
set
d
used
by
rigaux
and
spyratos
2004
corresponds
to
the
set
udo
of
definition
4
1
and
the
creation
of
a
document
corresponds
to
the
transition
from
do
k
to
do
k
1
where
an
object
from
udo
is
chosen
and
is
added
to
do
k
thus
creating
do
k
1
as
explained
above
furthermore
we
provide
a
formal
mechanism
for
describing
the
deletion
and
the
modification
of
a
digital
object
as
well
in
the
following
sections
we
will
use
the
notation
do
k
which
explicitly
points
out
the
time
dimension
only
when
needed
otherwise
we
will
use
the
simpler
notation
do
without
explicitly
pointing
out
the
time
dimension
we
will
also
use
a
similar
notation
for
the
other
sets
we
will
define
in
the
following
4
3
handle
according
to
the
previous
discussion
we
can
assume
that
each
digital
object
is
identified
by
a
unique
handle
which
is
a
name
assigned
to
a
digital
object
to
identify
and
to
facilitate
the
referencing
process
to
the
digital
object
over
the
past
years
various
syntaxes
mechanisms
and
systems
have
been
developed
to
provide
handles
or
identifiers
for
digital
objects
--
uniform
resource
identifier
uri
is
a
compact
string
of
characters
for
identifying
an
abstract
or
physical
resource
berners-lee
1994b
kunze
1995
berners-lee
et
al
1998
mealling
and
denenberg
2002
the
term
url
refers
to
the
subset
of
uris
that
identify
resources
via
a
representation
of
their
primary
access
mechanism
e
g
their
network
location
rather
than
identifying
the
resource
by
name
or
by
some
other
attribute
s
of
that
resource
the
term
uniform
resource
name
urn
refers
to
the
subset
of
uris
that
are
required
to
remain
globally
unique
and
persistent
even
when
the
resource
ceases
to
exist
or
becomes
unavailable
berners-lee
et
al
1998
--digital
object
identifier
doi
is
a
system
that
provides
a
mechanism
to
interoperably
identify
and
exchange
intellectual
property
in
the
digital
environment
doi
conforms
to
a
uri
and
provides
an
extensible
framework
for
managing
intellectual
content
based
on
proven
standards
of
digital
object
architecture
and
intellectual
property
management
furthermore
it
is
an
open
system
based
on
non-proprietary
standards
paskin
2006
--
openurl
aims
at
standardizing
the
construction
of
packages
of
information
and
the
methods
by
which
they
may
be
transported
over
networks
niso
2005
therefore
openurl
is
a
standard
syntax
for
transporting
information
metadata
and
identifiers
about
one
or
multiple
resources
within
urls
i
e
it
provides
a
syntax
for
encoding
metadata
and
identifiers
limited
to
the
world
of
urls
paskin
2006
--
persistent
url
purl
12
instead
of
pointing
directly
to
the
location
of
an
internet
resource
a
purl
points
to
an
intermediate
resolution
service
that
associates
the
purl
with
the
actual
url
and
returns
that
url
to
the
client
as
a
standard
http
redirect
the
client
can
then
complete
the
url
transaction
in
the
normal
fashion
--
purl-based
object
identifier
poi
13
is
a
simple
specification
for
resource
identifiers
based
on
the
purl
system
and
closely
related
to
the
use
of
the
open
archives
initiative
protocol
for
metadata
harvesting
oai-pmh
defined
by
the
open
archives
initiative
oai
14
oai
2004
the
poi
is
a
relatively
persistent
identifier
for
resources
that
are
described
by
metadata
items
in
oai-compliant
repositories
the
following
definition
introduces
the
notion
of
handle
compatible
with
the
mechanisms
described
above
and
its
relationship
with
digital
objects
definition
4
2
h
is
a
set
of
handles
such
that
h
do
and
h
h
is
a
generic
handle
uh
is
a
universe
set
of
handles
which
is
the
set
of
all
the
possible
handles
such
that
u
h
udo
it
follows
that
h
u
h
we
define
a
bijective
function
h
u
h
udo
which
maps
a
handle
to
the
digital
object
identified
by
it
the
relationship
between
the
sets
h
and
u
h
is
the
same
as
the
one
between
the
sets
do
and
udo
described
in
section
4
2
5
cooperation
in
order
to
provide
users
with
annotations
as
an
effective
cooperation
tool
we
need
to
proceed
as
follows
first
we
need
to
define
the
notion
of
user
group
of
users
and
author
as
described
in
section
5
1
then
we
have
to
deal
with
both
scopes
of
annotation
as
explained
in
section
5
3
and
various
access
permissions
as
introduced
in
section
5
2
the
following
definitions
do
not
aim
at
building
a
new
authentication
and
authorization
scheme
from
scratch
since
many
such
schemes
already
exist
on
the
contrary
since
a
formal
model
needs
to
be
self-contained
and
coherent
they
aim
at
providing
users
of
this
model
with
a
basic
infrastructure
for
plugging
into
their
preferred
authentication
and
authorization
scheme
perhaps
extending
this
model
with
the
additional
concepts
peculiar
to
the
chosen
scheme
5
1
user
group
of
users
and
author
definition
5
1
let
usr
be
a
set
of
users
and
usr
usr
is
a
generic
user
uu
s
r
is
a
universe
set
of
users
which
is
the
set
of
all
the
possible
users
so
that
u
s
r
uu
s
r
g
r
2u
s
r
is
a
set
of
groups
of
users
and
g
g
r
is
a
generic
group
of
users
ug
r
2uu
s
r
is
a
universe
set
of
groups
of
users
which
is
the
set
of
all
the
possible
groups
of
users
so
that
g
r
ug
r
we
define
a
function
gr
u
s
r
2g
r
which
maps
a
user
to
the
groups
of
users
he
belongs
to
the
following
constraint
must
be
adhered
to
usr
usr
gr
usr
each
user
in
usr
must
belong
to
at
least
one
group
of
users
the
relationship
between
the
sets
usr
and
g
r
and
the
sets
uu
s
r
and
ug
r
is
the
same
as
the
one
between
the
sets
do
and
udo
described
in
section
4
2
note
that
the
constraint
on
the
gr
function
can
be
also
expressed
as
usr
u
s
r
g
g
r
usr
g
obviously
a
user
may
belong
to
more
than
one
group
of
users
since
g
r
is
a
subset
of
the
power
set
of
usr
or
equivalently
since
gr
usr
is
an
element
of
the
power
set
of
g
r
digital
objects--both
documents
and
annotations--always
have
at
least
one
author
who
authored
them
therefore
the
author
is
a
specialization
of
the
more
general
concept
of
user
introduced
in
the
definition
above
an
author
is
a
user
who
authored
one
or
more
digital
objects
definition
5
2
let
us
define
a
function
au
usr
2
h
which
maps
a
user
to
the
handles
of
the
digital
objects
authored
by
him
let
the
set
of
authors
au
be
the
following
set
au
usr
usr
au
usr
we
denote
with
au
au
usr
a
generic
author
the
following
constraint
must
be
adhered
to
h
h
au
au
h
au
au
each
digital
object
must
be
authored
by
at
least
one
author
the
function
au
characterizes
the
authors
distinguishing
them
from
generic
users
indeed
if
a
generic
user
usr
u
s
r
has
not
authored
any
digital
object
it
follows
that
au
usr
and
thus
usr
au
in
general
a
digital
object
may
have
more
than
one
author
in
other
words
there
may
exist
au1
au2
au
au
au1
au
au2
on
the
other
hand
an
author
may
author
more
than
one
digital
object
since
au
au
is
an
element
of
the
power
set
of
h
these
definitions
allow
us
to
organize
the
users
according
to
their
roles
and
qualifications
consider
for
example
the
case
of
a
genomic
annotation
that
relies
on
specially
trained
biologists
whose
job
it
is
to
create
highly
valuable
and
domain-specific
annotations
these
biologists
may
be
users
who
belong
to
a
group
of
curators
which
grants
them
the
necessary
access
rights
for
carrying
out
their
curatorial
role
finally
note
that
these
proposed
definitions
share
similarities
with
the
notion
of
society
proposed
by
goncalves
et
al
2004a
p
275
when
they
say
a
society
is
a
set
of
entities
and
the
relationships
between
them
the
above
definitions
simply
introduce
the
set
of
entities
that
come
into
play
plus
the
distinction
between
users
and
authors
which
represents
a
first
kind
of
relationship
between
those
entities
furthermore
the
definitions
in
the
following
sections
will
enable
the
introduction
of
more
relationships
between
users
and
groups
especially
with
respect
to
the
access
management
issue
which
is
part
of
what
goncalves
et
al
2004a
p
276
call
societal
governance
5
2
permission
as
discussed
in
section
3
an
annotation
can
have
different
access
permissions
--
denied
if
no
access
is
allowed
--
read
only
if
it
can
be
accessed
only
for
reading
--
read
and
write
if
it
can
be
read
modified
and
deleted
definition
5
3
let
p
denied
readonly
readwrite
be
a
set
of
access
permissions
and
p
p
is
an
access
permission
let
us
define
the
following
relations
--
equality
relation
p
p
p
p
p
p
denied
denied
readonly
readonly
readwrite
readwrite
--strict
ordering
relation
denied
readonly
denied
readwrite
readonly
readwrite
--
ordering
relation
p1
p2
p
p
p1
p2
p1
p2
in
contrast
to
the
set
of
the
previous
definitions
the
set
of
access
permissions
p
is
a
time-invariant
set
which
does
not
need
the
notation
for
taking
into
account
the
temporal
dimension
indeed
we
assume
that
an
annotation
can
only
have
the
access
permissions
previously
listed
note
that
p
is
a
totally
ordered
set
5
3
scope
as
discussed
in
section
3
an
annotation
can
have
one
of
the
following
scopes
which
are
mutually
exclusive
--
private
when
it
can
be
accessed
only
by
its
own
author
--
shared
when
it
can
be
accessed
only
by
a
desired
set
of
groups
of
users
--public
when
it
can
be
accessed
by
all
users
the
different
scopes
of
an
annotation
can
be
partnered
with
the
various
access
permissions
introduced
here
in
order
to
obtain
the
necessary
access
policies
for
example
in
the
case
of
a
public
annotation
we
might
decide
that
every
user
holds
read
only
access
to
it
while
its
author
has
read-
and
write-access
to
it
in
addition
to
the
policy
just
described
we
might
also
add
a
list
of
groups
of
users
each
one
with
its
specific
access
permission
in
order
to
pinpoint
finegrained
access
permission
even
in
the
context
of
a
public
annotation
therefore
the
proposed
formal
model
does
not
aim
at
describing
a
specific
access
policy
but
at
providing
the
means
for
describing
the
different
access
policies
one
might
need
to
deal
with
definition
5
4
let
sp
private
shared
public
be
a
set
of
scopes
and
sp
s
p
is
a
scope
let
us
define
the
following
relations
--equality
relation
sp
sp
s
p
s
p
sp
s
p
private
private
shared
shared
public
public
--
strict
ordering
relation
private
shared
private
public
shared
public
--
ordering
relation
sp1
sp2
s
p
s
p
sp1
sp2
sp1
sp2
as
in
the
case
of
the
set
of
access
permissions
the
set
of
scopes
s
p
is
also
a
time-invariant
set
because
we
assume
that
an
annotation
can
have
only
one
of
the
three
scopes
listed
above
note
that
s
p
is
a
totally
ordered
set
6
linking
in
order
to
link
annotations
to
digital
objects
and
to
correctly
anchor
annotations
to
digital
objects
we
need
to
proceed
as
follows
first
we
need
to
choose
a
linking
mechanism
and
define
the
link
types
that
can
exist
between
annotations
and
digital
objects
as
described
in
section
6
1
then
since
annotations
are
usually
linked
to
specific
parts
of
a
digital
object
we
need
to
model
the
content
of
digital
objects
as
explained
in
section
6
2
finally
a
suitable
anchoring
mechanism
for
annotations
has
to
be
provided
as
introduced
in
section
6
3
6
1
linking
annotations
to
digital
objects
handles
can
be
used
not
only
for
the
purpose
of
uniquely
identifying
a
digital
object
but
they
can
also
provide
us
with
a
means
for
linking
an
annotation
to
a
digital
object
this
use
of
handles
is
particularly
clear
if
we
think
about
urls
but
it
is
also
still
valid
in
the
case
of
the
other
types
of
handles
presented
in
section
4
3
once
we
have
decided
to
use
handles
as
a
basic
mechanism
for
linking
annotations
to
digital
objects
we
still
have
to
consider
the
kind
of
links
an
annotation
can
have
with
a
digital
object
annotations
can
be
linked
to
digital
objects
with
two
main
types
of
links
--
annotate
link
an
annotation
annotates
a
digital
object
which
can
be
a
document
or
another
annotation
the
annotate
link
is
intended
to
allow
an
annotation
to
only
annotate
one
or
more
parts
of
a
given
digital
object
therefore
this
kind
of
link
lets
the
annotation
express
intra-digital
object
relationships
meaning
that
the
annotation
creates
a
relationship
among
the
different
parts
of
the
annotated
digital
object
--
relate-to
link
an
annotation
relates
to
a
digital
object
which
can
be
a
document
or
another
annotation
the
relate-to
link
is
intended
to
allow
an
annotation
to
only
relate
to
one
or
more
parts
of
other
digital
objects
but
not
the
annotated
one
therefore
this
kind
of
link
lets
the
annotation
express
inter-digital
object
relationships
meaning
that
the
annotation
creates
a
relationship
between
the
annotated
digital
object
and
the
other
digital
objects
related
to
it
with
respect
to
these
two
main
types
of
link
we
introduce
the
following
constraint
an
annotation
must
annotate
one
and
only
one
digital
object
which
can
be
either
a
document
or
another
annotation--an
annotation
must
have
one
and
only
one
annotate
link
this
constraint
means
that
an
annotation
can
be
created
only
for
the
purpose
of
annotating
a
digital
object
and
not
exclusively
for
relating
to
a
digital
object
an
annotation
then
can
annotate
one
and
only
one
digital
object
because
the
annotate
link
expresses
intra-digital
object
relationships
and
thus
it
cannot
be
mutual
to
multiple
digital
objects
different
from
the
annotated
one
finally
this
constraint
does
not
prevent
the
annotation
from
relating
to
more
than
one
digital
object
from
having
more
than
one
relate-to
link
this
situation
is
very
similar
to
what
happens
in
the
real
world
when
we
deal
with
paper
documents
we
can
annotate
one
or
more
parts
of
the
document
that
we
have
at
hand
this
document
also
provides
us
with
the
physical
medium
for
writing
the
content
of
the
annotation
on
the
other
hand
the
content
of
the
annotation
can
contain
references
to
other
documents
in
other
words
it
can
relate
the
document
at
hand
to
other
documents
that
are
currently
being
viewed
therefore
the
act
of
annotating
concerns
one
and
only
one
document
to
which
the
annotation
is
anchored
although
there
may
be
one
or
more
references
that
relate
the
annotation
to
other
documents
one
could
argue
that
in
the
digital
world
these
limitations
could
be
overcome
and
that
an
annotation
could
annotate
multiple
documents
at
the
same
time
apart
from
being
possible
what
could
we
gain
from
this
option
if
we
allow
multiple
annotate
links
we
are
going
to
add
some
uncertainty
because
the
annotation
would
lose
its
strong
relationships
with
only
one
object
in
fact
this
object
represents
its
main
purpose
while
linking
the
annotation
to
multiple
objects
would
give
us
unclear
semantics
therefore
we
opt
for
constraining
the
link
types
that
an
annotation
can
have
and
the
following
definition
introduces
the
set
of
allowed
link
types
definition
6
1
let
lt
be
a
set
of
link
types
an
element
lt
lt
corresponds
to
one
of
the
allowed
link
types
the
set
lt
contains
the
following
link
types
lt
annotate
relateto
as
in
the
case
of
the
set
of
access
permissions
and
the
set
of
scopes
the
set
of
link
types
lt
is
also
a
time-invariant
set
because
we
assume
that
an
annotation
can
be
linked
to
digital
objects
only
with
the
link
types
listed
here
6
2
stream
digital
objects
can
be
very
different--texts
images
audio
videos
hypertexts
multimedia
objects
and
so
on--and
the
way
in
which
their
structure
and
content
is
modeled
and
expressed
can
also
widely
vary
across
different
conceptual
and
logical
models
of
dl
and
digital
object
nevertheless
many
of
these
types
of
models
share
the
idea
that
beyond
representing
the
structure
of
the
digital
object
the
model
also
has
to
take
into
account
a
mechanism
for
representing
the
actual
content
of
the
digital
object
for
example
navarro
and
baeza-yates
1997
and
goncalves
et
al
2004a
both
use
the
notion
of
stream
which
is
an
ordered
sequence
of
symbols
representing
the
actual
content
of
a
digital
object
or
part
of
it
bottoni
et
al
2003
define
the
content
of
a
digital
object
as
a
function
from
a
set
of
indices
to
a
set
representing
the
vocabulary
of
the
symbols
finally
castelli
and
pagano
2002a
2002b
associate
each
digital
object
with
many
different
manifestation
entities
which
represent
sequences
of
bytes
and
files
containing
different
parts
of
the
digital
object
itself
the
following
definition
introduces
the
concept
of
stream
in
order
to
represent
the
actual
content
of
a
digital
object
or
a
part
of
it
the
definition
of
stream
is
inspired
by
navarro
and
baeza-yates
1997
and
goncalves
et
al
2004a
but
with
some
differences
which
will
be
discussed
in
the
following
definition
6
2
a
stream
sm
is
a
finite
sequence
sm
i
1
2
n
n
n
where
is
the
alphabet
of
symbols
we
allow
the
existence
of
an
empty
stream
esm
s
m
is
a
set
of
streams
and
sm
s
m
is
a
stream
u
s
m
is
a
universe
set
of
streams
that
is
the
set
of
all
the
possible
streams
it
follows
that
s
m
us
m
we
define
a
function
hsm
h
2
s
m
which
maps
a
handle
of
a
digital
object
to
the
streams
contained
in
that
digital
object
the
following
constraint
must
be
adhered
to
h
h
hsm
h
each
digital
object
must
contain
at
least
one
stream
which
could
also
possibly
be
the
empty
stream
the
relationship
between
the
sets
s
m
and
u
s
m
is
the
same
as
the
one
between
the
sets
do
and
udo
described
in
section
4
2
the
stream
is
required
to
be
neither
a
surjective
nor
an
injective
function
we
can
exploit
the
non
surjectivity
of
the
stream
in
order
to
use
standard
sets--
characters
numbers
and
so
on--as
a
codomain
for
a
stream
otherwise
if
the
function
were
constrained
to
be
surjective
we
would
be
forced
to
use
an
ad
hoc
codomain
for
each
different
stream
on
the
other
hand
since
the
stream
is
not
an
injective
function
it
is
therefore
not
invertible
in
fact
when
given
a
symbol
we
cannot
trace
this
symbol
back
to
its
position
within
the
stream
for
example
if
we
consider
the
following
piece
of
text
1
2
3
4
5
6
7
8
9
10
title
then
we
can
define
the
stream
sm
i
1
2
10
a
b
z
a
b
z
such
that
sm
1
t
sm
2
i
sm
10
t
note
that
if
the
stream
was
constrained
to
be
surjective
we
should
use
a
codomain
constituted
only
by
t
i
t
l
e
x
in
any
case
the
letters
of
the
piece
of
text
shown
above
from
a
given
symbol
for
example
t
we
cannot
unambiguously
determine
its
position
in
the
stream
because
the
stream
is
not
injective--
t
is
given
by
both
sm
3
and
sm
10
in
particular
we
can
distinguish
two
main
kinds
of
streams
--
logical
stream
lsm
this
is
a
stream
in
which
each
element
represents
a
logical
symbol
within
the
stream
--
physical
stream
psm
this
is
a
stream
in
which
each
element
represents
a
physical
symbol
within
the
stream
now
we
will
discuss
the
distinction
between
logical
and
physical
streams
by
means
of
an
example
although
the
map
shown
between
natural
numbers
and
letters
is
quite
intuitive
it
should
be
pointed
out
that
the
elements
of
the
set
are
symbols
that
abstract
the
underlying
encoding
of
the
text
for
example
if
we
consider
an
ascii
text
each
element
of
the
set
corresponds
to
exactly
one
byte
in
the
physical
text
stream
thus
we
should
use
the
codomain
ascii
text
4116
4216
5a16
6116
6216
7a16
2016
nstead
of
to
represent
the
actual
stream
on
the
other
hand
in
the
case
of
a
unicode
text
each
element
of
the
set
corresponds
to
two
bytes
in
the
physical
text
stream
thus
we
should
use
the
codomain
unicode
0016
4116
0016
4216
0016
5a16
0016
6116
0016
6216
0016
7a16
0016
2016
instead
of
in
this
latter
case
we
are
forced
to
define
the
elements
of
unicode
as
two-byte
pairs
in
order
to
map
the
indices
of
i
into
the
symbols
of
unicode
as
a
result
we
would
not
be
able
to
access
each
byte
individually
if
we
wish
to
access
each
byte
of
the
unicode
stream
we
should
define
the
following
domain
i
1
2
20
and
codomain
unicode
0016
4116
4216
5a16
6116
6216
7a16
2016
for
the
stream
but
in
this
case
we
would
lose
the
correspondence
with
the
ten
letters
of
the
piece
of
text
because
two
indices
in
i
would
correspond
to
each
letter
of
the
piece
of
text
this
example
demonstrates
that
on
the
one
hand
we
have
a
logical
stream
which
represents
the
piece
of
text
and
on
the
other
there
are
one
or
more
physical
streams
that
represent
the
physical
encoding
of
the
piece
of
text
shown
similar
and
even
more
complex
considerations
can
be
made
in
the
case
of
audio
image
and
video
streams
where
the
complexity
of
such
streams
increases
the
choices
available
for
representing
them
both
in
logical
and
in
physical
terms
another
example
is
the
compression
of
streams
where
more
symbols
in
one
stream
correspond
to
fewer
symbols
in
the
other
stream
this
observation
points
out
the
need
to
carefully
define
the
level
of
abstraction
of
a
stream
and
the
degree
of
detail
that
need
to
be
adopted
when
defining
streams
in
other
words
should
we
model
the
physical
encoding
of
a
stream
or
some
more
abstract
representation
of
that
stream
depending
on
the
case
both
levels
of
abstraction
may
be
necessary
for
example
when
we
do
a
macrocomparison
of
two
digital
libraries
we
can
use
more
abstract
streams
on
the
other
hand
if
we
want
to
precisely
describe
the
functioning
of
some
component
of
a
digital
library
as
a
repository
we
need
to
use
streams
that
better
represent
the
physical
encoding
of
the
objects
in
the
repository
however
past
experience
in
the
field
of
dbms
teaches
us
that
it
is
better
to
keep
the
logical
and
the
physical
levels
distinct
this
is
why
we
want
to
distinguish
between
logical
and
physical
streams
note
that
navarro
and
baeza-yates
1997
make
use
of
logical
streams
but
they
do
not
specify
much
about
physical
streams
leaving
them
to
the
implementation
of
the
system
neither
bottoni
et
al
2003
nor
goncalves
et
al
2004a
addressed
this
problem
at
all
but
in
goncalves
and
fox
2002
it
turns
out
that
streams
are
essentially
identified
by
multipurpose
internet
mail
extensions
mime
types
freed
and
borenstein
1996a
1996a
1996b
moore
1996
freed
et
al
1996
and
thus
they
are
substantially
physical
streams
finally
since
the
notion
of
manifestation
used
by
castelli
and
pagano
2002b
2002a
refers
to
a
physical
file
holding
part
of
the
content
of
a
digital
object
these
authors
essentially
use
physical
streams
also
and
in
a
way
that
resembles
the
implementation
of
streams
by
goncalves
and
fox
2002
we
believe
that
the
research
field
of
digital
libraries
also
needs
to
clearly
distinguish
between
the
logical
and
physical
levels
this
distinction
is
a
prerequisite
for
each
formal
model
of
dl
that
aims
to
be
sufficiently
clear
expressive
and
flexible
moreover
an
explicit
and
formal
mechanism
for
modeling
the
relationship
between
logical
and
physical
streams
and
the
properties
of
such
a
relationship
is
needed
this
will
be
the
focus
of
the
next
definition
definition
6
3
given
two
streams
sm1
sm2
s
m
let
us
define
a
stream
mapping
relation
smr
sm1
sm2
i
j
ism1
ism2
sm1
i
is
mapped
to
sm2
j
let
us
define
a
stream
mapping
set
sms
smr
sm1
sm2
i
such
that
1
sm
s
m
smr
sms
smr
sm
sm
i
j
ism
ism
i
j
2
smr
sm1
sm2
sms
smr
sm2
sm1
sms
smr
sm2
sm1
smr-1
sm1
sm2
j
i
ism2
ism1
i
j
smr
sm1
sm2
3
smr
sm1
sm2
smr
sm2
sm3
sms
smr
sm1
sm3
sms
smr
sm1
sm3
smr
sm1
sm2
smr
sm2
sm3
i
k
ism1
ism3
i
j
smr
sm1
sm2
j
k
smr
sm2
sm3
let
us
define
a
stream
mapping
set
indicator
function
sms
sm1
sm2
1
if
smr
sm1
sm2
sms
0
if
smr
sm1
sm2
sms
each
element
i
j
smr
sm1
sm2
represents
the
fact
that
the
i-th
symbol
in
the
first
stream
is
related
to
the
j
-th
symbol
in
the
second
stream
the
smr
relation
represents
and
embeds
the
algorithm
that
allows
us
to
map
symbols
of
one
stream
into
those
of
the
other
in
particular
in
the
case
of
the
relationship
between
logical
and
physical
streams
the
smr
relation
represents
the
fact
that
given
logical
symbols
are
encoded
with
given
physical
symbols
in
this
way
it
clearly
models
the
distinction
and
the
passage
from
the
logical
to
the
physical
level
for
example
the
smr
relation
could
represent
the
mapping
between
the
pixels
of
an
image
an
its
joint
photographic
experts
group
jpeg
encoding
in
general
the
stream
mapping
relation
allows
us
to
express
many-to-many
relationships
between
symbols
of
two
streams
in
particular
we
are
interested
in
expressing
at
least
the
following
relationships
--
a
one-to-one
relationship
between
the
symbols
of
the
two
streams
as
in
the
previous
example
of
the
piece
of
text
and
its
ascii
encoding
--
a
one-to-many
relationship
between
the
symbols
of
the
two
streams
as
in
the
previous
example
of
the
piece
of
text
and
its
unicode
encoding
--
a
many-to-one
relationship
between
the
symbols
of
the
two
streams
as
in
the
case
of
compression
of
one
stream
into
another
the
stream
mapping
relation
provides
us
with
a
further
degree
of
freedom
since
we
can
have
symbols
in
a
stream
that
do
not
correspond
to
another
stream
in
this
way
we
can
model
some
kind
of
loss
of
information
due
to
different
encodings
finally
the
stream
mapping
relation
enables
the
same
logical
symbol
to
be
encoded
in
different
ways
according
to
its
position
in
the
stream
consider
the
letter
t
which
in
the
previous
example
appears
in
the
third
and
tenth
position
of
the
stream
it
could
be
encoded
in
two
different
ways
if
we
apply
some
compression
algorithm
to
that
stream
definition
6
3
allows
us
to
associate
a
set
of
physical
streams
to
the
same
logical
stream
providing
us
with
a
mechanism
to
enable
different
encodings
of
the
same
logical
stream
we
could
also
create
a
chain
of
streams
we
could
specify
that
a
logical
stream
is
encoded
with
a
given
physical
stream
and
that
this
physical
stream
is
mapped
to
another
physical
stream
and
so
on
these
observations
led
us
to
introduce
the
stream
mapping
set
which
contains
the
stream
mapping
relations
and
holds
the
intuitive
and
expected
properties
for
this
kind
of
set
--
reflexive
for
each
stream
the
obvious
mapping
of
the
stream
to
itself
exists
--
symmetric
if
we
know
how
to
map
one
stream
to
another
we
can
also
map
the
second
stream
back
to
the
first
one
--
transitive
if
we
know
the
mapping
between
one
stream
and
another
and
we
also
know
the
mapping
between
the
second
stream
and
a
third
we
know
how
to
map
the
first
to
the
third
now
we
can
study
the
impact
definition
6
3
has
on
the
set
of
streams
s
m
and
how
it
contributes
to
enforcing
the
distinction
between
the
logical
and
the
physical
levels
proposition
6
4
the
following
relation
sms
sm1
sm2
s
m
s
m
s
m
s
sm1
sm2
1
is
an
equivalence
relation
on
the
set
of
streams
s
m
the
sets
sm
sm1
sm2
s
m
sm1
sm2
sms
are
the
equivalence
classes
of
all
the
streams
of
s
m
that
are
mapped
one
to
another
and
s
m/sm
is
the
quotient
set
proof
the
relation
is
--
reflexive
sm
s
m
smr
sm
sm
s
m
s
s
m
s
sm
sm
1
--
symmetric
sm1
sm2
s
m
s
m
s
sm1
sm2
1
smr
sm2
sm1
s
m
s
s
m
s
sm2
sm1
1
--
transitive
sm1
sm2
sm3
s
m
s
m
s
sm1
sm2
s
m
s
sm2
sm3
1
smr
sm1
sm3
s
m
s
s
m
s
sm1
sm3
1
therefore
it
is
an
equivalence
relation
we
can
choose
the
logical
stream
as
representative
of
the
equivalence
class
the
sms
equivalence
relation
allows
us
to
deal
only
with
logical
streams
in
fact
it
removes
us
from
the
physical
level
and
hides
the
details
of
the
representation
and
the
encoding
of
logical
streams
into
physical
ones
therefore
the
sms
equivalence
relation
enforces
the
distinction
between
the
logical
and
the
physical
levels
and
provides
us
with
the
means
of
working
and
reasoning
at
a
logical
level
clearly
separating
it
from
the
physical
one
furthermore
we
can
iterate
this
line
of
reasoning
and
use
this
equivalence
relation
as
a
basic
mechanism
for
introducing
further
levels
of
abstraction
and
creating
a
kind
of
hierarchy
among
streams
indeed
on
the
quotient
set
sm/sm
we
could
introduce
an
equivalence
relation
similar
to
sms
in
order
to
express
the
fact
that
two
or
more
logical
streams
can
be
mapped
to
each
other
this
is
how
we
can
maintain
more
abstract
classes
of
equivalent
logical
streams
on
the
quotient
set
sm/sm
by
keeping
them
distinct
from
the
different
ways
in
which
they
can
be
encoded
this
different
encoding
is
in
turn
represented
by
the
less
abstract
equivalence
classes
on
the
set
s
m
this
procedure
can
be
repeated
as
many
times
as
needed
in
relation
to
the
number
of
levels
of
abstraction
for
example
suppose
we
have
a
piece
of
text
that
can
be
represented
either
as
a
sequence
of
characters
or
as
a
scanned
image
these
are
two
different
logical
streams
that
can
be
encoded
with
many
different
physical
streams
in
this
case
a
first
level
of
abstraction
is
to
put
all
the
physical
streams
that
encode
the
character
streams
into
one
equivalence
class
created
on
sm
and
all
the
physical
streams
that
encode
the
image
stream
into
another
equivalence
class
created
on
sm
however
a
higher
level
of
abstraction
is
to
put
both
the
equivalence
class
of
the
text
stream
and
the
equivalence
class
of
the
image
stream
into
a
new
and
more
abstract
equivalence
class
created
on
s
m/sm
in
order
to
express
the
fact
that
both
of
them
are
representations
of
the
same
piece
of
text
for
all
these
reasons
definition
6
3
and
proposition
6
4
constitute
a
step
forward
with
respect
to
previous
models
castelli
and
pagano
2002a
2002b
goncalves
et
al
2004a
2004b
navarro
and
baeza-yates
1997
bottoni
et
al
2003
which
only
partially
address
this
issue
or
do
not
address
it
at
all
on
the
other
hand
definitions
6
2
6
3
and
proposition
6
4
are
fully
compatible
with
the
definition
of
stream
provided
by
both
navarro
and
baeza-yates
1997
and
goncalves
et
al
2004a
thus
we
can
utilize
the
proposed
distinction
between
logical
and
physical
streams
in
both
the
models
provided
by
navarro
and
baezayates
1997
and
goncalves
et
al
2004a
in
order
to
extend
such
models
if
necessary
6
3
segment
the
handles
discussed
in
section
4
3
may
be
capable
not
only
of
uniquely
identifying
a
digital
object
but
also
of
indicating
a
part
of
the
identified
digital
object
for
example
a
url
can
point
to
any
given
anchor
within
a
html
document
or
we
can
use
an
xpath
expression
to
point
to
a
specific
element
within
an
xml
document
on
the
other
hand
parts
of
a
digital
object
cannot
always
be
identified
with
an
arbitrary
degree
of
detail
for
example
a
url
cannot
point
to
a
given
word
of
a
html
document
if
this
word
is
not
marked
with
an
anchor
therefore
we
need
some
further
mechanism
for
identifying
parts
of
a
digital
object
with
the
necessary
degree
of
detail
the
following
definition
introduces
the
notion
of
segment
which
is
a
mechanism
for
selecting
parts
of
a
stream
this
mechanism
can
be
partnered
with
the
handle
of
a
digital
object
to
provide
access
to
a
digital
object
with
the
necessary
degree
of
detail
definition
6
5
given
a
stream
sm
i
1
2
n
s
m
a
segment
is
a
pair
stsm
a
b
1
a
b
n
a
b
n
n
n
sm
a
stream
segment
is
a
restriction
sm
a
b
of
the
stream
sm
to
interval
a
b
associated
with
the
segment
stsm
st
is
a
set
of
segments
and
stsm
st
is
a
generic
segment
u
st
is
a
universe
set
of
segments
which
is
the
set
of
all
the
possible
segments
so
that
st
u
st
the
relationship
between
the
sets
st
and
u
st
is
the
same
as
the
relationship
between
the
sets
do
and
udo
described
in
section
4
2
definition
6
5
resembles
the
definition
of
segment
provided
in
navarro
and
baeza-yates
1997
and
goncalves
et
al
2004a
we
can
assume
that
logically
related
symbols
of
logical
streams
are
contiguous
and
are
in
an
ascending
order
this
assumption
goes
well
with
definition
6
5
which
selects
a
series
of
contiguous
symbols
on
the
other
hand
definition
6
3
allows
us
to
disregard
this
constraint
for
the
mapping
to
physical
streams
since
the
stream
mapping
relation
smr
allows
us
to
map
the
contiguous
symbols
of
the
logical
stream
to
noncontiguous
symbols
of
the
physical
stream
for
example
the
indices
ilsm
1
2
3
4
5
of
a
logical
stream
could
be
mapped
to
the
indices
ipsm
13
7
19
9
15
of
a
physical
stream
if
we
choose
the
segment
stlsm
2
4
which
is
associated
with
the
interval
2
4
for
the
logical
stream
we
are
not
forced
to
map
it
to
the
interval
7
9
obtained
by
mapping
the
segment
stlsm
to
a
corresponding
segment
stpsm
7
9
of
the
physical
stream
on
the
contrary
we
can
map
each
index
in
the
interval
2
4
to
its
corresponding
index
in
the
physical
stream
obtaining
the
set
of
indices
7
19
9
which
do
not
fit
in
the
interval
7
9
see
navarro
and
baeza-yates
1997
for
further
explanation
about
ordering
in
multimedia
streams
this
feature
is
important
because
symbols
that
are
contiguous
in
a
logical
stream
can
correspond
to
non-contiguous
symbols
in
a
physical
stream
due
to
some
kind
of
compression
for
example
in
addition
proposition
6
4
allows
us
to
reason
only
in
terms
of
logical
streams
that
comply
with
the
assumption
made
above
without
worrying
about
the
physical
streams
that
are
in
the
same
equivalence
class
of
the
logical
stream
this
observation
further
highlights
the
benefits
that
may
arise
by
clearly
distinguishing
between
the
logical
and
the
physical
levels
all
of
the
introduced
concepts
namely
handle
stream
and
segment
provide
us
with
the
formal
means
needed
to
deal
with
the
linking
and
anchoring
problem
related
to
annotations
by
using
a
handle
h
we
can
link
an
annotation
to
a
digital
object
then
the
function
hsm
h
allows
us
to
select
the
desired
stream
sm
of
the
digital
object
identified
by
h
be
it
a
physical
or
a
logical
view
of
the
actual
content
of
the
digital
object
finally
a
segment
stsm
enables
the
fine-tuned
anchoring
of
the
annotation
to
the
digital
object
last
we
can
also
rely
on
these
concepts
to
address
the
annotation
repositioning
problem
that
arises
when
the
content
of
annotated
digital
objects
changes
indeed
the
introduced
concepts
offer
us
the
possibility
of
modeling
what
phelps
and
wilensky
2000b
call
robust
location
which
are
redundant
descriptors
of
locations
within
a
digital
object
created
by
using
a
number
of
different
data
records
moreover
we
can
also
express
the
algorithms
they
propose
for
reattaching
annotations
to
a
digital
object
when
the
annotated
digital
object
is
modified
furthermore
these
concepts
can
provide
us
with
a
common
grounding
not
only
for
designing
repositioning
algorithms
as
in
the
case
of
phelps
and
wilensky
2000b
but
also
for
studying
and
describing
what
users
expect
an
annotation
system
to
do
when
annotated
digital
objects
change
as
done
in
the
user
study
conducted
by
brush
et
al
2001
7
materialization
as
in
agosti
and
ferro
2003a
we
define
sign
of
annotation
as
the
basic
way
in
which
an
annotation
can
take
shape
the
way
of
representing
and
materializing
the
semantics
of
annotation
for
example
we
can
identify
the
following
basic
signs
of
annotations
--
textual
sign
a
textual
materialization
of
the
semantics
of
an
annotation
which
is
expressed
by
a
piece
of
text
added
to
a
digital
object
--graphic
sign
the
graphic
materialization
of
the
semantics
of
an
annotation
which
is
expressed
by
a
graphic
mark
added
to
a
digital
object
--video
sign
the
video
materialization
of
the
semantics
of
an
annotation
which
is
expressed
by
a
video
fragment
added
to
a
digital
object
--auditive
sign
the
auditive
materialization
of
the
semantics
of
an
annotation
which
is
expressed
by
an
audio
fragment
added
to
a
digital
object
these
basic
signs
can
be
combined
to
express
more
complex
signs
of
annotation
consider
the
example
of
figure
2
where
two
annotations
are
shown
one
in
the
upper
part
near
the
auditive
sign
bullet
and
the
other
in
the
lower
part
near
the
auditive
sign
bullet
the
first
annotation
is
constituted
by
both
a
basic
sign
and
a
compound
sign
the
highlight
is
a
basic
graphic
sign
while
the
call-out
is
a
compound
sign
it
is
in
turn
formed
by
two
graphic
signs
the
box
and
the
arrow
and
by
a
textual
sign
which
is
the
question
wouldn
t
also
it
be
useful
for
visually
impaired
people
the
second
annotation
is
made
by
three
basic
signs
two
graphic
signs
the
arrow
and
the
cloud
and
a
textual
sign
which
is
the
answer
i
think
so
in
conclusion
by
using
the
notion
of
sign
of
annotation
we
consider
the
annotation
as
possibly
complex
multimedia
constituted
by
different
parts
each
one
with
its
own
medium
the
following
definition
formally
introduces
the
concept
of
sign
of
annotation
definition
7
1
a
sign
of
annotation
is
a
stream
s
n
s
m
is
a
set
of
signs
of
annotation
and
sn
s
n
is
a
sign
u
s
n
u
s
m
is
a
universe
set
of
signs
of
annotation
which
is
the
set
of
all
the
possible
signs
of
annotation
so
that
s
n
u
s
n
the
relationship
between
the
sets
s
n
and
u
s
n
is
the
same
as
the
relationship
between
the
sets
do
and
udo
described
in
section
4
2
henceforth
we
will
use
the
term
sign
of
annotation
or
briefly
stated
as
sign
to
indicate
a
stream
that
belongs
to
an
annotation
on
the
other
hand
we
will
use
the
term
stream
to
indicate
a
stream
that
belongs
to
a
digital
object
without
the
need
of
specifying
if
the
digital
object
is
a
document
or
an
annotation
8
semantics
as
in
agosti
and
ferro
2003a
we
define
meaning
of
annotation
as
a
main
feature
of
the
concept
of
annotation
which
identifies
conceptual
differences
within
the
semantics
of
the
annotation
or
part
of
it
for
example
looking
at
the
different
points
of
view
concerning
annotations
introduced
in
section
2
we
can
see
that
they
correspond
to
different
and
very
broad
meanings
of
annotation
furthermore
we
can
identify
different
meanings
of
annotation
within
each
given
viewpoint
for
example
within
the
viewpoint
called
annotation
as
content
we
can
identify
at
least
the
following
meanings
of
annotation
but
many
others
would
be
possible
also
depending
on
specific
domains
--
comprehension
and
study
annotating
a
document
is
a
way
of
better
investigating
and
understanding
a
concept
this
process
principally
involves
a
private
scope
because
the
recipient
of
an
annotation
is
the
person
who
created
it
other
people
reading
an
annotated
document
may
benefit
from
existing
annotations
as
well
--
interpretation
and
elucidation
annotating
a
document
could
be
a
way
of
adding
comments
and
explaining
sentences
within
it
the
aim
is
to
make
it
more
comprehensible
and
to
exchange
ideas
on
a
topic
an
example
could
be
an
expert
in
literature
who
explains
and
annotates
the
divine
comedy
this
process
principally
involves
a
public
scope
and
the
recipients
of
an
annotation
are
people
who
are
not
necessarily
related
to
the
creator
of
the
annotation
--
cooperation
and
revision
a
team
of
people
could
annotate
a
document
for
various
purposes
as
they
are
working
on
a
common
document
or
they
are
reviewing
someone
else
s
work
annotating
a
text
is
thus
a
way
of
sharing
ideas
and
opinions
in
order
to
improve
a
text
this
process
principally
involves
a
shared
scope
because
the
recipient
of
an
annotation
is
a
team
of
people
working
together
on
a
given
subject
as
a
further
example
if
we
consider
annotations
as
metadata
the
meaning
of
the
annotation
could
be
provided
by
some
standard
metadata
specification
such
as
the
ones
provided
by
the
dcmi
which
deals
with
the
development
of
interoperable
online
metadata
standards
last
it
is
also
possible
to
organize
the
meanings
of
annotations
according
to
some
kind
of
hierarchy
such
as
a
taxonomy
or
an
ontology
in
order
to
provide
navigation
capabilities
among
different
meanings
of
annotation
definition
8
1
m
is
a
set
of
meanings
of
annotations
and
m
m
is
a
generic
meaning
of
annotation
the
meanings
graph
is
a
labeled
directed
graph
g
m
l
m
where
g
m
m
e
m
m
m
and
l
m
e
m
l
m
with
l
m
set
of
labels
the
meanings
function
m
s
n
2
m
associates
each
sign
of
annotation
with
its
corresponding
meanings
of
annotation
the
following
constraint
must
be
satisfied
sn
s
n
m
sn
each
sign
of
annotation
has
at
least
one
meaning
of
annotation
as
in
the
case
of
the
set
of
access
permissions
the
set
of
scopes
and
the
set
of
link
types
lt
the
set
of
meanings
m
is
a
time-invariant
set
because
we
assume
that
meanings
represent
preexisting
knowledge
that
does
not
change
over
time
therefore
all
the
needed
meanings
of
annotation
are
already
elements
of
the
set
m
the
goal
of
the
meanings
graph
is
to
provide
structure
and
hierarchy
among
the
meanings
of
annotation
in
order
to
navigate
and
browse
through
them
the
relation
e
m
can
be
constrained
in
many
ways
to
obtain
the
necessary
structure
of
meanings
which
can
represent
some
domain
specific
knowledge
the
labelling
function
l
m
can
be
further
exploited
to
distinguish
different
kinds
of
arcs
in
the
set
e
m
in
order
to
better
explain
the
kind
of
relationship
between
two
different
meanings
goncalves
et
al
2004a
introduce
the
general
notion
of
structure
in
dls
rep
resented
by
a
labeled
directed
graph
as
a
means
of
expressing
different
kinds
of
structure
that
might
be
needed
in
dls
such
as
taxonomies
metadata
and
so
on
therefore
the
meanings
graph
adheres
to
this
definition
of
structure
and
it
is
a
structure
aimed
at
enabling
navigation
through
the
different
meanings
of
annotation
the
meanings
function
allows
us
to
associate
each
sign
of
annotation
with
its
corresponding
meanings
in
order
to
clarify
the
semantics
of
the
sign
note
that
the
meanings
function
is
neither
injective
nor
surjective
in
conclusion
an
annotation
is
expressed
by
one
or
more
signs
of
annotation
which
in
turn
are
characterized
by
one
or
more
meanings
of
annotation
thus
defining
the
overall
semantics
of
the
annotation
the
explicit
distinction
between
the
meaning
and
the
sign
of
annotation
is
quite
new
in
the
field
of
annotations
indeed
annotations
are
generally
typed
as
a
whole
object
according
to
some
predefined
set
of
annotation
types
w3c
2005a
kahan
and
koivunen
2001
frommholz
et
al
2003
bottoni
et
al
2003
2004
but
there
is
usually
no
means
for
describing
the
semantics
of
an
annotation
with
the
necessary
level
of
precision
however
this
is
possible
with
the
meanings
of
annotation
furthermore
annotation
types
do
not
allow
any
kind
of
navigation
among
different
types
while
meanings
of
annotations
can
be
organized
to
do
that
some
helpful
information
about
the
choice
of
distinguishing
between
meaning
and
sign
of
annotation
can
be
obtained
from
the
field
of
human
computer
interaction
bottoni
et
al
1999
deal
with
visual
languages
and
define
characteristic
structures
cs
as
sets
of
image
pixels
forming
functional
or
perceptual
units
whose
recognition
results
in
the
association
of
the
cs
with
a
meaning
they
call
characteristic
patterns
cp
the
cs
along
with
descriptions
of
the
cs
and
a
relation
that
associates
descriptions
to
cs
and
viceversa
the
distinction
between
cs
and
cp
resembles
the
distinction
between
sign
and
meaning
of
annotation
fogli
et
al
2004
also
recognize
this
correspondence
and
say
that
an
annotation
is
a
complex
cs
interpreted
by
a
human
as
a
cp
on
the
other
hand
bottoni
et
al
2003
also
adopt
the
cs
and
cp
mechanism
in
the
context
of
annotations
but
they
use
this
mechanism
to
place
annotations
on
information
resources
rather
than
to
distinguish
between
the
semantics
and
the
materialization
of
annotations
we
are
interested
in
studying
the
sharing
of
common
meanings
among
different
signs
defined
as
the
basic
mechanism
for
relating
and
gathering
up
different
signs
that
express
common
semantics
this
is
very
helpful
in
the
case
of
annotations
made
by
two
different
users
for
example
they
may
use
different
signs
to
indicate
the
importance
of
a
passage--an
asterisk
or
an
exclamation
mark
knowing
that
these
two
different
signs
have
the
same
meaning
allows
these
two
users
to
communicate
and
interact
with
each
other
in
addition
this
would
also
help
us
to
disambiguate
cases
where
two
signs
that
look
exactly
the
same
have
two
different
meanings
for
example
consider
the
case
of
a
user
who
is
performing
different
tasks
while
he
is
studying
a
paper
he
may
highlight
a
passage
to
indicate
that
it
is
worth
further
investigation
while
he
is
reviewing
a
paper
he
may
highlight
a
passage
to
indicate
that
it
is
not
correct
in
both
cases
he
uses
the
same
kind
of
sign
but
with
two
different
semantics
the
most
immediate
way
of
approaching
this
issue
is
to
introduce
the
following
relation
m1
sn1
sn2
s
n
s
n
m
sn1
m
sn2
this
relation
clearly
highlights
the
signs
that
directly
share
some
common
meanings
however
this
relation
is
not
able
to
relate
two
signs
that
do
not
directly
share
a
common
meaning
therefore
a
step
forward
also
considers
both
the
meanings
graph
g
m
m
e
m
and
its
reflexive
transitive
closure
g
m
e
m
so
that
we
can
m
introduce
the
following
relation
m2
sn1
sn2
s
n
s
n
m
m
m1
m
sn1
m2
m
sn2
m1
m2
e
m
m2
m1
e
m
m
m1
e
m
m
m2
e
m
m1
m
e
m
m2
m
e
m
the
m2
relation
means
that
two
signs
s1
and
s2
are
in
relation
if
among
their
meanings
which
are
obtained
by
m
sn1
and
m
sn2
at
least
--
one
is
the
ancestor
of
the
other
m1
m2
e
m
m2
m1
e
m
or
--
they
both
have
a
common
ancestor
m
m1
e
m
m
m2
e
m
or
--
they
both
are
the
ancestors
of
a
common
meaning
m1
m
e
m
m2
m
e
m
or
--
two
meanings
are
equal--as
in
the
case
of
the
m1
relation
indeed
m1
m2
because
s1
s2
s
n
sn1
sn2
m1
m
m
sn1
m
sn2
m
m
e
m
sn1
sn2
m2
m2
is
a
very
broad
relation
that
allows
us
to
relate
different
signs
according
to
the
four
strategies
outlined
above
where
needed
we
could
use
limited
versions
of
m2
that
adopt
only
some
of
the
strategies
introduced
here--for
example
m1
uses
only
the
last
strategy
further
strategies
can
be
envisaged
to
group
signs
on
the
basis
of
their
meanings
for
example
we
could
take
into
consideration
the
predecessor
of
a
meaning
instead
of
its
ancestor
as
in
m2
therefore
the
m1
and
m2
relations
are
examples
of
the
utilization
of
the
meanings
graph
however
they
are
not
intended
to
be
exhaustive
for
example
rigaux
and
spyratos
2004
propose
a
subsumption
relation
on
the
terms
of
a
taxonomy
and
a
way
of
navigating
through
them
that
can
also
be
very
useful
in
the
context
of
the
meanings
of
annotation
9
annotation
we
are
now
ready
to
introduce
the
definition
of
annotation
summing
up
the
concepts
introduced
in
the
previous
sections
we
can
briefly
say
that
an
annotation
is
expressed
by
one
or
more
signs
of
annotation
such
as
a
piece
of
text
or
some
graphic
mark
which
are
the
way
an
annotation
takes
shape
the
semantics
of
each
sign
is
in
turn
defined
by
one
or
more
meanings
of
annotation
with
respect
to
the
linking
issue
an
annotation
must
annotate
one
and
only
one
digital
object
identified
by
its
handle
while
it
may
relate
to
one
or
more
digital
objects
last
the
mechanism
introduced
in
section
4
2
on
how
to
address
the
time
dimension
is
fundamental
to
properly
define
the
relationship
between
the
annotation
and
the
annotated
digital
object
definition
9
1
an
annotation
a
a
k
is
a
tuple
a
ha
h
k
aua
u
s
r
k
-
1
g
a
2g
r
k-1
p
spa
sp
aa
s
n
k
lt
st
k
s
m
k
-
1
h
k
-
1
where
--
ha
is
the
unique
handle
of
the
annotation
a
i
e
h
ha
a
--aua
is
the
author
of
the
annotation
a
i
e
ha
au
aua
--
g
a
are
the
groups
of
users
with
their
respective
access
permissions
for
the
annotation
a
specified
by
the
pairs
g
p
with
g
g
a
and
p
p
--
spa
is
the
scope
of
the
annotation
a
--
each
n-ple
of
the
aa
relation
means
that
the
annotation
a
by
means
of
a
sign
in
sn
k
and
a
link
type
in
lt
is
annotating
or
relating
to
a
segment
in
st
k
of
a
stream
in
sm
k
-
1
of
a
digital
object
identified
by
its
handle
in
h
k
-
1
note
that
since
sm
sm
k
-
1
aa
sn
t
stsm
sm
h
must
be
sm
hsm
h
in
other
words
the
stream
sm
must
be
contained
in
the
digital
object
identified
by
the
handle
h
we
introduce
the
following
auxiliary
sets
to
simplify
the
following
discussion
--
the
set
of
the
signs
of
annotation
that
belong
to
the
annotation
a
sna
sn
sn
k
aa
sn
l
t
stsm
sm
h
hsm
ha
--
the
set
of
the
handles
of
digital
objects
that
are
subject
to
the
tasks
of
the
annotation
a
ha
h
h
k
-
1
aa
sn
l
t
stsm
sm
h
the
following
constraints
must
be
adhered
to
1
the
annotation
a
must
annotate
one
and
only
one
digital
object
and
it
cannot
also
relate
to
this
digital
object
hence
h
ha
sn
s
na
aa
sn
annotate
stsm
sm
h
1
aa
1
sn1
relateto
stsm1
sm1
h
2
a
sign
in
s
na
cannot
relate
to
more
than
one
digital
object
hence
sn
s
n
a
1
2
aa
1
sn
relateto
stsm1
sm1
h1
2
sn
relateto
stsm2
sm2
h2
1
2
3
there
is
no
other
annotation
a1
a
k
-
1
that
shares
signs
of
annotation
with
a
hence
a1
a
k
-
1
s
na
s
na1
4
if
the
annotation
a
a
k
annotates
or
relates
to
another
annotation
a1
a
k
-
1
then
scope
and
access
permission
conflicts
have
to
be
avoided
let
us
define
the
conflict
detector
function
cd
a
k
a
k
-1
0
1
so
that
cd
a
a1
0
if
there
are
neither
scope
conflicts
nor
access
permission
conflicts
1
if
there
are
either
scope
conflicts
or
access
permission
conflicts
therefore
the
following
condition
must
be
satisfied
h
ha
h
h
a1
a
k
-
1
cd
a
a1
0
in
conclusion
the
first
part
of
the
annotation
tuple
is
devoted
to
providing
information
about
the
annotation
itself
because
it
specifies
the
handle
of
the
annotation
its
author
its
groups
of
users
with
their
respective
access
permissions
its
scope
the
signs
of
the
annotation
and
the
link
types
on
the
other
hand
the
second
part
of
the
annotation
tuple
provides
information
about
the
annotated
or
related
digital
objects
specifying
which
segment
of
which
stream
of
which
digital
object
is
being
annotated
or
related
to
as
shown
in
the
following
we
do
not
use
the
time
dimension
notation
for
space
reasons
as
it
is
not
needed
for
this
observation
a
ha
aua
g
a
p
spa
aa
s
n
lt
information
about
the
annotation
st
s
m
h
information
about
the
digital
object
note
that
the
author
aua
of
the
annotation
is
not
taken
from
the
set
of
authors
but
from
the
set
of
users
at
time
k
-
1
indeed
from
definition
5
2
an
author
is
a
user
who
authored
at
least
one
digital
object
thus
if
we
had
used
the
set
of
authors
in
the
definition
of
annotation
we
would
have
constrained
the
author
of
the
annotation
to
have
authored
at
least
one
other
digital
object
besides
the
annotation
in
question
in
contrast
if
we
pick
out
a
user
from
the
set
of
users
at
time
k
-
1
we
allow
that
user
to
become
an
author
at
time
k
simply
because
he
is
authoring
the
annotation
at
hand
moreover
in
contrast
to
the
case
of
the
generic
digital
object
introduced
in
section
5
1
the
annotation
is
constrained
to
be
authored
by
one
and
only
one
author
in
the
following
section
we
will
discuss
the
meaning
of
the
aa
relation
and
the
four
constraints
introduced
in
the
definition
of
annotation
in
more
detail
discussion
about
the
aa
relation
the
aa
relation
makes
extensive
use
of
the
mechanism
introduced
in
section
4
2
for
addressing
the
time
dimension
in
particular
the
aa
relation
aims
to
demonstrate
that
an
annotation
must
annotate
or
relate
to
digital
objects
that
already
exist
for
this
reason
in
definition
9
1
the
annotation
a
belongs
to
a
k
while
the
annotated
or
related
digital
objects
belong
to
do
k
-
1
and
are
identified
by
their
handles
in
h
k
-
1
this
notation
underlines
the
fact
that
the
annotation
belongs
to
the
set
of
digital
objects
at
time
k
but
it
works
with
the
previously
existing
digital
objects
that
belong
to
the
set
of
digital
objects
at
time
k
-
1
therefore
an
annotation
can
only
annotate
or
relate
to
already
existing
digital
objects
which
is
quite
intuitive
but
needs
to
be
properly
formalized
a
very
important
consequence
of
this
choice
is
that
ha
ha
in
fact
ha
h
k
h
k
-
1
while
ha
h
k
-
1
ha
is
precisely
the
handle
identified
by
the
transition
from
h
k
-
1
to
h
k
as
explained
in
section
4
2
therefore
an
annotation
cannot
be
self-referential
it
cannot
annotate
or
relate
to
itself
since
a
self-referential
annotation
would
be
useless
the
aa
relation
makes
use
of
the
set
of
signs
s
n
k
at
time
k
to
indicate
that
they
represent
the
signs
created
precisely
for
the
annotation
a
furthermore
aa
uses
the
set
of
segments
st
k
at
time
k
to
indicate
that
those
segments
are
created
solely
to
allow
the
annotation
a
to
point
to
the
requested
part
of
the
streams
contained
in
s
m
k
-
1
if
we
consider
the
mechanism
introduced
in
section
4
2
for
formalizing
the
temporal
dimension
when
at
time
k
-
1
we
pick
out
a
new
segment
stsm
st
k
-
1
u
st
it
can
refer
to
a
stream
sm
s
m
k
-
1
in
fact
that
stream
already
exists
at
time
k
-
1
even
though
the
new
segment
belongs
to
the
set
of
segment
st
k
only
at
time
k
note
that
the
aa
relation
uses
the
set
of
streams
s
m
k
-
1
at
time
k
-
1
because
those
are
the
streams
that
belong
to
the
digital
objects
identified
by
their
handles
in
h
k
-
1
in
conclusion
we
deal
with
digital
objects
and
their
corresponding
streams
which
already
exist
at
time
k
-
1
and
which
are
annotated
or
related
by
using
signs
and
segments
that
have
just
been
created
for
the
annotation
a
at
time
k
in
the
aa
relation
both
segments
and
streams
play
a
very
important
role
in
allowing
an
annotation
to
annotate
or
relate
to
the
requested
part
of
a
digital
object
in
this
context
the
distinction
between
logical
and
physical
streams
and
the
possibility
of
using
the
logical
streams
as
representatives
of
their
equivalence
classes
becomes
a
fundamental
issue
we
can
always
suppose
that
an
annotation
deals
with
logical
streams
because
the
mapping
to
different
physical
streams
is
correctly
managed
by
the
notion
of
the
stream
mapping
relation
as
introduced
in
section
6
2
in
this
way
an
annotation
can
annotate
a
logical
stream
and
it
is
also
implicitly
annotating
all
of
the
physical
streams
that
are
in
the
same
equivalence
class
of
the
logical
stream
furthermore
as
discussed
in
section
6
2
an
annotation
could
annotate
abstract
streams
belonging
to
equivalence
classes
created
on
the
quotient
set
s
m/sm
in
this
way
it
obtains
access
to
an
entire
hierarchy
of
different
representations
of
the
content
of
a
digital
object
last
logical
streams
simplify
the
use
of
segments
because
we
can
always
refer
to
contiguous
indices
in
the
logical
streams
even
though
they
are
not
contiguous
in
the
physical
streams
as
observed
in
section
6
3
this
possibility
makes
it
easier
to
determine
which
part
of
the
digital
object
is
being
annotated
or
related
because
we
can
always
make
the
assumption
that
we
are
dealing
with
contiguous
indices
in
the
stream
of
the
digital
object
last
the
aa
relation
does
not
explicitly
make
use
of
the
meanings
of
the
annotation
even
though
they
are
a
fundamental
part
of
our
model
as
explained
in
section
8
the
meanings
of
annotation
represent
a
kind
of
preexisting
and
superimposed
knowledge
which
does
not
belong
to
any
specific
sign
of
annotation
in
particular
but
rather
should
be
shared
by
different
signs
of
annotation
to
support
cooperation
and
interoperability
in
this
sense
the
meanings
of
annotation
are
not
directly
part
of
any
specific
annotation
on
the
other
hand
as
introduced
in
definition
8
1
the
meanings
function
m
allows
us
to
associate
each
sign
of
annotation
with
its
corresponding
meaning
of
annotation
therefore
for
each
sign
sn
s
na
we
can
use
m
sn
to
obtain
its
meanings
of
annotation
and
we
can
then
exploit
and
navigate
the
meanings
graph
if
necessary
discussion
about
the
constraints
of
the
annotation
the
first
two
constraints
are
intra-annotation
constraints
because
they
limit
the
aa
relation
which
is
the
core
of
the
annotation
the
second
two
constraints
instead
are
inter-annotation
constraints
because
they
regulate
the
relationships
of
the
annotation
with
respect
to
other
annotations
the
first
constraint
imposes
the
existence
and
uniqueness
of
the
annotated
digital
object
and
prevents
the
annotated
digital
object
from
being
related
as
well
in
this
way
the
constraint
introduced
in
section
6
1
an
annotation
must
annotate
one
and
only
one
digital
object
either
a
document
or
another
annotation
hence
an
annotation
must
have
one
and
only
one
annotate
link
is
complied
with
furthermore
it
enforces
the
distinction
between
the
annotate
link
and
the
relate-to
link
because
the
annotated
digital
object
cannot
also
be
related
therefore
it
underlines
the
fact
that
the
role
of
the
annotate
link
is
to
express
intradigital
object
relationships
while
the
relate-to
link
is
requested
to
express
only
interdigital
object
relationships
furthermore
each
sign
must
cooperate--once
and
only
once--in
expressing
such
intra-do
relationships
there
is
no
sign
whose
only
link
is
the
relate-to
link
a
consequence
of
this
constraint
is
h1
h2
ha
1
2
aa
1
sn1
annotate
stsm1
sm1
h1
2
sn2
annotate
stsm2
sm2
h2
h1
h2
the
second
constraint
aims
at
keeping
the
semantics
of
a
sign
as
clear
as
possibile
if
a
sign
sn
could
be
related
to
more
digital
objects
it
would
not
be
clear
which
of
its
meanings--given
by
m
sn
--should
be
applied
to
each
related
digital
object
in
conclusion
this
constraint
together
with
the
first
constraint
states
that
a
sign
of
annotation
must
annotate
one
and
only
one
segment
of
a
digital
object
and
it
may
relate
to
one
and
only
one
segment
of
another
digital
object
the
third
constraint
ensures
that
the
signs
of
an
annotation
are
not
shared
with
any
other
annotation
to
preserve
the
mechanism
of
sharing
common
semantics
among
annotations
as
explained
in
section
8
the
sharing
of
meanings
of
annotation
by
means
of
the
m1
and
m2
relations
is
the
mechanism
for
pointing
out
common
semantics
among
annotations
on
the
other
hand
the
direct
sharing
of
signs
of
annotation
could
be
misleading
in
fact
a
sign
is
a
materialization
of
a
meaning
the
same
sign
might
be
used
by
different
users
with
completely
different
semantics
while
different
signs
used
by
different
users
might
have
the
same
semantics
consider
for
example
two
users
who
use
the
star
symbol
one
uses
it
to
indicate
an
important
passage
while
the
other
uses
it
to
indicate
a
wrong
passage
in
this
case
we
have
two
signs
that
look
exactly
the
same
but
have
two
completely
different
meanings
another
situation
would
be
where
two
users
use
the
star
symbol
and
the
exclamation
mark
both
to
indicate
an
important
passage
in
this
case
we
have
two
signs
that
look
different
but
have
the
same
meaning
therefore
the
cooperation
among
users
happens
by
sharing
common
meanings
which
are
connected
by
way
of
the
meanings
graph
and
not
by
directly
sharing
signs
which
may
be
misleading
or
incoherent
finally
note
that
this
constraint
does
not
prevent
the
existence
of
two
signs
looking
exactly
the
same
but
it
means
that
these
two
signs
are
different
elements
in
the
set
s
n
the
fourth
constraint
prevents
pathologic
situations
such
as
for
example
a
public
annotation
that
annotates
or
relates
to
a
private
annotation
in
such
cases
there
is
a
scope
conflict
using
the
previously
mentioned
example
the
author
of
the
private
annotation
can
see
both
the
public
and
the
private
annotations
but
another
user
can
see
only
the
public
annotation
which
is
annotating
something
hidden
to
this
user
as
a
further
example
a
shared
annotation
for
which
a
given
group
has
denied
access
permission
should
not
be
annotated
by
or
related
to
another
shared
annotation
for
which
the
same
group
has
read
and
write
access
permission
because
in
this
case
we
would
obtain
an
access
permission
conflict
the
situation
may
be
even
more
complicated
if
both
scope
and
access
permission
conflicts
happen
at
the
same
time
to
avoid
such
situations
the
conflict
detector
cd
function
has
been
introduced
which
returns
1
if
there
is
any
kind
of
conflict
and
0
otherwise
as
we
introduced
in
sections
5
2
and
5
3
the
concepts
of
access
permission
and
scope
of
annotation
are
not
intended
to
enforce
a
specific
access
policy
but
rather
to
provide
us
with
the
means
of
expressing
any
necessary
access
policy
as
a
consequence
the
actual
definition
of
the
conflict
detector
function
has
to
be
done
case
by
case
in
order
to
carry
out
the
necessary
access
policy
we
cannot
provide
an
a
priori
definition
of
it
that
is
appropriate
for
all
cases
for
example
if
we
set
the
following
rules
--
a
public
annotation
a1
can
be
freely
annotated
or
related
to
by
any
annotation
a
without
further
restrictions
--
a
shared
annotation
a1
can
be
annotated
or
related
to
only
by
a
shared
or
private
annotation
a
in
the
case
of
a
private
annotation
a
the
author
of
the
annotation
a
must
belong
to
at
least
one
of
the
groups
of
users
sharing
the
annotation
a1
provided
that
the
access
permission
for
that
group
is
not
denied
in
a1
in
the
case
of
a
shared
annotation
a
all
of
the
groups
of
users
sharing
the
annotation
a
must
also
be
sharing
the
annotation
a1
provided
that
none
of
them
has
been
denied
access
permission
in
a1
it
follows
that
a
shared
annotation
cannot
be
annotated
by
or
related
to
a
public
annotation
--
a
private
annotation
a1
can
be
annotated
or
related
to
only
by
a
private
annotation
a
provided
that
they
have
the
same
author
it
follows
that
a
private
annotation
can
be
annotated
by
or
related
to
neither
a
public
annotation
nor
a
shared
annotation
then
we
can
provide
the
following
definition
of
the
conflict
detector
function
0
if
spa1
public
0
if
spa
shared
spa
private
g
1
p1
g
a
p
1
1
aua
g
1
p1
denied
cd
a
a1
0
if
spa1
spa
shared
g
p
g
a
g
1
p1
g
a1
g
g
1
p1
denied
0
if
spa
spa
private
aua
aua
1
1
1
otherwise
10
document-annotation
hypertext
as
explained
in
section
3
we
consider
that
existing
digital
objects
and
annotations
constitute
a
hypertext
the
definition
and
the
properties
of
this
hypertext
directly
follow
from
the
definition
of
annotation
we
provided
in
the
previous
sections
therefore
we
can
consider
the
document-annotation
hypertext
as
a
kind
of
view
of
the
set
of
documents
and
annotations
the
aim
is
to
mask
all
of
the
details
involved
by
the
definition
of
the
annotation
itself
and
to
provide
us
with
a
more
abstract
representation
of
the
objects
we
dealt
with
and
of
their
structural
relationships
we
will
introduce
the
definition
of
document-annotation
hypertext
and
we
will
study
its
properties
by
directly
using
the
set
of
digital
objects
do
and
the
set
of
annotations
a
in
the
following
we
will
not
make
use
of
the
set
of
handles
h
as
might
be
expected
from
the
previous
discussion
where
annotations
can
be
linked
to
digital
objects
only
by
using
their
handles
this
choice
allows
us
to
explain
the
properties
of
the
document-annotation
hypertext
in
a
clearer
and
more
intuitive
way
than
doing
so
by
using
handles
which
would
just
add
a
further
level
of
indirection
indeed
if
we
used
handles
instead
of
digital
objects
in
the
explanation
we
would
have
to
map
each
handle
back
to
the
corresponding
digital
object
by
using
the
h
function
in
order
to
exploit
the
characteristics
of
the
digital
object
in
the
reasoning
about
the
document-annotation
hypertext
on
the
other
hand
since
the
h
function
is
bijective
we
are
sure
that
the
properties
of
the
document-annotation
hypertext
demonstrated
by
directly
using
digital
objects
hold
and
are
valid
even
in
the
case
of
the
use
of
handles
this
is
quite
important
because
according
to
the
line
of
reasoning
developed
in
the
previous
sections
we
may
not
directly
deal
with
documents
which
could
be
independently
managed
by
external
dlms
but
we
will
always
have
the
possibility
of
referring
to
those
digital
objects
by
using
their
handles
in
conclusion
the
actual
document-annotation
hypertext
could
be
constructed
by
using
the
handles
of
the
digital
objects
even
though
its
properties
are
better
explained
and
demonstrated
by
directly
using
the
digital
objects
definition
10
1
graph
the
document-annotation
hypertext
is
a
labeled
directed
hda
do
eda
a
do
lda
where
--
do
a
d
is
a
set
of
vertices
--
ed
a
a
do
a
do
aa
sn
t
stsm
sm
h-1
do
is
a
set
of
edges
--
ld
a
ed
a
lt
is
a
labelling
function
such
that
for
each
e
a
d
o
ed
a
there
is
a
l
t-labeled
edge
from
the
annotation
a
to
the
generic
digital
object
d
o
annotate
if
aa
sn
annotate
stsm
sm
h-1
do
lda
a
d
o
relateto
if
aa
sn
relateto
stsm
sm
h-1
do
the
document-annotation
hypertext
is
constructed
by
putting
an
edge
between
an
annotation
vertex
and
a
digital
object
vertex
if
the
annotation
is
annotating
or
relating
to
that
digital
object
note
that
we
used
h-1
do
in
ed
a
to
track
the
digital
object
back
to
its
handle
the
edge
is
then
labeled
with
the
corresponding
link
type
each
edge
e
a
do
eda
always
starts
from
an
annotation
a
a
while
e
eda
which
starts
from
a
document
d
d
does
not
exist
note
that
we
deal
with
a
graph
hda
and
not
with
a
multigraph--a
graph
where
multiple
edges
between
the
same
vertices
are
allowed--as
may
happen
in
the
case
of
an
annotation
relating
to
a
different
part
of
the
same
digital
object
therefore
we
consider
that
multiple
edges
with
the
same
direction
between
the
same
vertices
are
collapsed
into
a
single
edge
table
ii
summarizes
the
graphical
conventions
adopted
in
the
following
figures
figure
3
shows
an
example
of
document-annotation
hypertext
hd
a
--
d
d
1
d
2
d
3
d
4
d
5
we
can
assume
that
the
subscript
of
each
document
indicates
the
time
in
which
the
document
became
an
element
of
the
set
d
--
a
a1
a2
a3
a4
a5
a6
a7
a8
a9
a10
a11
a12
a13
a14
we
can
assume
that
the
subscript
of
each
annotation
indicates
the
time
in
which
the
annotation
became
an
element
of
the
set
a
--
we
can
express
for
example
--
annotation
sets
concerning
a
document
a1
a2
is
an
annotation
set
concerning
the
document
d
1
--annotation
sets
concerning
an
annotation
a8
a9
is
an
annotation
set
concerning
the
annotation
a7
--annotation
threads
concerning
a
document
a1
a3
a4
is
an
annotation
thread
concerning
the
document
d
1
--
annotation
threads
concerning
an
annotation
a8
a10
is
an
annotation
thread
concerning
the
annotation
a7
--
multiple
annotation
threads
concerning
a
document
a7
a8
a10
and
a12
a13
a14
are
two
different
annotations
threads
both
concerning
the
document
d
3
--
multiple
annotation
threads
concerning
an
annotation
a8
a10
and
a9
a11
are
two
annotation
threads
both
concerning
the
annotation
a7
--
nested
annotation
threads
concerning
a
document
a8
a10
and
a9
a11
are
two
different
and
nested
annotation
threads
both
concerning
the
document
d
3
figure
3
also
points
out
another
important
feature
of
the
documentannotation
hypertext
it
can
span
and
cross
the
boundaries
of
the
single
ims
as
discussed
in
section
3
ims1
manages
d
1
and
d
2
while
ims2
manages
d
3
d
4
and
d
5
there
are
annotations
that
act
as
a
bridge
between
two
imss
for
example
a5
annotates
d
2
which
is
managed
by
ims1
and
refers
to
d
3
which
is
managed
by
ims2
proposition
10
2
properties
the
document-annotation
hypertext
has
the
following
1
the
graph
does
not
contain
loops
a
a
e
a
do
eda
a
do
2
each
annotation
a
is
incident
with
one
and
only
one
edge
labeled
annotate
a
a
e
a
do
eda
lda
e
annotate
3
the
graph
does
not
contain
cycles
c
a1
ak
ak-1
a2
a1
e1
a1
ak
ek
ak
ak-1
e2
a2
a1
ed
a
k
1
4
given
a
set
a
a
there
are
at
least
a
edges
in
hd
a
incident
on
elements
of
a
therefore
the
following
relationship
holds
for
the
size
of
hd
a
hd
a
a
proof
we
can
show
that
1
from
definition
9
1
it
follows
that
ha
ha
and
as
explained
in
section
9
aa
sn
t
stsm
sm
ha
thus
e
a
a
ed
a
2
from
definition
9
1
we
have
the
following
constraint
h
ha
sn
s
na
aa
sn
annotate
stsm
sm
h
it
follows
that
a
a
d
o
do
aa
sn
annotate
stsm
sm
h-1
d
o
thus
there
exists
a
unique
edge
such
that
ld
a
a
d
o
annotate
3
annotations
entail
a
temporal
dimension
since
each
annotation
must
annotate
or
relate
to
an
already
existing
digital
object
as
explained
in
sections
4
2
and
9
from
definition
9
1
the
aa
relation
involves
the
set
h
k
-
1
of
handles
of
digital
objects
that
already
belong
to
the
set
of
digital
objects
at
time
k
-
1
while
the
annotation
belongs
to
the
set
a
k
therefore
by
means
of
the
aa
relation
an
annotation
a
a
k
can
annotate
or
relate
to
only
digital
objects
that
already
exist
at
time
k
an
annotation
cannot
annotate
or
relate
to
another
annotation
that
does
not
already
exist
at
time
k
it
follows
cycles
such
as
the
one
shown
in
figure
4
where
the
oldest
annotation
a1
a
1
annotates
or
relates
to
the
newest
annotation
ak
a
k
with
k
1
are
not
possibile
in
fact
when
the
oldest
annotation
a1
a
1
was
created
at
time
1
the
newest
annotation
ak
a
k
ak
a
0
did
not
exist
and
so
it
could
not
have
been
involved
in
aa1
which
makes
use
of
digital
objects
belonging
to
the
set
of
digital
objects
at
time
0
note
that
this
issue
does
not
exist
for
document
d
d
vertices
since
edges
can
start
only
from
annotation
vertices
4
since
for
item
number
2
each
annotation
a
must
be
incident
with
one
and
only
one
annotate
edge
then
for
a
annotations
there
are
at
least
a
edges
there
may
be
more
if
there
are
also
relateto
edges
in
hd
a
there
are
a
annotations
and
therefore
hd
a
a
proposition
10
2
expresses
the
constraints
imposed
on
the
annotation
in
definition
9
1
in
terms
of
a
graph
first
the
graph
does
not
contain
loops
corresponding
to
self-referential
annotations
that
are
useless
for
our
purposes
second
each
annotation
is
incident
with
one
and
only
one
edge
of
the
kind
annotate
link
thus
formalizing
the
constraint
on
the
link
types
introduced
in
section
6
1
third
since
each
annotation
can
annotate
or
relate
to
an
already
existing
do
the
third
property
ensures
that
there
are
no
cycles
where
the
oldest
annotation
a1
annotates
or
relates
to
the
newest
annotation
ak
as
shown
in
figure
4
last
the
fourth
property
sets
a
lower
bound
to
the
size
of
hd
a
figure
5
shows
the
patterns
that
can
be
obtained
by
combining
the
allowed
link
types
note
that
each
pattern
is
characterized
by
only
one
edge
of
the
type
annotate
link
furthermore
an
annotation
is
not
allowed
to
exclusively
have
relateto
link
edges
note
that
the
example
of
document-annotation
hypertext
shown
in
figure
3
is
the
result
of
the
combination
of
these
basic
allowed
annotation
patterns
proposition
10
3
--e
let
hd
a
do
ed
a
be
the
subgraph
of
hd
a
such
that
e
eda
lda
e
annotate
sda
--
do
do
do
e
ed
a
e
a
do
hd
a
is
the
subgraph
whose
edges
are
of
the
kind
annotate
and
whose
vertices
are
incident
with
at
least
one
of
these
edges
let
hd
a
do
ed
a
be
the
underlying
graph
of
hd
a
which
is
the
undirected
version
of
hd
a
the
following
properties
hold
1
hda
hda
a
2
h
is
a
forest
sda
3
every
tree
in
hda
contains
a
unique
document
vertex
d
proof
we
can
show
that
1
according
to
proposition
10
2
hd
a
hd
a
a
but
since
in
hd
a
and
hd
a
there
are
only
annotate
edges
we
have
hd
a
hd
a
a
2
ab
absurdo
if
hd
a
was
not
a
forest
then
it
would
be
a
cyclic
graph
the
only
way
of
obtaining
a
cycle
in
hd
a
is
that
in
hd
a
a
a
e1
a
d
o1
e2
a
d
o2
ed
a
d
o1
d
o2
ld
a
e1
ld
a
e2
annotate
an
annotation
exists
in
hd
a
from
which
two
edges
of
the
kind
annotate
start
but
this
contradicts
the
definition
10
1
given
for
the
graph
hd
a
and
thus
hd
a
is
a
forest
3
since
hd
a
is
a
forest
its
components
are
trees
ab
absurdo
suppose
that
there
is
a
tree
t
whose
vertices
are
only
annotations
a
tree
t
with
n
vertices
has
n
-
1
edges
but
for
proposition
10
2
in
hd
a
and
also
in
hd
a
n
annotations
are
incident
with
n
edges
so
t
cannot
be
a
tree
therefore
every
tree
in
hd
a
contains
at
least
one
document
vertex
d
suppose
now
that
there
is
a
tree
t
which
contains
two
document
vertices
d
1
and
d
2
d
1
d
2
since
for
every
two
vertices
in
a
tree
there
is
a
unique
path
connecting
them
in
the
path
p
d
1
a1
ai
ak
d
2
there
must
be
an
annotation
ai
from
which
in
hd
a
two
edges
of
the
kind
annotate
start
since
by
definition
of
hd
a
there
are
no
edges
of
the
type
e
d
m
d
n
ed
a
however
the
annotation
ai
contradicts
the
definition
of
hd
a
and
thus
there
is
a
unique
document
vertex
d
in
t
note
that
if
we
had
not
removed
the
relateto
link
edges
from
the
graph
hd
a
it
could
have
contained
cycles
consider
figure
3
for
example
a
cycle
would
be
c
a7
a6
a10
a8
a7
because
in
hd
a
we
do
not
consider
the
direction
of
the
edges
figure
6
shows
an
example
of
the
hd
a
subgraph
obtained
from
the
documentannotation
hypertext
hd
a
of
figure
3
11
conclusions
and
future
work
we
have
discussed
the
problem
of
providing
users
with
annotations
on
different
kinds
of
digital
objects
managed
by
ims
that
can
range
from
dbms
to
dlms
in
particular
we
have
addressed
this
problem
in
the
context
of
dlms
and
their
evolution
delos
2004
ioannidis
et
al
2005
candela
et
al
2006
to
this
end
annotations
have
been
studied
and
formalized
as
an
effective
tool
suitable
for
enabling
and
carrying
out
the
evolution
of
dlms
we
have
introduced
the
distinction
between
the
meaning
and
the
sign
of
an
annotation
this
distinction
has
allowed
us
to
better
describe
both
the
semantics
and
the
materialization
of
the
annotation
and
to
adopt
a
flexible
approach
in
modeling
annotations
in
fact
this
gives
us
the
opportunity
to
deal
with
the
semantics
of
annotations
in
a
flexible
way
avoiding
predefined
types
and
making
it
possible
to
exploit
them
as
an
effective
collaboration
tool
for
users
we
have
proposed
a
formal
model
of
annotations
on
digital
content
which
until
now
has
been
absent
from
the
literature
concerning
annotations
this
formal
model
not
only
captures
all
the
aspects
we
have
described
but
it
also
effectively
formalizes
the
time
dimension
entailed
by
annotations
furthermore
it
introduces
the
notion
of
document-annotation
hypertext
and
explores
some
of
its
properties
finally
it
provides
us
with
a
sound
theoretical
basis
for
future
research
on
this
matter
future
research
work
will
concern
the
use
of
annotations
in
order
to
search
for
documents
and
the
proposed
formal
model
constitutes
the
necessary
groundwork
to
be
able
to
design
and
formalize
search
algorithms
and
to
express
query
languages
that
take
annotations
into
account
annotations
provide
us
with
an
additional
source
of
evidence
which
is
complementary
to
that
already
contained
in
the
set
of
documents
therefore
we
can
exploit
annotations
with
the
two
final
goals
of
retrieving
more
relevant
documents
and
ranking
them
better
furthermore
the
paths
that
connect
annotations
to
documents
become
the
vehicle
for
moving
this
further
source
of
evidence
towards
the
documents
in
fact
the
document-annotation
hypertext
is
the
basic
infrastructure
for
combining
the
sources
of
evidence
that
come
from
documents
and
annotations
we
have
already
started
to
work
on
this
problem
in
the
context
of
data
fusion
agosti
and
ferro
2005b
2006
this
is
because
we
need
to
combine
the
source
of
evidence
which
comes
from
annotations
with
the
source
which
comes
from
documents
for
the
future
we
plan
to
employ
both
hypertext
information
retrieval
agosti
and
smeaton
1996
and
link
fusion
techniques
xi
et
al
2004
for
designing
advanced
search
algorithms
that
involve
annotations
based
on
our
formal
model
once
we
have
developed
search
strategies
that
exploit
annotations
we
will
therefore
need
to
evaluate
their
retrieval
performances
by
using
standard
information
retrieval
methodologies
we
plan
to
adopt
the
cranfield
methodology
cleverdon
1997
which
makes
use
of
experimental
collections
to
measure
the
performances
of
an
information
retrieval
system
the
performances
are
measured
by
using
the
standard
precision
and
recall
figures
van
rijsbergen
1979
salton
and
mcgill
1983
but
according
to
hull
1993
we
also
need
a
statistical
methodology
for
judging
whether
the
measured
performances
can
be
considered
statistically
significant
the
next
step
will
be
to
investigate
the
possibility
of
using
measures
that
differ
from
precision
and
recall
and
are
better
tailored
to
the
features
of
annotations
finally
there
is
a
lack
of
experimental
test
collections
with
annotated
digital
contents
we
have
already
started
to
work
on
this
problem
agosti
et
al
2007b
and
our
future
research
work
will
also
concern
the
design
and
development
of
this
kind
of
test
collection
a
field
evaluation
of
an
adaptable
two-interface
design
for
feature-rich
software
joanna
mcgrenere
university
of
british
columbia
ronald
m
baecker
university
of
toronto
and
kellogg
s
booth
university
of
british
columbia
two
approaches
for
supporting
personalization
in
complex
software
are
system-controlled
adaptive
menus
and
user-controlled
adaptable
menus
we
evaluate
a
novel
interface
design
for
feature-rich
productivity
software
based
on
adaptable
menus
the
design
allows
the
user
to
easily
customize
a
personalized
interface
and
also
supports
quick
access
to
the
default
interface
with
all
of
the
standard
features
this
design
was
prototyped
as
a
front-end
to
a
commercial
word
processor
a
field
experiment
investigated
users
personalizing
behavior
and
tested
the
effects
of
different
interface
designs
on
users
satisfaction
and
their
perceived
ability
to
navigate
control
and
learn
the
software
there
were
two
conditions
a
commercial
word
processor
with
adaptive
menus
and
our
prototype
with
adaptable
menus
for
the
same
word
processor
our
evaluation
shows
1
when
provided
with
a
flexible
easy-to-use
and
easy-to-understand
customization
mechanism
the
majority
of
users
do
effectively
personalize
their
interface
and
2
user-controlled
interface
adaptation
with
our
adaptable
menus
results
in
better
navigation
and
learnability
and
allows
for
the
adoption
of
different
personalization
strategies
as
compared
to
a
particular
system-controlled
adaptive
menu
system
that
implements
a
single
strategy
we
report
qualitative
data
obtained
from
interviews
and
questionnaires
with
participants
in
the
evaluation
in
addition
to
quantitative
data
1
introduction
desktop
applications
such
as
word
processors
spreadsheets
and
web
browsers
have
become
woven
into
the
daily
lives
of
many
people
in
the
developed
world
these
applications
have
traditionally
started
small
in
terms
of
functionality
and
have
grown
with
each
new
release
this
phenomenon
sometimes
called
creeping
featurism
hsi
and
potts
2000
norman
1998
or
bloatware
kaufman
and
weed
1998
is
pervasive
a
long
feature
list
is
seen
as
essential
for
products
to
compete
in
the
marketplace
applications
have
become
more
visually
complex
menus
have
multiplied
in
size
and
number
and
toolbars
have
been
introduced
to
reduce
complexity
but
they
too
have
grown
in
a
similar
fashion
insufficient
attention
has
been
paid
to
the
impact
of
this
functionality
explosion
on
the
user
we
introduce
a
design
that
supports
two
interfaces
between
which
the
user
can
easily
toggle
1
an
interface
personalized
by
the
user
containing
desired
features
only
and
2
the
default
interface
with
all
of
the
standard
features
the
target
for
the
design
is
feature-rich
productivity
applications
used
by
a
diversity
of
users
the
design
has
been
tested
in
a
prototype
front-end
to
the
commercial
word
processor
microsoft
word
2000
msw2k
and
evaluated
in
a
field
experiment
with
20
participants
the
two
main
goals
of
the
evaluation
were
1
to
understand
the
users
personalization
behavior
with
the
new
design
and
2
to
compare
our
design
to
the
adaptive
interface
of
msw2k
our
choice
of
goals
determined
our
choice
of
methodology
our
first
goal
was
exploratory
in
nature
our
second
goal
was
comparative
there
is
a
distinction
between
controlled
laboratory
evaluation
where
statistical
significance
is
the
norm
and
field
evaluation
where
qualitative
methods
are
given
more
weight
we
have
included
comments
from
participants
during
interviews
which
complement
the
quantitative
data
providing
a
richer
account
of
their
experience
during
the
experiment
our
evaluation
shows
1
when
provided
with
a
flexible
easy-to-use
customization
mechanism
the
majority
of
users
do
effectively
personalize
their
interface
and
2
user-controlled
interface
adaptation
with
our
adaptable
menus
results
in
better
navigation
and
learnability
and
allows
for
the
adoption
of
different
personalization
strategies
as
compared
to
the
particular
systemcontrolled
adaptive
menu
system
in
msw2k
which
implements
a
single
strategy
1
1
design
solutions
to
complex
software
the
traditional
all-in-one
interface
has
menus
and
toolbars
that
are
static
so
every
user
regardless
of
task
or
experience
has
the
same
interface
there
are
a
number
of
alternative
interface
designs
aimed
at
reducing
user
interface
complexity
although
most
have
received
minimal
to
no
evaluation
design
solutions
tend
to
fit
into
one
of
two
categories
1
ones
that
take
a
level-structured
approach
shneiderman
1997
and
2
ones
that
offer
a
personalized
interface
for
each
user
most
commonly
using
artificial
intelligence
a
classic
level-structured
design
includes
two
or
more
interfaces
each
containing
a
predetermined
set
of
functions
the
user
has
the
option
to
select
an
interface
level
but
not
to
select
which
functions
appear
in
that
level
preliminary
research
suggests
that
when
an
interface
is
missing
even
one
needed
function
the
user
is
forced
to
the
next
level
of
the
interface
which
results
in
frustration
mcgrenere
2002
we
address
this
limitation
in
our
design
which
uses
a
level-structured
approach
while
allowing
the
user
to
modify
the
contents
of
one
of
the
levels
there
are
a
small
number
of
commercial
applications
that
provide
a
level-structured
interface
e
g
hypercard
and
framemaker
some
applications
such
as
eudora
provide
a
level-structured
approach
across
versions
by
offering
both
pro
and
lite
versions
such
product
versioning
however
seems
to
be
motivated
more
by
business
considerations
than
by
an
attempt
to
meet
user
needs
the
carroll
and
carrithers
training
wheels
interface
to
an
early
word
processor
adopts
a
level-structured-like
approach
although
there
was
only
one
interface
all
of
the
functionality
that
was
not
needed
for
simple
tasks
was
blocked
off
such
that
when
the
user
clicked
on
a
blocked
function
a
dialog
appeared
indicating
that
the
function
was
unavailable
in
the
training
wheels
system
the
design
had
the
user
progressing
through
two
distinct
phases
after
the
first
phase
the
training
wheels
were
removed
launching
the
user
into
the
full
system
novice
users
were
able
to
accomplish
tasks
significantly
faster
and
with
significantly
fewer
errors
than
novice
users
using
the
full
version
carroll
and
carrithers
1984
despite
the
promise
of
this
early
work
mechanisms
to
support
the
transition
between
the
blocked
and
unblocked
states
were
never
investigated
in
our
design
users
can
move
easily
back
and
forth
between
the
designs
unlike
a
classic
level-structured
user
interface
a
personalized
interface
is
one
that
is
tailored
to
each
individual
user
the
two
main
ways
for
achieving
personalization
are
through
system-initiated
adaptation
namely
adaptive
interfaces
and
through
user-initiated
customization
namely
adaptable
interfaces
these
two
approaches
have
significant
differences
with
respect
to
the
goal
of
reducing
interface
complexity
while
the
broad
goal
of
adaptive
and
more
generally
intelligent
user
interfaces
is
to
assist
the
user
by
offloading
complexity
miller
et
al
1991
a
common
complaint
about
adaptive
interfaces
is
that
they
result
in
the
user
perceiving
a
loss
of
control
dieterich
et
al
1993
fischer
1993
adaptable
interfaces
by
contrast
have
not
typically
been
designed
for
the
purposes
of
reducing
complexity
and
so
they
are
often
difficult
to
use
however
they
do
not
suffer
the
same
user
control
problem
fischer
1993
there
has
been
a
debate
in
the
user
interface
design
community
between
those
who
promote
the
use
of
artificial
intelligence
in
the
interface
and
those
who
promote
comprehensible
predictable
and
controllable
interfaces
that
give
users
the
sense
of
power
mastery
control
and
accomplishment
shneiderman
and
maes
1997
we
briefly
survey
adaptable
and
adaptive
interfaces
in
turn
before
describing
our
design
and
evaluation
in
terms
of
adaptable
interfaces
many
commercial
applications
allow
the
user
to
reconfigure
the
interface
in
predetermined
ways
such
as
by
adding/removing
functions
to/from
the
menus
and
toolbars
and
by
moving
functions
from
one
menu/toolbar
to
another
despite
the
prevalence
of
such
customization
facilities
there
has
been
relatively
little
research
into
their
design
a
common
complaint
however
is
that
the
mechanisms
for
customizing
are
complex
and
can
therefore
require
significant
time
for
both
learning
and
doing
the
customization
thus
only
the
more
sophisticated
users
are
able
to
customize
mackay
1990
1991
found
the
latter
to
be
true
in
the
case
of
unix
customization
she
identified
a
small
group
of
users
which
she
called
the
translators
who
shared
their
customizations
with
the
rest
of
the
organization
others
have
identified
this
role
and
assigned
their
own
names
tinkerer
maclean
et
al
1990
and
local
developer
gantt
and
nandi
1992
by
contrast
page
et
al
1996
found
that
92
of
participants
in
a
large
field
study
customized
their
word
processor
closer
examination
of
their
work
shows
however
that
a
very
broad
notion
of
customization
was
used
for
example
changing
the
zoom
setting
in
a
dropdown
button
on
the
toolbar
was
considered
a
customization
this
points
to
a
need
for
a
better
understanding
of
the
various
forms
of
customization
in
our
study
customization
is
narrowly
defined
as
adding/deleting
items
to/from
the
menu/toolbar
which
is
at
least
in
many
modern
graphical
user
interfaces
significantly
more
difficult
to
do
than
parameter
adjustments
relative
to
adaptable
interfaces
adaptive
interfaces
have
enjoyed
considerable
attention
by
the
research
community
given
the
breadth
of
work
we
are
unable
to
do
it
justice
in
this
short
review
instead
we
summarize
some
relevant
trends
and
highlight
selected
projects
for
greater
depth
the
reader
is
referred
to
browne
et
al
1990
and
schneider-hufschmidt
et
al
1993
for
early
books
on
the
topic
more
recent
developments
are
discussed
by
karat
et
al
2004
one
well-known
limitation
of
early
work
in
adaptive
interfaces
was
that
it
was
too
technology
focused
systems
were
built
but
relatively
little
user
testing
was
conducted
maybury
and
wahlster
1999
this
can
partially
be
explained
by
the
fact
that
evaluation
of
adaptive
interfaces
is
more
complex
than
that
of
standard
interfaces
there
is
greater
variability
with
adaptive
interfaces
and
the
evaluation
methodology
needs
to
accommodate
this
variability
greenberg
and
witten
1985
maybury
and
wahlster
1999
some
early
examples
of
adaptive
interfaces
include
the
adaptive
telephone
directory
in
which
the
hierarchy
of
names
in
the
directory
adapted
to
the
user
s
interactions
such
that
the
most
frequently
accessed
names
were
located
at
the
upper
levels
of
the
hierarch
and
the
least
frequently
accessed
names
were
located
at
the
lower
levels
greenberg
and
witten
1985
adaptive
prompting
augmented
an
interface
by
providing
a
permanently
visible
dynamic
menu
that
included
only
the
most
appropriate
and
most
likely
to
be
chosen
actions
based
on
the
user
s
context
malinowski
et
al
1993
both
the
aida
system
cote-munoz
1993
and
the
skill
adaptive
interface
gong
and
salrendy
1995
dynamically
adjusted
the
balance
of
functionality
offered
to
the
user
through
graphical
elements
icons
and
menus
and
the
command
line
depending
on
the
user
s
level
of
expertise
the
eager
system
detected
a
repetitive
activity
and
highlighted
menus
and
objects
on
the
screen
to
indicate
what
it
predicted
the
user
would
do
next
cypher
1991
of
the
adaptive
designs
described
above
there
was
user
testing
reported
for
the
adaptive
telephone
directory
and
eager
for
the
former
the
results
strongly
favoured
the
adaptive
directory
compared
to
a
static
one
user
testing
of
eager
showed
promise
in
its
ability
to
detect
and
highlight
the
correct
menus
and
objects
however
the
most
striking
finding
was
that
all
subjects
were
uncomfortable
with
giving
up
control
when
eager
took
over
p
37
the
work
on
the
adaptive
telephone
directory
is
a
constrained
example
but
does
provide
an
existence
proof
for
the
efficacy
of
adaptive
interfaces
another
limitation
of
early
adaptive
user
interface
research
is
that
it
focused
largely
on
prototype
systems
thomas
and
krogsoeter
1993
we
note
some
exceptions
here
the
aid
project
included
an
adaptive
front
end
for
the
british
telecom
electronic
mail
system
browne
et
al
1990
among
other
things
it
provided
adaptive
help
based
on
the
user
s
level
of
expertise
via
an
application
expert
user
testing
over
three
half-hour
sessions
each
separated
by
three
days
showed
relatively
poor
results
an
independent
expert
judged
that
only
7
of
the
adaptations
made
by
the
system
based
on
inferred
user
difficulties
and
expertise
were
useful
flexcel
was
a
modified
version
of
ms
excel
that
provided
a
separate
adaptation
toolbar
that
allowed
the
user
to
define
new
menu
entries
and
new
key
shortcuts
for
function
parameterization
krogsoeter
et
al
1994
in
addition
there
were
system-generated
adaptation
suggestions
which
the
user
accessed
at
her
convenience
user
testing
showed
some
acceptance
of
the
adaptation
but
revealed
that
the
transition
between
the
user
accepting
systemdefined
adpation
suggestions
to
actually
initiating
adaptations
him/herself
was
not
satisfactory
debevc
et
al
s
1996
adaptive
toolbar
for
ms
word
proposed
command
icon
changes
based
on
frequency
and
probability
of
specific
command
use
user
testing
comparing
the
adaptive
toolbar
to
a
fixed
toolbar
to
which
users
could
somehow
add/delete
functions
showed
that
the
adaptive
bar
improved
performance
for
certain
tasks
and
that
users
were
generally
satisfied
with
the
adaptive
bar
similar
adaptive
toolbars
for
ms
word
have
been
also
been
proposed
lim
et
al
2005
miah
et
al
1997
lastly
linton
et
al
s
2000
recommender
system
alerted
users
to
functionality
in
ms
word
currently
being
used
by
co-workers
doing
similar
tasks
no
user
testing
has
been
reported
we
note
that
all
the
user
testing
mentioned
above
has
been
done
in
the
lab
and
with
the
exception
of
the
aid
project
it
has
all
been
single
session
as
seen
above
the
microsoft
office
suite
of
applications
is
a
common
target
for
adaptive
user
interface
research
it
is
not
surprising
therefore
that
msw2k
introduced
an
adaptive
user
interface
namely
menus
that
adapt
to
each
individual
s
usage
microsoft
office
2000
products
enhancements
guide
2000
when
a
menu
is
initially
opened
a
short
menu
containing
only
a
subset
of
the
full
menu
contents
is
displayed
by
default
to
access
the
long
menu
one
must
hover
over
the
menu
with
the
mouse
for
a
few
seconds
or
click
on
the
arrow
icon
at
the
bottom
of
the
short
menu
when
an
item
is
selected
from
the
long
menu
it
will
then
appear
in
the
short
menu
the
next
time
the
menu
is
invoked
after
some
period
of
non-use
menu
items
will
disappear
from
the
short
menu
but
will
always
be
available
in
the
long
menu
users
cannot
view
or
change
the
underlying
user
model
maintained
by
the
system
their
only
control
is
to
turn
the
adaptive
menus
on/off
and
to
reset
the
data
collected
in
the
user
model
csinger
et
al
1994
have
investigated
the
utility
of
an
inspectable
user
model
the
work
documented
in
this
article
aims
to
address
a
number
of
the
shortcomings
in
the
literature
reported
above
we
compare
an
easy-to-use
adaptable
two-level
interface
that
was
designed
specifically
to
reduce
complexity
to
the
adaptive
menus
in
msw2k
the
adaptable
model
is
a
fully
functioning
frontend
to
msw2k
that
enabled
us
to
conduct
a
longitudinal
field
study
and
collect
data
reflecting
actual
personalization
behavior
as
well
as
self-reported
data
on
preferences
2
design
overview
2
1
conceptual
design
what
makes
our
design
unique
is
the
combination
of
three
design
elements
1
there
are
two
interfaces
one
that
is
personalized
the
personal
interface
and
one
that
contains
the
full
set
of
functions
the
full
interface
there
is
a
switching
mechanism
between
interfaces
that
requires
only
a
single
button
click
2
the
personal
interface
is
adaptable
by
the
user
with
an
easy-to-understand
adaptation
mechanism
3
the
personal
interface
begins
small
and
unless
the
user
adds
many
functions
it
remains
a
minimal
interface
relative
to
the
full
interface
the
only
difference
between
the
two
interfaces
is
the
functions
that
are
displayed
visually
in
the
menus
and
toolbars
the
set
of
functions
in
a
particular
menu
in
the
personal
interface
is
always
a
subset
of
those
in
the
same
menu
in
the
full
interface
and
the
relative
ordering
of
functions
is
preserved
thus
the
only
choice
users
make
with
respect
to
their
personal
interfaces
is
whether
or
not
to
include
particular
functions
the
conceptual
design
was
proposed
by
mcgrenere
and
moore
2000
based
on
a
study
of
53
members
of
the
general
population
who
used
msword
97
they
found
that
while
many
users
were
having
a
negative
experience
with
the
feature-richness
of
the
software
the
majority
of
users
would
not
choose
a
word
processor
that
gave
them
only
the
functions
that
they
are
currently
using
users
want
the
ability
to
discover
new
functions
the
proposed
design
allows
users
to
work
in
a
personalized
interface
with
a
reduced
feature
set
while
providing
one-button
access
to
the
standard
interface
with
all
features
by
default
the
personal
interface
is
displayed
when
the
application
is
launched
there
are
several
reasons
for
having
a
user-controlled
personalizable
interface
rather
than
a
predetermined
static
small
interface
not
only
do
users
typically
use
very
few
features
linton
et
al
2000
mcgrenere
and
moore
2000
but
the
overlap
between
the
command
vocabulary
of
different
users
is
minimal
even
for
users
in
the
same
group
who
perform
similar
tasks
and
who
have
similar
computer
expertise
greenberg
1993
the
limited
command
overlap
between
users
suggests
that
determining
appropriate
personal
interfaces
a
priori
is
not
possible
many
users
do
not
take
advantage
of
customization
features
mackay
1991
likely
because
of
complexity
inherent
in
customization
this
is
a
primary
argument
for
an
adaptive
interface
our
goal
has
been
to
make
an
easy-to-understand
adaptable
interface
instead
by
starting
users
with
a
small
personal
interface
users
are
encouraged
to
customize
and
take
control
of
their
interfaces
right
from
the
outset
although
our
proposed
multiple-interface
design
may
seem
at
first
glance
to
be
somewhat
awkward
and
nonintuitive
it
was
motivated
by
our
earlier
research
findings
and
other
results
from
the
literature
the
experiment
was
designed
to
assess
the
effectiveness
of
the
design
and
to
do
a
first
comparison
with
the
adaptive
interface
of
msw2k
2
2
implementation
our
conceptual
design
is
intended
to
generalize
to
any
productivity
application
used
by
a
diversity
of
users
with
a
broad
range
of
tasks
we
chose
to
implement
our
design
as
a
front-end
to
msword
1
because
word
processing
tends
to
be
a
canonical
example
in
hci
research
2
because
msword
is
relatively
easy
to
program
through
visual
basic
for
applications
vba
and
3
because
msword
dominates
in
the
marketplace
so
we
believed
that
participants
would
be
easy
to
find
in
order
to
evaluate
this
design
in
a
field
experiment
with
participants
who
were
already
users
of
msw2k
our
prototype
was
implemented
so
that
it
did
not
interfere
with
any
customization
that
participants
had
already
made
to
their
msword
interface
it
was
also
designed
to
be
easily
installed
on
top
of
an
existing
installation
of
msword
this
was
accomplished
by
placing
the
required
vba
code
in
a
specialized
document
template
that
was
loaded
into
msword
on
each
startup
if
necessary
a
participant
could
remove
the
prototype
by
simply
deleting
this
template
and
re-launching
word
the
information
about
function
availability
in
the
personal
interface
was
stored
in
a
flat
file
enabling
the
prototype
to
be
effectively
stateless
this
would
facilitate
the
quick
reconstruction
of
a
personal
interface
should
a
problem
occur
with
the
software
figures
1
and
2
show
screen
captures
of
the
two
interfaces
as
well
as
the
personalizing
mechanism
3
participants
twenty
experienced
msword
users
participated
in
this
evaluation
participation
was
solicited
electronically
through
newsgroups
and
listservs
that
were
broadcast
across
the
university
of
toronto
campus
and
the
surrounding
city
in
order
to
participate
users
had
to
meet
the
following
base
criteria
they
had
to
have
been
using
msw2k
for
at
least
1
month
they
had
to
do
the
majority
of
their
word
processing
on
a
single
computer
they
had
to
spend
a
minimum
average
of
3
hours
word
processing
per
week
they
had
to
have
msword
expertise
above
the
novice
level
they
had
to
be
at
least
20
years
of
age
and
they
had
to
live
at
most
one
half
hour
s
drive
from
campus
in
order
to
ensure
that
these
criteria
were
satisfied
prospective
participants
completed
an
online
screening
questionnaire
ninety-eight
people
completed
this
questionnaire
they
were
considered
in
the
order
in
which
they
applied
a
participant
s
level
of
expertise
was
assessed
with
a
microsoft
office
screening
questionnaire1
that
was
embedded
within
the
online
preliminary
questionnaire
the
screening
questionnaire
categorizes
expertise
into
five
groups
novice
beginner
intermediate
advanced
and
expert
a
particpant
had
to
be
ranked
at
least
as
a
beginner
in
order
to
participate
in
our
evaluation
personality
differences
with
respect
to
feature-rich
software
were
considered
we
included
10
feature-keen
participants
and
10
feature-shy
as
assessed
by
an
instrument
developed
by
mcgrenere
and
moore
2000
a
person
is
categorized
as
feature-keen
feature-neutral
or
feature-shy
based
on
his/her
response
to
statements
about
1
having
many
functions
in
the
interface
2
the
desire
to
have
a
complete
version
of
msword
i
e
not
a
lite
version
and
3
the
desire
to
have
an
up-to-date
version
of
msword
there
was
no
sampling
frame
of
the
user
population
available
to
us
so
we
weren
t
able
to
achieve
a
simple
random
sample
that
is
a
representative
sample
we
have
therefore
described
our
sample
because
it
may
suggest
limits
to
generalizability
an
aggregate
description
of
the
participants
is
found
in
table
i
the
participants
are
described
individually
in
table
ii
independent
samples
t-tests
showed
that
there
were
no
statistically
significant
differences
between
the
feature-shy
and
feature-keen
participants
in
terms
of
gender
distribution
age
education
msoffice
expertise
or
number
of
years
using
msword
there
were
however
a
number
of
attributes
of
our
sample
that
lead
us
to
believe
that
it
was
not
fully
representative
on
average
the
participants
appeared
to
be
highly
educated
a
rating
of
5
equals
the
completion
of
an
undergraduate
degree
and
long-term
users
of
msword
there
were
no
administrative
assistants
roles
that
clearly
include
many
users
who
do
word
processing
although
an
earlier
study
we
conducted
did
include
them
we
do
not
have
a
definitive
reason
why
we
did
not
get
participants
in
these
roles
perhaps
the
call
for
participation
did
not
reach
these
groups
or
they
were
reached
but
individual
users
did
not
feel
that
they
could
participate
in
this
evaluation
another
likely
point
of
difference
between
our
sample
and
a
representative
sample
is
that
graduate
students
make
up
one
quarter
of
the
participants
this
is
easily
explained
by
the
fact
that
the
call
for
participation
was
sent
to
newsgroups
on
a
university
campus
4
experiment
protocol
we
prioritized
our
two
main
evaluation
goals
as
follows
1
to
understand
the
participants
personalization
behavior
with
the
new
design
2
to
compare
our
design
to
the
adaptive
interface
of
msw2k
a
commercial
interface
design
for
feature-rich
software
given
this
prioritization
a
field
experiment
was
chosen
instead
of
a
laboratory
experiment
because
it
was
expected
that
true
personalizing
behavior
would
be
significantly
more
likely
to
occur
with
users
doing
their
own
tasks
in
their
normal
work
context
rather
than
in
a
lab
setting
with
prescribed
tasks
that
would
likely
be
artificial
and
unfamiliar
to
the
extent
that
it
was
possible
we
did
introduce
some
controls
in
the
experiment
and
so
our
protocol
is
best
described
as
a
quasi-experimental
design
2
there
are
several
differences
between
the
two
interface
designs
that
we
compared
most
notably
the
degree
of
user
control
of
customization
and
the
use
of
a
dual
versus
single
interface
as
will
become
evident
our
study
was
not
designed
to
tease
these
factors
apart
we
instead
elected
to
examine
the
overall
efficacy
of
an
adaptable
approach
using
the
combination
of
factors
that
we
believed
offered
the
best
alternative
to
the
existing
adaptive
interface
isolating
the
impact
of
each
factor
once
we
were
able
to
show
that
the
features
together
offer
an
advantage
is
an
obvious
next
step
each
participant
was
involved
for
approximately
6
weeks
they
used
msw2k
prior
to
the
start
of
the
evaluation
worked
with
our
new
design
for
4
weeks
and
returned
to
msw2k
for
the
remaining
2
weeks
participants
met
with
the
experimenter
on
three
occasions
and
completed
a
series
of
short
on-line
questionnaires
q1
q8
to
assess
experience
with
the
software
and
a
final
in-depth
semistructured
interview
figure
3
provides
a
timeline
for
the
experiment
protocol
first
meeting
and
questionnaire
q1
the
participant
completed
a
printed
version
of
questionnaire
q1
that
assessed
the
participant
s
experience
with
msw2k
at
the
same
time
that
the
participant
was
filling
in
the
questionnaire
four
programs
were
installed
on
the
participant
s
machine--the
prototype
software
which
we
called
msw
personal
3
a
software
logger
for
capturing
usage
a
small
script
to
transfer
the
log
files
to
a
backup
server
on
the
internet
and
a
script
to
delete
the
prototype
in
the
event
of
any
technical
malfunction
each
participant
s
personal
interface
contained
only
six
functions
initially
exit
and
the
list
of
most-recently-used
files
in
the
file
menu
considered
a
single
function
open
and
save
on
the
standard
toolbar
and
font
and
font
size
on
the
formatting
toolbar
these
six
functions
were
selected
judgmentally
such
that
two
frequently
used
functions
were
included
for
each
of
the
file
menu
and
the
two
toolbars
questionnaires
q2
through
q6
these
questionnaires
assessed
msw
personal
q2
was
completed
within
2
days
of
the
first
meeting
and
was
intended
to
capture
the
participant
s
first
impression
of
msw
personal
q3
q4
q5
and
q6
were
completed
1
2
3
and
4
weeks
respectively
from
the
first
meeting
and
were
intended
to
capture
the
participant
s
experience
of
msw
personal
over
time
second
meeting
the
second
meeting
was
held
within
1
day
of
q6
being
completed
msw
personal
was
uninstalled
leaving
the
participant
with
msw2k
the
log
files
were
collected
on
diskette
questionnaire
q7
q7
assessed
the
participant
s
experience
of
msw2k
one
week
following
the
second
meeting
it
was
intended
to
capture
the
participant
s
reaction
to
returning
to
msw2k
questionnaire
q8
q8
asked
the
participant
to
rank
msw2k
and
msw
personal
in
terms
of
each
of
the
dependent
measures
2
weeks
following
the
second
meeting
in
contrast
to
the
first
seven
questionnaires
which
captured
participants
feedback
on
just
one
of
msw2k
or
msw
personal
q8
captured
feedback
on
both
interfaces
in
particular
rankings
of
the
interfaces
third
meeting
the
third
meeting
was
held
within
1
day
of
q8
being
completed
the
log
files
were
collected
on
diskette
and
the
participant
s
machine
was
completely
restored
to
the
state
it
was
in
prior
to
the
experiment
a
final
in-depth
semi-structured
debriefing
interview
was
conducted
with
each
participant
instructions
given
to
the
participants
in
advance
of
the
experiment
participants
were
only
told
that
some
changes
would
be
made
to
their
word
processing
software
but
they
were
not
told
the
nature
of
the
changes
at
the
first
meeting
they
were
told
that
a
new
version
of
the
software
had
been
installed--msw
personal--which
contained
two
interfaces
the
experimenter
toggled
between
the
two
interfaces
once
as
a
brief
demonstration
it
was
pointed
out
that
the
personal
interface
contained
very
few
functions
initially
but
that
functions
could
be
added
or
deleted
with
the
modify
button
the
process
of
personalizing
however
was
not
demonstrated
participants
were
told
that
there
was
no
right
or
wrong
way
to
use
the
interfaces
and
it
was
specifically
mentioned
that
they
could
choose
to
use
just
one
of
the
two
interfaces
and
essentially
ignore
the
other
or
they
could
switch
between
the
interfaces
in
any
way
that
suited
their
needs
participants
were
not
told
that
msw
personal
would
be
uninstalled
at
the
second
meeting
the
impression
intended
was
that
it
would
be
used
for
the
entire
duration
of
the
experiment
in
addition
to
providing
the
instructions
verbally
printouts
of
the
text
of
the
instructions
were
given
no
information
about
the
goals
or
objectives
of
the
evaluation
was
provided
to
the
participants
at
any
time
during
the
experiment
scheduling
in
order
to
ensure
the
timely
completion
of
questionnaires
and
meetings
an
individual
web
page
was
constructed
for
each
participant
that
contained
all
the
necessary
due
dates
as
well
as
urls
to
all
the
questionnaires
this
acted
as
a
shared
resource
between
the
researcher
and
each
participant
in
addition
email
reminders
were
sent
by
9
00
am
on
the
due
date
of
each
questionnaire
with
the
participant
s
web
page
url
directly
embedded
in
the
email
which
facilitated
quick
access
to
the
questionnaires
reminders
for
each
of
the
three
meetings
were
sent
one
business
day
in
advance
the
participants
web
pages
were
updated
regularly
to
reflect
completed
activities
there
was
some
flexibility
in
the
scheduling
if
a
participant
was
unable
to
complete
a
questionnaire
or
attend
a
meeting
on
the
scheduled
date
the
date
could
be
adjusted
slightly
adjustments
to
the
schedule
were
mostly
made
during
the
first
meeting
our
goal
with
respect
to
scheduling
was
to
avoid
any
confusion
about
the
time
and
dates
of
meetings
and
the
due
dates
of
questionnaires
in
general
we
were
very
successful
in
that
very
few
meetings
had
to
be
rescheduled
during
the
evaluation
and
there
were
few
questionnaires
that
arrived
late
compensation
each
participant
received
a
100
gift
certificate
for
a
local
department
store
as
compensation
in
addition
there
was
one
100
gift
certificate
awarded
as
a
prize
to
the
participant
who
completed
the
most
number
of
questionnaires
on
time
formal
design
the
logistical
constraints
in
conducting
this
experiment
in
the
field
precluded
the
counterbalancing
of
word
processor
conditions
the
design
is
a
2
personality
types
between
subjects
3
levels
levels
1
3
msw2k
level
2
msw
personal
within
subjects
design
where
level
2
is
nested
with
five
repetitions
the
fact
that
there
was
no
control
group
made
this
a
quasi-experimental
design
rather
than
an
experimental
design
campbell
and
stanley
1972
5
measures
the
dependent
measures
were
based
on
logging
data
and
data
collected
from
the
eight
questionnaires
from
the
logged
data
we
extracted
the
total
time
spent
doing
word
processing
the
time
spent
in
each
interface
the
number
of
toggles
between
interfaces
the
trace
of
the
modifications
made
to
the
personal
interface
the
trace
of
functions
used
and
summary
statistics
of
function
use
the
on-line
questionnaires
included
a
number
of
self-reported
measures
each
questionnaire
presented
the
same
series
of
13
statements
that
were
rated
on
a
five-point
likert
scale
strongly
disagree
disagree
neutral
agree
strongly
agree
the
statements
are
given
below
in
the
order
in
which
they
appear
in
the
questionnaires
ease
of
use
menu
control
learnability
navigation
this
software
is
easy
to
use
i
am
in
control
of
the
contents
of
the
menus
and
toolbars
i
will
be
able
to
learn
how
to
use
all
that
is
offered
in
this
software
navigating
through
the
menus
and
toolbars
is
easy
to
do
this
software
is
engaging
the
contents
of
the
menus
and
the
toolbars
match
my
needs
getting
started
with
this
version
of
the
software
is
easy
this
software
is
flexible
finding
the
options
that
i
want
in
the
menus
and
toolbars
is
easy
it
is
easy
to
make
the
software
do
exactly
what
i
want
discovering
new
features
is
easy
i
get
my
word
processing
tasks
done
quickly
with
this
software
this
software
is
satisfying
to
use
q2
through
q6
also
included
statements
specific
to
msw
personal
these
were
rated
on
the
same
five-point
likert
scale
personalizing
mechanism
easy
to
use
the
mechanism
for
adding/deleting
functions
to/from
my
personal
interface
is
easy
to
use
this
mechanism
for
adding/deleting
functions
to/from
my
personal
interface
is
intuitive
this
mechanism
for
adding/deleting
functions
to/from
my
personal
interface
is
flexible--i
can
modify
my
personal
interface
so
it
is
exactly
how
i
want
it
this
mechanism
for
switching
between
interfaces
is
easy
to
use
this
concept
of
having
two
interfaces
is
easy
to
understand
having
two
interfaces
is
a
good
idea
6
hypotheses
the
hypotheses
below
are
categorized
according
to
the
two
main
goals
of
the
evaluation
1
to
understand
the
participants
personalization
behavior
with
the
new
design
and
2
to
compare
our
design
to
the
adaptive
interface
of
msw2k
pilot
testing
in
the
field
with
4
participants
who
each
used
an
earlier
version
of
the
prototype
for
2
3
months
assisted
in
the
formulation
of
our
hypotheses
6
1
personalization
behavior
with
multiple-interfaces
design
we
wanted
to
understand
whether
users
could
use
the
msword
personal
design
effectively
effectiveness
relates
to
being
able
to
personalize
according
to
command
usage
in
a
way
that
is
not
overly
cumbersome
we
also
wanted
to
see
whether
msword
personal
is
sufficiently
flexible
to
accommodate
a
variety
of
personalization
strategies
h1
usage
hypothesis
the
majority
of
the
participants
will
choose
to
use
their
personal
interface--they
will
find
the
personalizing
mechanism
easy
to
use
intuitive
and
flexible
enough
such
that
they
will
use
the
mechanism
to
include
all
frequently
used
functions
and
will
spend
the
majority
of
their
time
in
their
personal
interface
h2
approaches
to
personalization
hypothesis
the
multiple-interfaces
design
will
allow
for
different
approaches
to
personalization
individual
differences
feature-shy
vs
feature-keen
will
influence
the
strategies
adopted
h3
growth
of
personal
interface
hypothesis
modifications
to
participants
personal
interfaces
will
be
dominated
by
additions
and
the
size
of
the
personal
interfaces
will
reach
a
steady
state
users
will
not
continually
need
to
modify
their
personal
interfaces
h4
modification
triggers
hypothesis
related
to
growth
there
will
be
identifiable
triggers4
that
prompt
participants
to
modify
their
personal
interface
for
example
and
most
obviously
there
will
be
an
initial
trigger
to
add
functions
because
the
personal
interface
will
otherwise
be
almost
unusable
the
last
hypothesis
while
not
directly
related
to
effectiveness
and
flexibility
was
deemed
important
because
if
such
triggers
could
be
identified
they
could
provide
a
design
basis
for
user-assisted
personalization
6
2
comparison
to
adaptive
interface
we
hypothesized
about
how
the
multiple-interfaces
design
of
msw
personal
compares
to
one
particular
instance
of
an
adaptive
design
namely
msw2k
while
we
expected
differences
in
satisfaction
we
also
expected
that
because
users
operate
within
a
small
individually
constructed
interface
they
would
find
navigation
easier
and
would
feel
a
greater
overall
sense
of
control
with
msw
personal
we
also
expected
the
ability
to
learn
features
would
be
positively
impacted
because
operating
in
a
user-controlled
small
interface
acts
as
a
training
wheels
interface
to
the
full
interface
with
many
of
these
hypotheses
we
expected
individual
differences
to
play
a
role
h5
satisfaction
hypothesis
feature-shy
participants
will
be
more
satisfied
with
msw
personal
than
with
msw2k
the
feature-shy
will
be
more
satisfied
than
the
feature-keen
with
msw
personal
h6
navigation
hypothesis
both
feature-shy
and
feature-keen
participants
will
feel
that
they
are
better
able
to
navigate
the
menus
and
toolbars
with
msw
personal
than
with
msw2k
h7
control
hypothesis
both
feature-shy
and
feature-keen
participants
will
feel
a
better
sense
of
control
with
msw
personal
than
with
msw2k
h8
learnability
hypothesis
feature-shy
participants
will
feel
that
they
are
better
able
to
learn
the
available
features
with
msw
personal
than
with
msw2k
h9
three-way
comparison
hypothesis
when
asked
to
compare
their
overall
preference
for
msw
personal
msw2k
and
msw2k
with
the
adaptive
menus
turned
off
standard
all-in-one
interface
feature-shy
participants
will
prefer
the
multiple-interfaces
design
to
an
all-in-one
design
but
will
prefer
all-in-one
to
adaptive
feature-keen
participants
will
prefer
all-in-one
to
both
the
adaptive
and
multiple-interfaces
designs
7
results
most
of
the
logging
data
for
one
participant
participant
9
was
lost
due
to
technical
reasons
we
collected
his
interface
toggling
and
personal
interface
modification
data
but
missed
his
function
usage
data
where
this
is
relevant
we
note
n
19
otherwise
n
20
can
be
assumed
unless
otherwise
stated
the
analysis
in
section
7
2
which
focuses
on
uncovering
the
participants
personalization
behavior
with
msw
personal
relies
on
descriptive
statistics
the
analysis
in
section
0
which
focuses
on
the
comparison
between
our
design
and
the
adaptive
design
in
word
2000
includes
both
inferential
and
descriptive
statistics
findings
that
are
p
0
05
are
deemed
to
be
significant
however
due
to
the
qualitative
nature
of
our
field
evaluation
our
inability
to
control
many
of
the
variables
and
our
small
number
of
participants
we
accept
a
looser
criterion
for
borderline
significant
results
using
the
range
of
0
05
p
0
10
many
qualitative
studies
are
done
on
so
few
participants
that
no
analyses
can
be
done
we
are
able
to
do
some
preliminary
statistical
analyses
to
suggest
phenomena
for
future
investigation
both
sections
conclude
with
discussions
that
incorporate
the
qualitative
questionnaire
and
interview
data
7
1
experience
of
multiple-interfaces
design
our
primary
goal
was
to
understand
personalization
behavior
with
the
new
design
we
examine
the
results
for
these
hypotheses
first
h1
usage
hypothesis
in
the
4
weeks
msword
personal
was
installed
the
19
participants
each
spent
on
average
596
minutes
word
processing
sd
554
min
histogram
in
figure
4
fourteen
of
the
participants
74
spent
50
or
more
of
their
word
processing
time
in
their
personal
interface
histogram
in
figure
5
these
same
participants
added
all
frequently
used
functions
to
their
personal
interface
those
functions
used
on
at
least
half
of
the
usage
days
there
are
approximately
203
functions
in
msw2k
that
users
could
include
in
their
personal
interfaces
5
participants
did
add
most
of
their
frequently
used
functions
the
more
frequently
a
function
was
used
the
more
likely
it
was
added
as
shown
in
figure
6
for
example
19
participants
had
on
average
29
8
functions
that
were
used
on
25
or
fewer
of
their
usage
days
and
on
average
participants
added
19
7
66
of
those
functions
to
their
personal
interfaces
the
percentage
of
functions
added
increases
to
90
96
and
100
respectively
for
the
next
three
quartiles
although
the
majority
of
the
frequently
used
functions
were
added
to
the
personal
interfaces
there
were
relatively
few
such
functions
figure
6
shows
that
on
average
participants
only
had
0
7
functions
that
were
used
75
or
more
of
their
usage
days
6
questionnaire
data
indicated
that
participants
found
the
personalizing
mechanism
easy
to
use
intuitive
and
flexible
these
three
attributes
had
mean
ratings
out
of
5
of
4
3
4
1
and
4
0
respectively
note
that
all
the
data
reported
above
represents
aggregate
data
for
both
the
feature-keen
and
the
feature-shy
participants
independent
sample
t-tests
were
first
run
to
see
if
there
were
any
significant
differences
between
the
groups
of
participants
for
the
dependent
measures
investigated
and
no
statistically
significant
differences
were
revealed
the
largest
personal
interface
was
75
functions
by
a
feature-keen
participant
there
was
no
correlation
between
time
spent
using
the
msword
personal
and
the
number
of
functions
added
to
the
personal
interface
terminology
as
follows
frequently-used
add
frequently-used
functions
only
all
add
all
functions
that
are
used
upfront
add
majority
of
functions
right
away
as-you-go
add
functions
as
they
are
needed
none
participant
did
not
personalize
gave
up
abandoned
desired
approach
n
20
summary
participants
personalized
their
interfaces
according
to
the
frequency
with
which
they
used
functions
74
of
the
participants
spent
50
or
more
of
their
time
in
their
personal
interface
and
participants
agreed
that
the
personalizing
mechanism
was
easy
to
use
intuitive
and
flexible
hypothesis
supported
yes
h2
approach
to
using
two
interfaces
participants
were
not
told
how
they
should
use
the
two
interfaces
in
msw
personal
and
were
therefore
able
to
approach
it
in
any
way
that
met
their
needs
analysis
of
the
log
data
and
the
debriefing
interviews
allows
us
to
approximately
discern
the
general
approaches
that
participants
took
to
constructing
their
personal
interfaces
in
general
each
approach
can
be
broken
down
into
two
independent
components
1
which
functions
were
added
namely
only
the
most
frequently-used
functions
or
all
used
functions
and
2
when
functions
were
added
namely
upfront
within
the
first
few
days
of
usage
or
gradually
as
functions
were
needed
as-you-go
the
top
of
table
iii
gives
a
detailed
breakdown
and
an
overall
summary
is
given
at
the
bottom
of
the
table
we
look
first
at
which
functions
were
added
including
both
participant
groups
we
see
that
participants
were
almost
twice
as
likely
to
add
all
used
functions
12
participants
as
only
the
frequently
used
functions
7
participants
participants
who
added
all
used
functions
generally
expected
to
use
their
personal
interface
exclusively
whereas
those
who
added
only
regularly
used
functions
expected
to
switch
to
the
full
interface
when
irregularly
used
functions
were
needed
taking
individual
differences
into
account
feature-shy
were
unexpectedly
almost
evenly
divided
between
only
including
frequently
used
functions
4
participants
and
all
functions
5
participants
we
thought
feature-shy
participants
would
want
as
minimal
an
interface
as
possible
that
is
to
only
add
their
frequently
used
functions
feature-keen
participants
on
the
other
hand
were
more
than
twice
as
likely
to
want
all
their
used
functions
in
their
personal
interface
7
participants
rather
than
just
the
ones
they
used
frequently
3
participants
their
preference
for
all
functions
was
expected
in
terms
of
when
the
functions
were
added
including
both
participant
groups
6
participants
took
the
approach
of
adding
the
great
majority
of
functions
that
they
expected
to
add
upfront
and
then
only
rarely
adding
additional
functions
by
contrast
13
participants
took
the
approach
of
adding
some
functions
in
the
beginning
and
then
gradually
adding
additional
functions
as-you-go
accounting
for
individual
differences
feature-shy
participants
clearly
favored
the
add
as-you-go
strategy
7
participants
to
the
add
upfront
strategy
2
participants
feature-keen
participants
also
appeared
to
favor
the
add
as-you-go
strategy
6
participants
to
4
but
not
as
decisively
as
the
feature-shy
seven
participants
gave
up
on
their
desired
approach
to
some
degree
for
most
this
meant
that
they
stopped
personalizing
or
only
added
very
few
functions
beyond
their
first
few
days
of
using
msw
personal
these
participants
used
their
personal
interface
to
the
extent
that
they
could
but
would
then
switch
to
the
full
interface
when
a
function
not
available
in
their
personal
interface
was
needed
rather
than
taking
the
time
to
add
it
for
example
participant
1
is
categorized
as
all
as-you-go
gave
up
she
wanted
to
have
all
the
functions
she
used
in
her
personal
interface
but
in
the
end
she
realized
she
was
using
a
lot
more
functions
than
she
expected
some
of
which
she
was
learning
for
a
new
contract
she
started
during
the
evaluation
she
did
continue
to
personalize
throughout
the
experiment
but
ended
up
just
adding
the
most
frequently
used
functions
which
was
not
her
desired
approach
more
typical
behavior
of
participants
who
gave
up
is
described
here
by
a
participant
who
is
categorized
as
all
as-you-go
gave
up
i
would
start
out
of
the
personal
at
the
beginning
i
was
adding
things
pretty
regularly
to
the
personal
but
i
felt
that
i
was
just
continually
adding
things
and
so
eventually
i
would
just
start
out
and
use
the
personal
as
long
as
it
was
convenient
and
then
i
would
just
switch
once
i
felt
like
i
needed
to
add
another
function
interview
participant
10
participant
14
was
categorized
as
none
gave
up
because
he
used
the
full
interface
almost
exclusively
and
clearly
wasn
t
willing
to
spend
the
time
to
explore
his
personal
interface
it
was
obvious
from
his
comments
in
the
debriefing
interview
that
he
really
did
not
understand
the
concept
of
a
personal
interface
and
by
the
time
of
the
interview
he
had
completely
forgotten
that
he
himself
had
added
four
functions
to
his
personal
interface
during
the
first
meeting
summary
participants
adopted
various
approaches
to
personalization
in
terms
of
which
functions
were
added
to
their
personal
interfaces
and
when
they
were
added
no
one
strategy
dominated
outright
all
as-you-go
was
the
most
popular
8
participants
but
the
other
three
combinations
were
also
adopted
frequently-used
upfront
2
participants
frequently-used
as-you-go
5
participants
and
all
upfront
4
participants
individual
differences
appear
to
play
a
role
in
the
strategy
adopted
but
not
a
decisive
role
feature-keen
participants
were
more
likely
to
add
all
used
functions
rather
than
just
their
frequently
used
functions
seven
participants
gave
up
to
some
extent
on
their
preferred
approach
hypothesis
supported
partially
the
multiple-interfaces
design
did
allow
for
different
approaches
to
personalization
but
individual
differences
did
not
strongly
influence
strategy
h3
growth
of
personal
interface
hypothesis
the
size
of
participants
personal
interfaces
with
the
exception
of
seven
deleted
functions
increased
monotonically
all
participants
added
functions
and
only
3
participants
deleted
a
combined
total
of
seven
functions
for
the
purposes
of
understanding
how
personal
interfaces
evolved
we
directed
our
attention
exclusively
to
the
13
participants
who
did
not
give
up
their
desired
approach
to
personalization
as
these
participants
set
up
their
personal
interface
in
a
way
that
met
their
needs
this
group
includes
participant
9
for
whom
we
only
have
partial
logging
data
and
so
we
omit
his
data
from
this
analysis
which
leaves
12
participants
for
this
group
of
participants
there
was
an
initial
period
when
modifications
were
made
regularly
this
series
can
be
defined
by
a
first
modification
followed
by
subsequent
modifications
that
were
at
most
2
usage
days
from
the
previous
modification
table
iv
shows
that
the
average
duration
of
this
initial
period
lasted
2
8
days
and
for
the
participant
who
had
the
longest
initial
period
it
only
lasted
5
days
of
the
total
number
of
functions
that
these
participants
added
during
the
4
weeks
on
average
each
had
added
82
of
his/her
total
by
the
end
of
the
initial
period
on
average
participants
personalized
on
3
8
of
the
days
that
word
processing
occurred
with
the
participant
who
most
frequently
personalized
doing
so
on
6
days
the
size
of
participants
personal
interfaces
did
approximately
reach
steady
state
in
that
there
was
a
point
at
which
the
size
of
the
personal
interfaces
did
not
increase/decrease
by
more
than
10
we
expected
this
point
to
be
within
the
initial
period
for
the
majority
of
the
12
participants
the
results
show
however
that
the
steady
state
point
was
reached
by
the
end
of
the
initial
period
for
only
half
of
the
12
participants
for
2
participants
steady
state
was
equal
to
the
initial
period
and
for
4
it
was
less
than
the
initial
period
on
average
this
steady
state
period
came
after
4
8
days
of
usage
which
was
on
average
31
of
the
way
through
the
4
weeks
in
which
msw
personal
was
used
on
average
the
last
modification
came
75
through
the
4
weeks
the
data
can
be
viewed
on
a
day-by-day
basis
the
cumulative
total
number
of
functions
added
by
the
12
participants
was
485
figure
7
shows
that
within
the
first
2
days
that
msw
personal
was
used
81
of
all
485
functions
had
been
added
summary
personalization
was
dominated
by
additions
participants
added
on
average
90
or
more
of
the
functions
that
they
were
going
to
add
within
4
8
usage
days
but
participants
were
on
average
not
finished
personalizing
until
almost
three
quarters
of
the
way
through
the
4
weeks
participants
were
not
continually
personalizing
hypothesis
supported
yes
h4
modification
triggers
hypothesis
in
order
to
evaluate
this
hypothesis
we
began
by
identifying
the
patterns
of
usage
with
respect
to
addition
in
particular
whether
a
function
was
used
before
or
after
it
was
added
and
if
so
how
soon
it
was
used
before
or
after
if
was
added
a
detailed
analysis
of
these
usage
patterns
enabled
us
to
define
four
triggers
for
addition
immediate-need
trigger
the
user
has
an
immediate
need
to
use
a
function
that
is
not
currently
in
his/her
personal
interface
and
therefore
adds
it
this
is
shown
clearly
in
the
log
file
by
the
pattern
add
function
x
followed
directly
by
use
function
x
initial
trigger
the
user
desires
to
add
functions
when
first
using
msw
personal
any
function
added
within
the
first
2
days
of
usage
that
does
not
satisfy
the
immediate-need-trigger
satisfies
this
trigger
previously-used
trigger
the
user
has
already
used
a
function
and
expects
to
use
it
in
the
future
any
function
that
has
been
used
before
it
is
added
and
does
not
satisfy
the
immediate-need
or
initial
triggers
satisfies
this
trigger
future-use
trigger
the
user
expects
to
use
a
function
in
the
future
and
so
adds
it
to
the
personal
interface
any
function
that
does
not
satisfy
any
of
the
first
three
triggers
satisfies
this
trigger
table
v
summarizes
our
findings
with
respect
to
triggers
for
the
addition
of
functions
we
see
that
the
initial
trigger
and
the
immediate-need
trigger
together
accounted
for
almost
86
of
the
functions
added
the
previously
used
and
future-use
triggers
accounted
for
the
addition
of
the
remaining
functions
which
were
typically
added
while
an
immediately-needed
function
was
being
added
there
were
two
triggers
for
deletion
mistaken-addition
trigger
a
function
was
added
by
mistake
it
was
not
intended
or
was
the
wrong
function
non-use
trigger
a
function
is
not
being
used
only
3
participants
deleted
a
combined
total
of
seven
functions
one
two
and
four
functions
respectively
in
all
seven
cases
the
function
was
deleted
directly
after
it
was
added
when
queried
in
the
debriefing
interview
the
participant
who
had
deleted
four
functions
indicated
that
she
had
been
testing
the
personalizing
mechanism
and
the
other
2
participants
said
that
the
deleted
functions
had
initially
been
added
by
mistake
thus
the
mistaken-addition
trigger
applied
to
the
deletion
of
three
functions
and
the
non-use
trigger
did
not
apply
to
any
deleted
functions
one
might
initially
assume
that
the
lack
of
the
non-use
trigger
is
explainable
by
the
fact
that
msw
personal
was
only
used
for
one
month
to
counter
this
however
the
great
majority
of
participants
indicated
in
the
debriefing
interview
that
they
would
not
likely
have
bothered
to
delete
functions
even
if
they
were
not
being
used
finally
to
double
check
that
our
participants
were
not
simply
customizing
when
reminded
of
their
study
participation
we
investigated
a
potential
correlation
between
the
times
when
subjects
personalized
their
interfaces
and
times
when
questionnaires
were
due
for
questionnaires
q1
and
q2
68
and
63
of
the
subjects
respectively
did
perform
at
least
one
customization
on
the
days
when
those
questionnaires
were
answered
this
is
not
surprising
however
given
the
minimal
starting
personal
interface
and
the
strong
initial
trigger
excluding
customization
that
overlapped
with
q1
and
q2
there
was
only
an
average
17
3
overlap
of
customization
with
q3
through
q6
n
19
if
we
consider
only
those
who
didn
t
give
up
that
average
is
only
slightly
higher
at
20
1
n
12
thus
the
data
suggests
that
personalization
was
not
being
triggered
by
questionnaire
completion
but
was
done
at
various
times
during
the
roughly
six
days
preceding
the
days
when
each
of
the
q3
q4
q5
and
q6
questionnaires
were
answered
summary
the
initial
trigger
accounted
for
the
majority
of
additions
77
when
participants
added
an
immediately
needed
function
9
they
would
typically
also
add
a
function
they
expected
to
use
in
the
future
13
or
one
they
had
already
used
1
seven
functions
were
deleted
four
to
test
the
mechanism
and
three
because
they
had
originally
been
added
by
mistake
hypothesis
supported
yes
discussion
and
additional
qualitative
feedback
here
we
discuss
our
findings
about
personalization
behavior
and
specifically
how
the
multiple-interfaces
design
impacted
this
behavior
we
include
selected
comments
made
by
the
participants
about
msw
personal
both
from
the
open-ended
sections
in
questionnaires
q1
through
q8
and
in
the
final
debriefing
interview
the
goal
is
to
bring
the
quantitative
data
to
life
by
placing
it
in
the
context
of
the
qualitative
data
approach
to
personalization
msword
personal
appears
to
provide
sufficient
flexibility
to
allow
users
to
personalize
as
they
see
fit
of
the
13
participants
who
did
not
give
up
there
was
almost
an
even
distribution
between
the
four
combinations
of
when
and
which
frequently-used
upfront
2
frequently-used
as-you-go
4
all
upfront
4
all
as-you-go
3
we
speculate
that
this
flexibility
played
a
key
role
in
the
success
of
the
multiple-interfaces
design
having
said
that
it
is
interesting
to
note
that
none
of
the
participants
who
took
the
approach
of
adding
functions
upfront
ended
up
giving
up
this
suggests
that
personalizing
upfront
may
be
a
more
effective
strategy
than
as-you-go
user-assisted
personalization
four
participants
suggested
having
some
automated
assistance
in
the
construction
of
their
personal
interfaces
but
some
of
them
did
acknowledge
loss
of
control
as
a
potential
negative
side
effect
interestingly
the
quote
below
is
from
participant
7
he
was
annoyed
by
the
adaptive
menus
and
yet
he
was
still
hopeful
that
the
personal
interface
could
be
built
automatically
so
if
i
could
have
something
where
it
automatically
creates
the
personal
based
on
my
use
without
having
to
point
and
click
buttons
around
which
is
easy
enough
but
a
bit
of
a
pain
if
it
automatically
could
do
it
for
me
so
that
over
time
i
created
a
personal
interface
by
default
so
my
usage
pattern
creates
it
without
that
annoying
short
menu
stuff
that
would
be
nice
because
then
i
wouldn
t
actually
have
to
think
about
it
i
d
just
use
word
and
it
would
create
it
as
i
go
interview
participant
7
it
is
not
yet
clear
whether
fully
automated
interface
construction
is
effective
but
user-assisted
personalization
has
potential
knowledge
about
customization
triggers
should
play
a
role
in
how
the
assistance
is
designed
we
return
to
this
topic
in
the
final
section
of
this
paper
role
of
the
full
interface
we
checked
whether
having
the
full
interface
was
considered
important
customizable
interfaces
do
not
generally
provide
easy
access
to
the
entire
set
of
functionality
except
through
a
full
reset
which
results
in
loss
of
the
customization
effort
we
found
that
having
the
full
interface
was
generally
well
liked
while
there
were
two
participants
who
did
not
make
use
of
it
at
all
the
great
majority
did
use
it
at
least
once
and
appreciated
having
it
available
example
representative
comments
include
i
d
always
want
to
have
the
full
interface
to
go
to
just
as
like
a
baseline
kind
of
thing
because
that
word
the
full
interface
was
there
it
was
this
safety
net
of--yah
i
know
it
s
over
there
if
i
ever
do
need
it
anyways
interview
participant
11
i
like
the
security
blanket
of
having
the
full
interface
but
over
a
longer
period
of
time
i
probably
would
have
extinguished
my
use
of
it
pretty
much
interview
participant
17
what
i
would
really
hate
is
if
the
personal
one--if
you
couldn
t
go
back
and
forth
the
fact
that
you
could
back
and
forth
and
that
it
was
so
easy
to
go
back
and
forth
that
was
very
good
interview
participant
13
so
although
our
evaluation
did
not
assess
an
easy-to-customize
personal
interface
in
the
absence
of
an
easy-to-access
full
interface
our
data
suggests
that
the
full
interface
is
an
integral
part
of
the
design
and
that
personalizing
behavior
would
be
significantly
altered
without
having
a
full
interface
available
initial
personal
interface
we
initialized
the
personal
interface
to
be
small
with
the
goal
of
forcing
customization
two
participants
felt
that
the
personal
interface
should
have
initially
included
more
functions--those
functions
that
are
used
by
everyone
for
example
if
there
was
an
interface
maybe
it
differs
by
industry
i
don
t
know
but
with
the
most
common
functions--like
print
everyone
prints
everyone
makes
things
bold
so
if
there
was
a
kind
of
pre-selected
simple
menu
that
wasn
t
overwhelming
i
would
maybe
be
tempted
to
prefer
something
like
that
like
it
would
make
my
decision
harder
to
decide
should
i
use
the
full
2000
interface
or
the
personal
interface
and
then
be
able
to
switch
that
distinction
would
be
a
little
less
clear
in
my
mind
so
if
there
was
a
pre-selected
bunch
of
functions
that
everyone
happens
to
use
and
i
happen
to
fit
into
that
everyone
category
interview
participant
3
the
implicit
assumption
by
these
participants
is
that
such
a
set
exists
research
by
greenberg
into
unix
command
usage
showed
however
that
there
is
only
minimal
command
overlap
even
between
participants
within
the
same
group
who
are
performing
similar
tasks
greenberg
1993
we
expect
that
similar
results
would
be
found
for
msword
command
usage
7
but
additional
research
would
be
required
to
substantiate
that
claim
assuming
for
the
moment
the
existence
of
a
reasonable
set
of
functions
that
could
be
used
to
initialize
the
personal
interface
it
would
be
interesting
to
see
how
personalizing
behavior
might
change
with
such
a
relatively
large
initial
personal
interface
on
the
one
hand
users
would
not
have
to
take
the
time
to
do
any
initial
customization
on
the
other
hand
users
would
not
be
taking
ownership
of
their
personal
interfaces
from
the
outset
which
could
diminish
their
overall
engagement
in
the
personalization
process
another
possible
research
avenue
would
be
to
investigate
appropriate
initial
personal
interfaces
based
on
individual
differences
feature-keen/shy
usability
of
msword
personal
our
participants
identified
some
basic
usability
improvements
to
the
implementation
of
the
personalizing
mechanism
three
participants
commented
that
it
was
somewhat
cumbersome
the
confirming
dialog
box
that
appears
after
the
selection
of
each
function
was
seen
to
be
unnecessary
for
example
the
double
menu
that
you
get
i
found
confusing
a
little
bit
but
it
was
easy
to
use
it
s
a
little
clunky
interview
participant
17
i
mean
it
wasn
t
difficult
it
was
just
you
had
to
click
and
you
had
to
click
again
and
click
again
and
go
back
and
go
forth--it
was
just
bulky
interview
participant
22
three
participants
wanted
to
be
able
to
add
an
entire
menu
at
once
the
current
design
requires
each
menu
item
to
be
added
one
at
a
time
and
because
of
the
number
of
steps
involved
this
can
be
time
consuming
if
the
majority
of
a
menu
is
desired
four
participants
felt
that
msw
personal
was
a
good
start
but
in
addition
to
simply
selecting
a
subset
of
functions
for
their
personal
interfaces
they
wanted
to
be
able
to
restructure
the
menu
hierarchies
i
would
like
to
be
able
to
rewrite
the
stupid
menu
structure
of
the
ms
word
program
not
just
select
the
options
that
i
want
within
the
stupid
tree
structure
q4
participant
7
i
would
have
liked
to
put
things
in
different
places
you
know
and
this
is
bad
because
i
do
this
in
word
because
i
don
t
like
what
they
have
decided
is
on
this
menu
interview
participant
11
both
of
these
participants
were
surprised
when
they
were
informed
at
the
end
of
the
evaluation
that
this
restructuring
functionality
was
available
through
msword
s
native
customization
facility
it
is
worth
noting
that
these
comments
were
made
by
participants
7
and
11
both
of
whom
are
expert
longterm
users
of
msword
one
participant
requested
the
ability
to
have
more
than
two
interfaces--she
wanted
different
personal
interfaces
related
to
the
different
tasks
she
performed
one
thing
that
would
have
been
cool
is
if
i
could
have
had
different
settings
like
if
you
have
the
default
word
and
then
a
personal
because
i
work
on
charts
and
i
work
on
just
regular
reports
if
i
could
have
two
kind
of
different
settings--say
this
is
going
to
be
my
flowchart
settings
and
those
would
have
draw
all
the
draw
and
shape
tools
and
everything
and
then
this
is
my
report
interface
and
that
would
just
have
regular
stuff
that
would
have
been
cool
and
i
don
t
know
if
a
lot
of
people
work
like
that
because
sometimes
i
m
just
writing
reports
or
writing
proposals
and
stuff
like
that
and
then
other
times
i
m
just
doing
very
different
things
and
i
need
to
switch
my
orientation
around
and
like
have
all
this
drawing
stuff
and
that
just
doesn
t
fit
with
regular
writing
interview
participant
11
8
our
design
goal
for
the
customization
mechanism
was
to
make
it
straightforward/understandable
so
we
opted
for
a
design
that
offered
only
basic
functionality
adding/deleting
functions
and
that
could
be
learned
quickly
through
trial
and
error
despite
the
comments
above
all
participants
noted
how
easy
it
was
to
use
the
addition/deletion
mechanism
thus
we
believe
that
it
was
easy
to
use
in
the
sense
that
it
was
easy
to
figure
out
what
to
do
and
no
errors
occurred
but
there
were
too
many
steps
required
one
participant
pinpointed
our
trade-off
the
add/delete
procedure
seems
slow
and
redundant
for
some
reason
but
is
rather
idiot-proof
we
could
rectify
the
clunkiness
by
removing
the
confirmation
dialog
box
and
designing
a
new
form
of
menu
such
that
when
the
user
is
selecting
items
from
a
menu
to
add
to
the
personal
interface
the
menu
stays
open
and
check
boxes
appear
adjacent
to
each
item
indicating
its
availability
in
the
personal
interface
currently
in
order
to
add
a
menu
item
the
user
selects
the
item
as
in
normal
menu
usage
after
selection
the
menu
disappears
and
the
user
must
reopen
the
menu
in
order
to
add
another
item
we
believe
that
personalization
was
affected
by
the
design
of
the
personalizing
mechanism
some
participants
would
not
likely
have
given
up
if
functions
could
have
been
added
more
quickly
some
wanted
more
flexibility
but
to
support
menu
hierarchy
restructuring
and
more
than
one
personal
interface
would
likely
make
the
personalizing
mechanism
inaccessible
to
nonadvanced
users
the
native
msword
customization
facility
does
allow
for
some
of
this
but
relative
to
our
personalizing
mechanism
it
requires
substantially
more
skill
to
use
we
had
expected
the
flexibility
ratings
recorded
in
the
questionnaires
to
reflect
the
limitations
of
the
personalizing
mechanism
but
the
participants
rated
flexibility
4
0
out
of
5
0
so
although
some
users
did
articulate
preferences
for
additional
flexibility
the
quantitative
data
shows
that
the
mechanism
was
sufficiently
flexible
for
most
participants
one
alternative
design
to
explore
would
be
two
levels
of
customization
basic
and
advanced
greater
flexibility
would
be
available
through
the
advanced
level
but
users
would
by
default
start
in
the
basic
level
individual
differences
the
expected
differences
between
the
feature-shy
and
the
feature-keen
participants
did
not
play
out
in
any
substantial
way
in
how
they
personalized
and
used
msw
personal
and
what
they
had
to
say
about
their
experience
using
it
significant
differences
between
the
two
groups
of
participants
did
appear
however
in
terms
of
how
msw
personal
was
compared
to
other
interfaces
as
will
be
shown
in
the
next
section
7
2
comparison
with
the
adaptive
interface
we
turn
now
to
the
remaining
hypotheses
which
cover
our
secondary
evaluation
goal
namely
to
compare
the
multiple-interfaces
design
of
msw
personal
to
the
adaptive
interface
of
msw2k
the
first
four
of
these
hypotheses
h5
h8
compare
the
two
interfaces
with
respect
to
satisfaction
navigation
control
and
learning
the
means
of
each
of
these
four
dependent
measures
at
q1
through
q7
separated
by
personality
type
are
given
in
figure
8
a
series
of
three
factorial
anovas
analysis
of
variance
was
run
to
test
for
significant
differences
1
q1
vs
q6
compares
measures
after
extended
time
in
each
condition
q1
responses
reflect
usage
of
1
month
or
more
with
msw2k
q6
reflects
1
month
s
use
of
msw
personal
2
q6
vs
q7
compares
measures
as
an
initial
reaction
of
returning
to
msw2k
after
1
month
s
use
of
msw
personal
3
q2
q3
q4
q5
q6
compares
measures
at
regular
intervals
during
4-week
usage
of
msw
personal
in
addition
to
reporting
statistical
significance
we
report
the
effect
size
etasquared
2
which
is
a
measure
of
the
magnitude
of
the
effect
of
a
difference
that
is
independent
of
sample
size
both
landauer
1997
and
vicente
and
torenvliet
2000
note
that
effect
size
is
often
more
appropriate
than
statistical
significance
in
applied
research
such
as
human-computer
interaction
the
commonly
accepted
metric
for
interpreting
eta-squared
is
0
01
is
a
small
effect
0
06
is
medium
and
0
14
is
large
h5
satisfaction
hypothesis
the
msword
versions
impacted
the
satisfaction
of
the
two
groups
of
participants
differently
figure
8
there
was
a
borderline
significant
cross-over
interaction
for
q1
vs
q6
f
1
18
4
12
mse
0
98
p
0
057
2
0
19
prompting
us
to
test
the
simple
effects
for
each
group
of
participants
independently
the
q1
vs
q6
comparison
was
not
significant
for
the
feature-keen
participants
however
the
increase
in
satisfaction
was
borderline
significant
for
the
feature-shy
f
1
9
3
65
mse
1
34
p
0
089
2
0
29
two
further
tests
compared
the
satisfaction
of
the
feature-shy
participants
to
the
feature-keen
participants
at
q1
and
then
at
q6
the
feature-shy
were
found
to
be
borderline
significantly
less
satisfied
than
the
feature-keen
while
using
msw2k
at
q1
t
18
-2
04
p
0
056
however
there
was
no
significant
difference
detected
between
the
two
groups
while
using
msw
personal
at
q6
when
participants
returned
to
msw2k
q6
to
q7
the
feature-shy
appear
to
have
dropped
in
satisfaction
and
the
feature-keen
had
effectively
no
change
but
this
cross-over
interaction
was
not
significant
summary
the
analysis
suggests
that
the
feature-shy
participants
were
less
satisfied
than
the
feature-keen
participants
when
using
msw2k
however
the
feature-shy
participants
experienced
an
increase
in
satisfaction
while
using
msw
personal
the
feature-keen
participants
did
not
experience
any
change
in
satisfaction
when
they
switched
to
msw
personal
hypothesis
supported
partially
feature-shy
participants
were
more
satisfied
with
msw
personal
than
with
msw2k
but
they
were
not
more
satisfied
with
msword
personal
than
the
feature-keen
participants
h6
navigation
hypothesis
the
version
of
msword
had
a
significant
main
effect
on
participants
perceived
ability
to
navigate
in
both
the
q1
vs
q6
comparison
f
1
18
5
76
mse
1
05
p
0
027
2
0
24
and
the
q6
vs
q7
comparison
f
1
18
8
02
mse
1
22
p
0
011
2
0
31
figure
8
both
comparisons
favored
msw
personal
there
was
a
borderline
significant
learning
effect
in
q2
through
q6
f
4
72
2
38
mse
0
18
p
0
06
2
0
12
indicating
that
navigation
became
easier
over
time
unsurprisingly
none
of
the
post
hoc
pairwise
comparisons
with
the
bonferonni
error
correction
were
significant
summary
the
analysis
suggests
that
both
the
feature-keen
and
the
featureshy
participants
found
it
easier
to
navigate
the
menus
and
the
toolbars
using
msw
personal
than
msw2k
hypothesis
supported
yes
h7
control
hypothesis
the
results
of
the
q1
vs
q6
comparison
of
control
are
dominated
by
a
borderline
significant
interaction
f
1
18
4
38
mse
0
82
p
0
051
2
0
20
figure
8
testing
the
simple
effects
found
the
q1
vs
q6
comparison
to
be
nonsignificant
for
the
feature-keen
participants
however
the
feature-shy
perceived
a
significant
increase
in
control
f
1
9
11
17
mse
0
64
p
0
009
2
0
55
two
further
tests
compared
control
for
the
feature-shy
participants
to
the
feature-keen
participants
at
q1
and
then
at
q6
the
feature-shy
reported
significantly
less
control
than
the
feature-keen
while
using
msw2k
at
q1
t
18
-2
72
p
0
014
however
there
was
no
significant
difference
detected
between
the
two
groups
while
using
msw
personal
at
q6
there
was
a
main
effect
for
control
from
q6
to
q7
f
1
18
5
89
mse
0
51
p
0
026
2
0
25
suggesting
that
both
groups
of
participants
felt
a
loss
of
control
when
returning
to
msw2k
the
statement
being
rated
reflects
a
participant
s
general
sense
of
control
over
the
software
and
not
simply
their
control
of
the
menus
and
toolbars
summary
the
analysis
suggests
that
at
the
outset
the
feature-shy
participants
felt
that
they
were
less
in
control
of
the
msw2k
software
than
did
the
feature-keen
participants
however
the
feature-shy
participants
experienced
an
increase
in
control
with
msw
personal
the
feature-keen
participants
did
not
experience
a
change
in
control
when
they
switched
to
msw
personal
both
groups
of
participants
appear
to
have
experienced
a
loss
of
control
when
they
switched
back
to
msw2k
after
having
used
msw
personal
for
4
weeks
hypothesis
supported
partially
feature-shy
participants
felt
a
better
sense
of
control
with
msword
personal
but
this
was
not
the
case
for
the
feature-keen
participants
h8
learnability
hypothesis
in
the
q1
vs
q6
comparison
the
msword
version
had
a
borderline
significant
main
effect
on
learnability
f
1
18
4
13
mse
0
61
p
0
057
2
0
19
showing
that
both
groups
of
participants
perceived
ability
to
learn
the
available
functions
was
greater
with
msw
personal
than
with
msw2k
figure
8
personality
type
also
had
a
borderline
significant
main
effect
on
learnability
f
1
18
4
07
mse
0
60
p
0
059
2
0
18
showing
that
independent
of
software
version
feature-keen
participants
felt
better
able
to
learn
the
functionality
offered
than
did
the
feature-shy
participants
the
q6
vs
q7
comparison
showed
that
the
software
version
had
a
borderline
significant
main
effect
f
1
18
3
08
mse
0
20
p
0
096
2
0
15
whereby
participants
perceived
ability
to
learn
decreased
when
they
returned
to
msw2k
summary
the
analysis
suggests
that
the
feature-keen
participants
generally
find
it
easier
to
learn
functions
than
do
the
feature-shy
participants
and
that
overall
it
was
easier
to
learn
functions
with
msw
personal
than
with
msw2k
hypothesis
supported
yes
h9
three-way
comparison
hypothesis
in
the
final
debriefing
interview
participants
were
asked
if
they
could
explain
how
what
they
called
the
changing
menus
worked
msw2k
s
adaptive
menus
although
all
participants
were
aware
of
the
short
and
long
menus
and
could
explain
how
to
expand
the
menus
7
of
the
20
participants
35
had
to
be
informed
that
the
short
menus
were
in
fact
adapting
to
their
personal
usage
given
our
sample
which
included
no
novice
users
this
was
particularly
suprising
participants
were
then
asked
to
rank
according
to
preference
msw
personal
msw2k
with
adaptive
menus
and
msw2k
without
adaptive
menus
the
standard
all-in-one
style
interface
figure
9
shows
the
frequency
of
the
three-way
rankings
of
the
six
possible
rankings
only
five
occurred
we
analyzed
the
frequency
with
which
each
menu
condition
was
ranked
first
by
calculating
the
chi-square
statistic
to
determine
if
actual
frequencies
were
significantly
different
from
the
case
in
which
all
frequencies
are
equal
if
we
consider
all
20
participants
there
was
a
significant
overall
preference
for
msw
personal
13
participants
65
2
2
20
9
10
p
0
011
we
cannot
apply
the
chi-square
statistic
independently
for
the
feature-keen
and
feature-shy
because
of
our
small
sample
sizes
instead
we
next
describe
the
data
for
each
group
to
indicate
possible
trends
to
make
two-way
comparisons
between
the
interfaces
for
each
of
the
personality
types
we
aggregated
across
the
rankings
for
example
by
looking
at
the
two
leftmost
ranking
orders
in
the
figure
we
see
that
7
feature-shy
participants
preferred
msw
personal
to
the
other
two
designs
from
the
remaining
ranking
orders
we
see
that
3
feature-shy
participants
ranked
the
all-in-one
design
before
the
msw
personal
design
this
shows
that
for
the
feature-shy
there
was
preference
for
the
msw
personal
to
the
all-in-one
design
7
participants
to
3
participants
one
can
repeat
the
same
steps
to
find
that
the
feature-shy
preferred
the
all-in-one
to
the
adaptive
design
8
to
2
however
the
feature-keen
did
not
prefer
the
all-in-one
to
both
the
adaptive
and
msw
personal
designs
as
expected
in
fact
msw
personal
was
preferred
to
adaptive
7
to
3
and
preferred
to
the
all-in-one
6
to
4
but
the
adaptive
was
preferred
to
the
all-in-one
7
to
3
only
2
of
the
feature-shy
ranked
adaptive
before
all-in-one
as
compared
to
7
of
the
feature-keen
summary
although
all
participants
were
aware
of
the
short
and
long
menus
in
msw2k
35
had
to
be
told
that
the
contents
of
the
short
menus
were
adapting
to
their
function
usage
msword
personal
was
preferred
by
the
majority
of
feature-shy
and
feature-keen
participants
65
of
all
participants
feature-shy
s
overall
ranking
was
adaptable
all-in-one
adaptive
feature-keen
s
overall
ranking
was
adaptable
adaptive
all-in-one
hypothesis
supported
partially
feature-shy
participants
did
rank
adaptable
all-in-one
and
then
adaptive
but
the
feature-keen
participants
did
not
rank
all-in-one
before
adaptive
and
adaptable
discussion
and
additional
qualitative
feedback
here
we
discuss
our
findings
related
to
the
comparison
of
the
two
interfaces
as
before
we
include
participants
comments
both
from
the
open-ended
sections
in
questionnaires
q1
through
q8
and
in
the
final
debriefing
interview
to
provide
more
context
for
the
quantitative
results
adaptive
menus
the
adaptive
menus
of
msw2k
were
liked
by
some
and
strongly
disliked
by
many
but
others
had
little
opinion
either
way
there
were
three
participants
who
ranked
the
adaptive
menus
in
word
2000
first
two
had
very
positive
comments
when
asked
if
they
were
aware
of
these
menus
and
if
they
knew
how
they
worked
for
example
yes
i
have
noticed
the
changing
menus
love
that
it
does
it
on
my
operating
system
as
well
yes
i
know
how
they
work
it
seems
that
the
functions
that
you
use
most
often
are
the
ones
that
show
up
or
i
don
t
know
if
they
are
the
ones
that
i
use
most
often
or
the
ones
that
are
used
most
often
i
haven
t
figured
that
one
out
yet
actually
i
don
t
think
it
is
the
ones
that
i
use
most
often
i
think
that
it
is
a
standard
small
set
and
then
you
click
on
the
bottom
and
the
whole
set
comes
up
interview
participant
22
it
seems
like
it
just
responds
to
whatever
functions
you
use
most
recently
it
gives
you
the
most
recent
five
or
whatever
i
like
that
kind
of
personalization
because
it
is
more
dynamic
and
it
just
seems
that
i
am
always
changing
what
i
am
doing
from
day
to
day
interview
participant
10
interestingly
both
participants
were
expert
long-term
users
of
msword
and
although
they
were
aware
of
the
adaptive
menus
neither
of
them
could
fully
explain
how
they
worked
participant
22
suspected
that
the
menus
adapted
to
her
usage
but
then
questioned
whether
this
was
right
participant
10
knew
that
they
were
adapting
but
implied
that
there
was
a
maximum
number
of
items
that
could
be
shown
when
in
fact
if
one
had
used
all
of
the
menu
items
recently
they
would
all
appear
in
the
short
menu
this
suggests
that
the
user
does
not
have
to
fully
understand
the
conceptual
model
of
an
adaptive
interface
in
order
to
be
satisfied
with
the
interface
if
the
adaptation
works
well
enough
then
understanding
the
underlying
mechanics
is
not
important
at
least
for
some
users
there
were
seven
participants
who
had
very
negative
experiences
of
the
adapting
menus
the
first
comment
below
refers
to
the
ordering
of
items
when
the
menu
expands
from
the
short
version
to
the
long
one
if
a
desired
item
is
not
found
in
the
short
menu
then
a
user
will
likely
have
to
rescan
the
full
long
menu
because
the
newly-visible
items
are
interspersed
with
those
that
have
already
been
scanned
this
usability
problem
might
be
fixed
by
highlighting
the
newly-visible
items
in
such
a
way
that
they
would
be
scanned
first
i
don
t
know
why
i
dislike
the
adaptive
menus
because
i
know
that
well
i
have
some
idea
why
but
well
okay
first
of
all
part
of
the
advantage
of
having
these
mega
menus
is
that
you
can
hunt
through
them
and
i
realize
that
the
stuff
you
use
more
tends
to
show
up
at
the
top
but
when
you
click
the
open
the
adaptive
menus
the
menu
shows
up
in
the
way
it
is
normally
so
if
there
s--if
you
use
two
functions
and
they
are
right
side
by
side
and
then
there
s
actually
a
bunch
of
stuff
in
between
that
will
show
up
in
between
and
so
i
find
it
s
like
i
have
to
start
all
over
again
looking
through
the
menus
for
the
functionality
which
i
find
really
annoying
and
i
don
t
know
why
it
s
confusing
i
just
find
it
more
confusing
i
think
that
s
ultimately
it
and
i
don
t
think
of
myself
as
a
na
ve
user
and
i
don
t
know
why
it
bothers
me
so
much
it
just
does
interview
i
participant
3
t
he
adaptive
menus
are
hell
i
don
t
like
them
at
all
so
like
that
s
a
definite
no--like
that
s
almost
a
zero
choice
i
would
never
pick
that
like
i
just
hate
it
interview
participant
7
i
hate
the
menus
where
only
your
most
recently
used
items
show
up
first
q7
participant
11
well
the
first
thing
is
with
the
word
2000--i
really
really
really
dislike
the--
i
mentioned
this
with
my
questionnaire
before--the
frequency
of
use
menu
i
was
often
making
mistakes
and
because
they
only
give
you
i
don
t
know
a
fixed
number
maybe
six
menu
items
i
tend
to
use
a
lot
of
different
functions
regularly
all
of
the
time
so
i
was
always
you
know
using
that
little
piece
down
arrow
icon
and
i
was
always
making
a
mistake
going--where
is
it
where
is
it
where
is
it
it
s
gone
it
fell
off
i
found
that
just
i
still
find
that
incredibly
frustrating
so
i
would
rather
not
do
that
and
word
personal
didn
t
do
that
so
i
much
prefer
it
interview
participant
16
four
additional
participants
felt
negatively
about
the
adaptive
menus
but
not
to
the
same
extreme
as
the
previous
seven
participants
for
example
i
don
t
really
feel
one
way
or
another
about
that
in
fact
i
d
rather
it
didn
t
do
that
because
sometimes
i
forget
like
i
m
looking
for
something
and
i
m
like--
oh
i
can
t
find
it
where
is
it
and
i
can
t
find
it
because
it
s
a
hidden
thingy
interview
participant
6
one
participant
did
appreciate
having
only
some
options
visible
through
the
shortened
menus
but
ultimately
found
that
msw
personal
provided
a
better
balance
for
him
this
feeling
that
you
will
forget
that
certain
functions
are
there
if
you
leave
the
adaptive
menus
turned
on
but
also
the
menus
are
way
too
long
if
you
leave
everything
on
i
e
adaptive
menus
turned
off
so
that
you
have
the
full
menu
so
it
s
a
balance
between
the
two
that
s
why
the
personal
gave
me
the
balance
i
wanted
interview
participant
17
to
summarize
13
participants
expressed
opinions
about
the
adaptive
menus
in
msw2k
for
two
participants
these
menus
worked
very
well
they
were
very
strongly
disliked
by
seven
and
four
were
mildly
negative
one
possible
explanation
is
that
the
adaptive
model
behind
the
menus
provided
a
better
fit
for
the
usage
patterns
of
the
two
satisfied
participants
than
for
the
other
participants
usage
patterns
understanding
the
required
degree
of
fit
of
an
adaptive
interface
in
order
to
achieve
usability
is
an
area
of
future
investigation
individual
differences
in
satisfaction
and
control
unlike
the
results
from
the
first
set
of
hypotheses
a
number
of
differences
between
the
feature-shy
and
the
feature-keen
participants
are
suggested
in
the
self-reported
measures
from
q1
through
q7
results
for
perceived
control
and
satisfaction
were
dominated
by
interactions
whereby
feature-shy
participants
experienced
an
increase
in
both
satisfaction
and
control
while
using
msw
personal
and
the
feature-keen
did
not
experience
any
significant
difference
one
way
this
can
be
interpreted
is
that
msw
personal
appears
to
have
improved
satisfaction
and
sense
of
control
for
the
feature-shy
without
negatively
affecting
the
feature-keen
once
they
had
used
msword
personal
for
4
weeks
the
feature-shy
were
able
to
achieve
a
comparable
level
of
satisfaction
and
perceived
control
to
the
feature-keen
this
suggests
that
through
the
redesign
of
the
user
interface
we
can
improve
the
experience
of
one
group
of
users
without
negatively
affecting
the
experience
of
another
group
navigation
the
comparison
of
the
q1
and
q6
data
showed
a
strong
effect
of
navigation
for
both
the
feature-shy
and
the
feature-keen
some
of
our
participants
specifically
noted
the
time
savings
when
there
were
fewer
options
to
navigate
through
in
the
menus
and
toolbars
representative
comments
include
i
really
like
having
only
the
tools
i
use
very
frequently
on
my
interface
if
i
so
choose
it
makes
me
more
efficient
as
i
don
t
have
to
look
around
for
the
function
i
need
q2
participant
15
while
i
use
a
standard
set
of
features
for
most
of
my
work
i
am
pleasantly
surprised
when
i
go
to
use
a
feature
i
haven
t
used
for
a
while
and
find
it
s
the
only
one
in
the
menu
it
makes
my
task
faster
and
less
frustrating
q5
participant
12
i
d
be
in
the
microsoft
word
interface
and
it
d
be
like--oh
god
just
too
many
buttons
like
i
don
t
think
that
microsoft
does
a
really
good
job
of
making
their
icon
match
what
the
button
actually
does
and
i
will
sit
there
and
i
will
have
to
hover
over
the
button
and
wait
for
the
explanation
to
come
up
and
it
s
like
oh
man
what
a
waste
of
time
so
that
s
when
i
d
find
myself
getting
like
okay
i
don
t
need
all
this
crap
right
now
it
s
too
hard
to
find
things
on
all
the
menubars
and
that
s
when
i
d
switch
back
to
the
personal
interview
participant
11
these
comments
suggest
that
the
difference
in
navigation
between
msword
personal
and
msw2k
is
not
a
subtle
difference
learnability
when
participants
were
asked
to
assess
the
learnability
of
the
multiple-interfaces
design
and
the
adaptive
design
in
questionnaires
q1
to
q7
the
multiple-interfaces
design
had
significantly
higher
ratings
in
the
debriefing
interview
however
the
all-in-one
style
interface
was
presented
as
an
option
alongside
the
other
two
interfaces
fifteen
participants
indicated
that
the
all-in-one
interface
was
best
and
the
standard
reason
given
was
that
seeing
all
of
the
menu
items
all
of
the
time
gives
one
a
sense
of
what
is
available
and
thereby
promotes
learning
the
available
functions
some
participants
specifically
mentioned
learning
through
exploration
two
representative
comments
are
2000
without
the
changing
menus
is
best
just
because
you
can
see
all
of
your
options
so
you
know
what
all
of
the
features
are
interview
participant
2
i
want
to
learn
them
all
or
nothing
in
general
i
think
that
if
they
are
there
you
are
more
likely
to
say--what
is
this
--and
use
it
so
maybe
a
little
bit
because
i
have
explored
the
only
way
i
ve
learned
to
use
the
program
is
by
playing
with
it
so
i
saw
the
indexes
and
i
went--how
do
you
do
that
--so
now
i
know
how
to
make
an
index
i
guess
if
i
never
saw
it
i
d
probably
never
have
played
with
it
interview
participant
22
two
participants
indicated
that
having
everything
available
in
the
full
interface
within
msw
personal
supported
learning
equally
as
well
as
did
the
allin-one
interface
they
did
not
need
to
be
accessing
the
full
menus
all
of
the
time
i
think
if
i
can
switch
to
the
full
interface
like
that
it
s
very
convenient
so
i
think
the
learning
ability
in
msw
personal
shouldn
t
be
impacted
just
one
click
interview
participant
5
i
want
it
all
or
i
want
mine
personal
interface
in
the
same
way
i
don
t
want
the
computer
deciding
what
it
s
going
to
show
me
i
want
to
decide
myself
if
i
don
t
know
how
to
do
something
then
i
want
to
go
and
use
the
full
interface
more
as
like
a
reference
or
something
and
have
it
all
kind
of
there
interview
participant
11
two
participants
indicated
that
they
learn
through
exploration
but
that
they
are
not
in
exploratory
mode
all
of
the
time
having
a
personal
interface
forced
them
to
take
ownership
of
learning
as
they
actively
decided
when
to
enter
exploratory
mode
by
switching
to
the
full
interface
for
example
i
think
the
one
with
the
adaptive
menus
doesn
t
support
it
learning
at
all
because
it
just
disappears
on
you
and
you
don
t
even
know
that
it
is
there
i
would
say
that
it
is
probably
similar
between
the
regular
word
long
menu
and
the
personal
one
because
you
still
have
to
think
that
you
need
something
different
and
find
it
often
my
strategy
around
that
is
that
somebody
says--oh
try
this--
or--there
must
be
a
way
to
do
this--and
then
go
to
help
or
whatever
what
about
learning
by
remembering
labels
you
have
seen
in
the
long
menus
that
s
not
been
my
experience
myself
i
must
say
that
whether
it
s
the
personal
sort
of
thing
or
the
long
menus
for
me
at
least
it
s--oh
i
have
to
go
exploring
i
m
going
to
go
look
because
with
your
sort
of
routine
daily
functions
i
am
not
using
the
menus
i
m
not
paying
attention
to
the
menus
so
i
m
not
in
explore
mode
i
m
not
even
in
attentive
mode
i
wouldn
t
even
notice
if
i
ve
seen
something
related
or
not
so
i
wouldn
t
usually
notice
interview
participant
16
none
of
the
participants
thought
that
the
adaptive
menus
best
supported
learning
which
is
a
strong
statement
understanding
learnability
is
a
rich
area
for
further
research
our
participants
certainly
did
not
perceive
that
minimalist
interfaces
provide
scaffolding
for
learning
we
saw
no
general
perception
of
a
training
wheels
effect
carroll
and
carrithers
1984
the
adaptive
interface
provides
minimalist
short
menus
and
they
were
ranked
last
for
learnability
by
all
of
our
participants
while
five
participants
ranked
the
adaptable
interface
first
the
majority
thought
that
allin-one
best
supported
learning
one
possible
next
step
would
be
to
evaluate
the
learnability
afforded
by
these
different
interfaces
with
novice
users
and
to
use
objective
measures
of
learning
rather
than
self
reports
we
did
not
include
any
novice
users
in
our
study
cost/benefit
trade-off
to
personalization
personalization
can
be
framed
in
terms
of
a
cost/benefit
trade-off
9
the
goal
is
that
the
cost
of
personalizing
time
attention
away
from
primary
task
will
be
outweighed
by
the
benefits
of
personalizing
we
have
already
mentioned
some
of
the
benefits
which
include
reduced
navigation
time
we
note
here
that
some
of
the
participants
were
analogously
very
aware
of
the
costs
for
example
the
cost
to
set
up
the
personal
interface
the
personal
interface
is
an
interesting
concept
but
seems
time
consuming
to
completely
set
up
i
am
still
adding
features
to
it
q3
participant
18
initial
configuration
was
time-consuming
but
it
is
ok
if
it
only
must
be
done
once
q8
participant
16
another
cost
is
the
additional
complexity
added
to
the
interface
while
the
goal
of
multiple-interfaces
is
to
allow
users
to
work
predominantly
in
a
simplified
interface
there
is
additional
functionality
that
needs
to
be
included
in
the
interface
in
order
to
make
this
possible
for
some
that
cost
dominated
the
thing
is
for
me
was
that
i
want
my
software
to
be
pretty
much
invisible
to
me
and
the
personal
required
more
visibility
than
i
would
have
liked
it
interview
participant
13
for
others
however
the
benefit
clearly
dominated
i
think
something
like
that
msw
personal
should
be
made
available
it
s
a
nice
thing--it
s
a
nice
interface
i
mean
you
know
i
don
t
know
how
easy
it
would
be
to
be
available
to
many
people
i
guess
that
you
would
have
to
package
it
or
whatever
but
it
was
a
nice
addition
i
actually
enjoyed
it
interview
participant
15
i
think
that
word
xp10
needs
a
personal
edition
even
more
than
2000
q8
participant
17
i
would
like
to
have
personal
re-enabled
on
my
machine
q8
participant
16
thus
for
some
users
even
if
the
cost
to
personalizing
was
relatively
high
there
was
sufficient
benefit
derived
from
a
personal
interface
to
make
it
worthwhile
for
others
the
cost
outweighed
the
benefit
the
difficulty
for
design
is
that
the
perceived
cost
and
benefit
are
both
dependent
on
individual
users
and
difficult
to
determine
a
priori
overall
interface
preference
msword
personal
was
preferred
by
the
majority
of
our
participants
seven
feature-shy
and
six
feature-keen
having
such
strong
support
by
the
feature-keen
was
unexpected
however
as
noted
above
the
two
groups
of
participants
differed
in
their
second
ranking
only
two
of
the
feature-shy
ranked
adaptive
before
all-in-one
as
compared
to
seven
of
the
feature-keen
this
can
perhaps
be
explained
in
part
by
the
fact
that
six
of
the
seven
participants
who
were
unaware
of
the
adapting
short
menus
were
feature-shy
participants
this
indicates
that
lack
of
knowledge
that
adaptation
is
taking
place
likely
contributes
to
overall
dissatisfaction
with
an
adaptive
interface
interestingly
of
the
13
participants
who
expressed
an
opinion
about
the
adaptive
menus
beyond
a
simple
ranking
only
two
were
positive
yet
three
participants
ranked
adaptive
first
and
9
ranked
it
second
so
adaptive
menus
did
have
supporters
the
imbalance
in
the
comments
about
these
menus
suggests
that
those
who
disliked
the
menus
had
a
more
extreme
or
passionate
dislike
as
compared
to
those
who
liked
the
menus
the
implication
for
user
interface
design
could
be
that
even
if
a
design
works
sufficiently
well
for
a
large
part
of
the
user
population
if
that
same
design
is
perceived
by
others
in
the
user
population
to
work
very
poorly
the
negative
views
will
dominate
not
surprisingly
the
group
of
13
participants
who
ranked
personal
first
is
almost
identical
to
the
group
of
13
who
did
not
give
up
on
their
desired
approach
to
personalization
one
participant
who
didn
t
give
up
did
not
rank
msw
personal
first
and
one
participant
who
did
give
up
did
rank
it
first
given
that
this
group
spanned
the
possible
personalization
strategies
it
suggests
that
flexibility
of
personalization
played
a
role
in
the
interface
ranking
users
have
the
ability
to
personalize
when
they
want
and
what
they
want
in
msword
personal
there
is
no
such
flexibility
in
msw2k
which
implements
a
single
personalization
strategy
independence
of
variables
one
might
argue
that
our
dependent
measures
of
satisfaction
navigation
control
and
learning
are
at
least
somewhat
related
to
our
independent
variable
of
personality
type
feature-keen
and
feature-shy
for
example
it
may
not
be
entirely
surprising
that
the
feature-shy
were
significantly
less
satisfied
than
the
feature-keen
with
msw2k
at
the
time
that
q1
was
administered
after
all
msw2k
is
a
feature-rich
application
there
was
another
independent
variable
however
namely
the
two
interface
conditions
it
is
the
impact
of
those
conditions
on
the
dependent
measures
that
is
most
interesting
in
the
findings
we
have
reported
in
this
section
the
results
of
this
evaluation
are
promising
however
there
were
inherent
limitations
and
constraints
to
the
experiment
design
that
may
have
affected
the
results
four
threats
to
validity
in
our
experiment
are
1
reactive
effect
participants
were
fully
aware
of
their
participation
in
the
experiment
and
some
may
have
adjusted
their
interactions
and
responses
2
multiple-treatment
interference
participants
were
exposed
to
two
versions
of
msword
and
there
were
likely
effects
of
having
used
one
version
that
were
not
erasable
when
using
the
second
3
researcher
interference
a
single
researcher
performed
the
role
of
the
experimenter
for
this
experiment
there
may
have
been
something
specific
to
the
particular
researcher
that
systematically
affected
the
results
4
selection
bias
participants
were
a
self-selected
group
because
we
did
not
have
a
sampling
frame
of
all
msw2k
users
and
therefore
could
not
draw
a
simple
random
sample
the
best
way
to
ensure
that
there
wasn
t
anything
incidental
in
our
experiment
execution
that
determined
the
results
would
be
to
replicate
the
experiment
ideally
we
would
want
to
conduct
a
longer
field
experiment
with
a
different
person
acting
in
the
researcher
role
counterbalancing
the
order
in
which
software
versions
are
used
would
be
ideal
in
addition
using
a
different
application
whether
it
be
another
word
processor
or
another
general
productivity
application
would
go
a
long
way
to
ensuring
the
generalizability
of
the
results
to
the
class
of
word
processing
applications
or
general
productivity
applications
as
a
whole
8
conclusions
and
future
work
we
conclude
with
some
final
thoughts
about
msword
personal
adaptive
versus
adaptable
designs
individual
differences
user
assisted
personalization
and
other
scenarios
of
use
for
multiple
interfaces
designs
msword
personal
the
multiple-interfaces
design
of
msword
personal
performed
very
well
in
our
field
evaluation
unlike
previous
work
by
mackay
1991
which
showed
that
users
customize
very
little
the
majority
of
our
participants
did
personalize
and
they
did
so
according
to
their
function
usage
the
fact
that
msw
personal
offers
a
new
style
of
interface
unfamiliar
to
all
our
participants
and
requires
effort
to
use
did
not
preclude
the
majority
of
participants
65
ranking
it
first
preferring
it
to
both
the
adaptive
interface
of
msw2k
or
an
all-in-one
style
interface
we
expect
that
had
it
been
possible
to
add
functions
faster
even
more
participants
would
have
ranked
personal
first
that
almost
as
many
of
the
feature-keen
6
participants
as
the
feature-shy
7
participants
ranked
personal
first
is
particularly
encouraging
adaptive
vs
adaptable
despite
the
breadth
of
research
into
adaptive
user
interfaces
there
has
been
relatively
little
empirical
comparison
between
adaptive
and
adaptable
interfaces
and
to
date
all
investigations
have
been
relatively
short
laboratory
experiments
e
g
debevc
et
al
1996
our
experiment
allowed
us
to
compare
one
instance
of
each
of
these
design
alternatives
in
the
context
of
a
commercial
software
application
with
users
carrying
out
real
tasks
in
their
own
environment
results
favored
the
adaptable
design
but
the
adaptive
interface
definitely
had
support
the
adaptable
design
implemented
in
msword
personal
combines
several
design
elements
two
interfaces
one
personal
interface
one
full
interface
a
simple
toggle
to
switch
between
the
interfaces
an
easily
adaptable
personal
interface
under
full
user
control
and
a
small
initial
personal
interface
the
interface
of
msw2k
by
comparison
has
a
single
interface
which
is
adapted
solely
by
the
system
with
the
exception
that
the
user
can
easily
open
a
short
menu
into
a
long
menu
our
evaluation
did
not
isolate
the
effects
of
the
different
interface
design
elements
although
where
possible
we
did
get
qualitative
feedback
on
those
elements
our
findings
suggest
that
msw
personal
was
preferred
to
msw2k
because
user-controlled
interface
adaptation
results
in
better
navigation
and
learnability
and
allows
for
the
adoption
of
different
personalization
strategies
as
compared
to
system-controlled
interface
adaptation
which
implements
a
single
strategy
because
there
were
several
differences
between
the
two
conditions
compared
we
do
not
assert
that
two
interfaces
are
always
better
than
one
nor
that
adaptable
is
always
better
than
adaptive
a
2
2
experimental
design
comparing
one/two
interfaces
by
adaptive/adaptable
would
be
required
to
tease
this
apart
we
did
not
do
this
for
the
reasons
given
earlier
based
on
the
qualitative
feedback
in
our
evaluation
however
we
strongly
believe
that
the
two-interface
aspect
of
msw
personal
was
a
key
contributing
factor
to
its
success
it
allowed
users
the
flexibility
to
adopt
different
personalization
strategies
we
did
in
fact
observe
different
personalization
strategies
we
note
that
the
effect
of
the
dual
interface
namely
support
to
easily
move
back
and
forth
between
a
personalized
interface
and
an
interface
with
default
settings
could
have
been
achieved
through
a
single-button
factory
reset
operation
if
the
reset
was
undoable
at
any
time
by
the
user
we
believe
that
conceptually
this
model
would
be
more
difficult
for
users
especially
novices
to
understand
and
trust
informative
next
steps
in
the
research
include
comparing
only
two-interface
designs
one
where
the
personal
interface
is
under
user
control
and
the
other
where
it
is
under
system
control
as
well
as
comparing
a
two-interface
design
to
a
factory
reset
model
that
achieves
the
same
outcome
but
with
a
different
conceptual
model
for
the
user
more
recent
laboratory
research
investigating
adaptive
designs
shows
conflicting
evidence
findlater
and
mcgrenere
2004
found
that
adaptive
split
menus
sears
and
shneiderman
1994
were
slower
than
static
split
menus
and
slower
than
adaptable
split
menus
in
most
circumstances
subjects
also
preferred
the
adaptable
menus
to
both
the
static
and
adaptive
ones
gajos
et
al
2006
found
an
adaptive
interface
to
be
faster
than
a
static
one
but
no
adaptable
alternative
was
evaluated
they
also
suggest
some
reasons
for
the
conflicting
evidence
for
example
the
frequency
with
which
adaptations
occur
but
more
work
is
needed
to
tease
these
issues
apart
alpert
et
al
2003
investigated
user
attitudes
regarding
a
user-adaptive
e-commerce
website
and
found
that
users
were
unenthusiastic
toward
system
attempts
to
infer
user
needs
and
provide
adapted
content
accordingly
a
strong
desire
to
have
full
and
explicit
control
of
the
content
and
interaction
was
expressed
jameson
and
schwarzkopf
2002
compared
adaptable
and
adaptive
systems
for
adding
items
to
a
hotlist
for
a
conference
web
site
while
there
was
no
performance
difference
anecdotal
evidence
showed
that
some
subjects
strongly
preferred
the
adaptive
system
while
others
strongly
preferred
the
adaptable
individual
differences
the
existence
of
individual
differences
with
respect
to
software
features
is
an
idea
that
has
been
proposed
in
the
literature
kaufman
and
weed
1998
mcgrenere
and
moore
2000
but
has
undergone
minimal
evaluation
based
on
our
research
it
appears
to
have
construct
validity
one
of
the
most
interesting
observations
is
that
while
there
were
no
substantial
differences
between
the
feature-keen
and
the
feature-shy
participants
in
terms
of
how
they
used
the
two
interfaces
and
how
they
approached
the
task
of
personalizing
there
were
a
number
of
differences
observed
in
terms
of
the
self-reported
measures
the
feature-shy
felt
more
satisfied
and
experienced
a
greater
sense
of
control
with
msw
personal
than
with
msw2k
whereas
there
were
no
differences
detected
for
the
feature-keen
on
these
measures
further
work
is
required
to
validate
the
instrument
used
to
assess
the
individual
differences
and
to
understand
how
this
aspect
of
personality
relates
to
other
well-documented
personality
differences
user-assisted
personalization
in
order
to
shift
the
cost
benefit
ratio
of
personalization
one
needs
to
increase
the
benefit
and
decrease
the
cost
benefit
can
be
increased
by
ensuring
that
the
personalized
interface
is
always
a
good
fit
for
the
user
and
that
the
cost
is
minimal
one
way
to
achieve
both
of
these
is
for
the
system
to
assist
the
user
in
personalizing
the
system
provides
adaptation
suggestions
based
on
usage
information
and
allows
the
user
to
accept
or
reject
the
suggestions
this
moves
the
design
in
the
direction
of
user-assisted
personalization
that
relies
on
user-modeling
technology
this
was
sometimes
called
user-controlled
self
adaptation
in
the
early
literature
dieterich
et
al
1993
and
more
recently
mixed-initiative
interaction
horvitz
1999
the
advantage
of
this
approach
is
that
the
user
retains
ultimate
control
and
the
system
does
the
bookkeeping
to
manage
the
knowledge
of
function
usage
and
changing
patterns
in
usage
a
task
at
which
the
system
is
particularly
adept
this
approach
has
been
investigated
by
others
for
example
in
flexcel
krogsoeter
et
al
1994
and
the
adaptive
toolbar
debevc
et
al
1996
both
with
mixed
user
testing
results
this
research
was
conducted
over
10
years
ago
but
has
not
been
commercialized
it
has
recently
resurfaced
in
the
research
community
lim
et
al
2005
miah
et
al
1997
suggesting
that
further
exploration
of
mixed-initiative
interaction
is
underway
key
aspects
of
ongoing
research
will
be
to
inform
how
and
when
to
provide
personalization
suggestions
in
terms
of
when
we
know
from
the
current
study
that
there
was
both
a
strong
initial
trigger
to
add
functions
and
a
need
to
amortize
the
cost
of
customizing
an
immediately-needed
function
by
at
the
same
time
adding
functions
that
are
expected
to
be
used
in
the
future
these
trigger
points
would
be
naturally
occuring
user
behaviors
upon
which
user-assisted
personalization
research
could
build
other
scenarios
for
multiple
interfaces
we
believe
the
concept
of
multiple
interfaces
has
potential
beyond
the
level-structured
design
seen
today
in
some
commercial
applications
a
possible
scenario
of
use
for
multiple
interfaces
is
to
support
users
making
the
transition
to
a
new
version
of
an
application
for
example
msw2k
could
include
the
msw
97
interface
often
users
delay
upgrading
their
software
because
of
the
time
required
to
learn
a
new
version
by
allowing
users
to
continue
to
work
in
their
old
interface
with
single-button
access
to
a
new
interface
users
would
be
able
to
transition
at
a
self-directed
pace
multiple
interfaces
might
also
be
used
to
mimic
a
competitor
s
interface
in
the
hopes
of
attracting
new
customers
for
example
msword
could
offer
the
full
interface
of
a
different
word
processor
such
as
wordperfect
or
vice-versa
with
a
single
button
click
in
order
to
support
users
making
the
transition
to
a
new
product
in
all
three
scenarios
help
facilities
could
take
advantage
of
the
fact
that
both
interfaces
are
accessible
to
show
the
user
how
functions
in
one
interface
map
to
functions
in
the
second
interface
this
is
a
good
example
of
how
the
adaptable
nature
of
a
multiple
interface
design
leaves
the
user
more
in
control
of
the
interface
we
believe
this
is
especially
important
during
critical
transitions
such
as
from
novice
to
experienced
user
from
one
version
of
a
product
to
the
next
and
from
one
product
to
a
competing
or
replacement
product
there
are
of
course
other
interface
differences
beyond
menus
and
toolbars
that
need
to
be
considered
for
the
multiple-interface
paradigm
this
too
is
an
area
for
future
work
